.. Automatically generated reST file from Doconce source
   (https://github.com/hplgit/doconce/)

Introduction to computing with finite difference methods
========================================================

:Author: Hans Petter Langtangen
:Date: Aug 18, 2013

Note: **PRELIMINARY VERSION**










Finite difference methods for partial differential equations (PDEs)
employ a range of concepts and tools that can be introduced and
illustrated in the context of simple ordinary differential equation
(ODE) examples.  This is what we do in the present document.  By
first working with ODEs, we keep the mathematical problems to be
solved as simple as possible (but no simpler), thereby allowing full
focus on understanding the key concepts and tools.  The choice of
topics in the forthcoming treatment of ODEs is therefore solely
dominated by what carries over to numerical methods for PDEs.

Theory and practice are primarily illustrated by solving the
very simple ODE :math:`u'=-au`, :math:`u(0)=I`, where :math:`a>0` is a constant,
but we also address the generalized problem :math:`u'=-a(t)u + b(t)`
and the nonlinear problem :math:`u'=f(u,t)`.
The following topics are introduced:

 * How to think when constructing finite difference methods, with special focus
   on the Forward Euler, Backward Euler, and Crank-Nicolson (midpoint)
   schemes

 * How to formulate a computational algorithm and translate it into
   Python code

 * How to make curve plots of the solutions

 * How to compute numerical errors

 * How to compute convergence rates

 * How to verify an implementation and automate verification
   through nose tests in Python

 * How to structure code in terms of functions, classes, and modules

 * How to work with Python concepts such as arrays, lists, dictionaries,
   lambda functions, functions in functions (closures), doctests,
   unit tests, command-line interfaces, graphical user interfaces

 * How to perform array computing and understand the difference from
   scalar computing

 * How to conduct and automate large-scale numerical experiments

 * How to generate scientific reports

 * How to uncover numerical artifacts in the computed solution

 * How to analyze the numerical schemes mathematically to understand
   why artifacts occur

 * How to derive mathematical expressions for various measures of
   the error in numerical methods, frequently by using the ``sympy`` software
   for symbolic computation

 * Introduce concepts such as finite difference operators,
   mesh (grid), mesh functions,
   stability, truncation error, consistency, and convergence

 * Present additional methods for the general nonlinear ODE :math:`u'=f(u,t)`,
   which is either a scalar ODE or a system of ODEs

 * How to access professional packages for solving ODEs

 * How the model equation :math:`u'=-au` arises in a wide range
   of phenomena in physics, biology, and finance


.. admonition:: The exposition in a nutshell

   Everything we cover is put into a practical, hands-on context. All
   mathematics is translated into working computing codes, and all the
   mathematical theory of finite difference methods presented here is
   motivated from a strong need to understand strange behavior of programs.
   Two fundamental questions saturate the text:
   
    * How to we solve a differential equation problem and produce numbers?
   
    * How to we trust the answer?


.. !split


.. _decay:basics:

Finite difference methods
=========================



.. admonition:: Goal

   We explain the basic ideas of finite difference methods
   using a simple ordinary differential equation :math:`u'=-au` as
   primary example.
   Emphasis is put on the reasoning when discretizing the problem and
   introduction of key concepts such as mesh, mesh function,
   finite difference approximations, averaging in a mesh,
   deriation of algorithms, and discrete operator notation.


.. _decay:model:

A basic model for exponential decay
-----------------------------------


.. index:: decay ODE

.. index:: exponential decay


Our model problem is perhaps the simplest ordinary differential
equation (ODE):


.. math::
        
        u'(t) = -au(t),
        

Here, :math:`a>0` is a constant and :math:`u'(t)` means differentiation with
respect to time :math:`t`. This type of equation arises in a number of
widely different phenomena where some quantity :math:`u` undergoes
exponential reduction. Examples include radioactive decay, population
decay, investment decay, cooling of an object, pressure decay in the
atmosphere, and retarded motion in fluids (for some of these models,
:math:`a` can be negative as well), see the section :ref:`decay:app` for details
and motivation.  We have chosen this particular ODE not only because
its applications are relevant, but even more because studying
numerical solution methods for this simple ODE gives important insight
that can be reused in much more complicated settings, in particular
when solving diffusion-type partial differential equations.

The analytical solution of the ODE is found by the method of
separation of variables, which results in


.. math::
         u(t) = Ce^{-at},

for any arbitrary constant :math:`C`.
To formulate a mathematical problem for which there
is a unique solution, we need a condition to fix the value of :math:`C`.
This condition is known as the *initial condition* and stated as
:math:`u(0)=I`. That is, we know the
value :math:`I` of :math:`u` when the process starts at :math:`t=0`. The exact solution
is then :math:`u(t)=Ie^{-at}`.

We seek the solution :math:`u(t)` of the ODE for :math:`t\in (0,T]`. The point :math:`t=0` is not
included since we know :math:`u` here and assume that the equation governs
:math:`u` for :math:`t>0`. The complete ODE problem then reads: find :math:`u(t)`
such that


.. math::
   :label: decay:problem
        
        u' = -au,\ t\in (0,T], \quad u(0)=I\thinspace .  
        

This is known as a *continuous problem* because the parameter :math:`t`
varies continuously from :math:`0` to :math:`T`. For each :math:`t` we have a corresponding
:math:`u(t)`. There are hence infinitely many values of :math:`t` and :math:`u(t)`.
The purpose of a numerical method is to formulate a corresponding
*discrete* problem whose solution is characterized by a finite number of values,
which can be computed in a finite number of steps on a computer.



.. _decay:schemes:FE:

The Forward Euler scheme
------------------------

Solving an ODE like :eq:`decay:problem` by a finite difference method
consists of the following four steps:

1. discretizing the domain,

2. fulfilling the equation at discrete time points,

3. replacing derivatives by finite differences,

4. formulating a recursive algorithm.

.. index:: mesh

.. index:: grid


Step 1: Discretizing the domain
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The time domain :math:`[0,T]` is represented by a finite number of
:math:`N_t+1` points


.. math::
        
        0 = t_0 < t_1 < t_2 < \cdots < t_{N_t-1} < t_{N_t} = T\thinspace .
        

The collection of points :math:`t_0,t_1,\ldots,t_{N_t}` constitutes a *mesh*
or *grid*. Often the mesh points will be uniformly spaced in
the domain :math:`[0,T]`, which means that the spacing :math:`t_{n+1}-t_n` is
the same for all :math:`n`. This spacing is often denoted by :math:`\Delta t`,
in this case :math:`t_n=n\Delta t`.


.. index:: mesh function


We seek the solution :math:`u` at the mesh points:
:math:`u(t_n)`, :math:`n=1,2,\ldots,N_t`. Note that :math:`u^0` is already known as :math:`I`.
A notational short-form for :math:`u(t_n)`,
which will be used extensively, is :math:`u^{n}`. More precisely, we let
:math:`u^n` be the *numerical approximation* to the exact solution :math:`u(t_n)`
at :math:`t=t_n`. The numerical approximation is a *mesh function*,
here defined only at the mesh points.
When we need to clearly distinguish between the numerical
and the exact solution, we often place a subscript e on the exact
solution, as in :math:`u_{\small\mbox{e}}(t_n)`. Figure :ref:`decay:fdu:e` shows the
:math:`t_n` and :math:`u_n` points for :math:`n=0,1,\ldots,N_t=7` as well as :math:`u_{\small\mbox{e}}(t)`
as the dashed line. The goal of a numerical method for ODEs is
to compute the mesh function by solving a finite set of
*algebraic equations* derived from the original ODE problem.


.. _decay:fdu:e:

.. figure:: fdm_u_ue.png
   :width: 600

   *Time mesh with discrete solution values*


Since finite difference methods produce solutions at the mesh
points only, it is an open question what the solution is between
the mesh points. One can use methods for interpolation to
compute the value of :math:`u` between mesh points. The simplest
(and most widely used) interpolation method is to assume that
:math:`u` varies linearly between the mesh points, see
Figure :ref:`decay:fdu:ei`. Given :math:`u^{n}`
and :math:`u^{n+1}`, the value of :math:`u` at some :math:`t\in [t_{n}, t_{n+1}]`
is by linear interpolation


.. math::
        
        u(t) \approx u^n + \frac{u^{n+1}-u^n}{t_{n+1}-t_n}(t - t_n)\thinspace .
        



.. _decay:fdu:ei:

.. figure:: fdm_u_uei.png
   :width: 600

   *Linear interpolation between the discrete solution values (dashed curve is exact solution)*



Step 2: Fulfilling the equation at discrete time points
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ODE is supposed to hold for all :math:`t\in (0,T]`, i.e., at an infinite
number of points. Now we relax that requirement and require that
the ODE is fulfilled at a finite set of discrete points in time.
The mesh points :math:`t_1,t_2,\ldots,t_{N_t}` are a natural choice of points.
The original ODE is then reduced to  the following :math:`N_t` equations:


.. math::
   :label: decay:step2
        
        u'(t_n) = -au(t_n),\quad n=1,\ldots,N_t\thinspace .
        
        



.. index:: finite differences


Step 3: Replacing derivatives by finite differences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next and most essential step of the method is to replace the
derivative :math:`u'` by a finite difference approximation. Let us first
try a one-sided difference approximation (see Figure :ref:`decay:sketch:FE`),


.. math::
   :label: decay:FEdiff
        
        u'(t_n) \approx \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n}\thinspace .
        
        

Inserting this approximation in :eq:`decay:step2` results in


.. math::
   :label: decay:step3
        
        \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -au^{n},\quad n=0,1,\ldots,N_t-1\thinspace .
        
        

This equation is the discrete counterpart to the original ODE problem
:eq:`decay:problem`, and often referred to as *finite difference scheme*
or more generally as the *discrete equations* of the problem.
The fundamental feature of these equations is that they are *algebraic*
and can hence be straightforwardly solved to produce the mesh function, i.e.,
the values of :math:`u` at
the mesh points (:math:`u^n`, :math:`n=1,2,\ldots,N_t`).


.. _decay:sketch:FE:

.. figure:: fd_forward.png
   :width: 400

   *Illustration of a forward difference*



.. index:: difference equation


.. index:: discrete equation


.. index:: algebraic equation


.. index:: finite difference scheme


.. index:: Forward Euler scheme


Step 4: Formulating a recursive algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The final step is to identify the computational algorithm to be implemented
in a program. The key observation here is to realize that
:eq:`decay:step3` can be used to compute :math:`u^{n+1}` if :math:`u^n` is known.
Starting with :math:`n=0`, :math:`u^0` is known since :math:`u^0=u(0)=I`, and
:eq:`decay:step3` gives an equation for :math:`u^1`. Knowing :math:`u^1`,
:math:`u^2` can be found from :eq:`decay:step3`. In general, :math:`u^n`
in :eq:`decay:step3` can be assumed known, and then we can easily solve for
the unknown :math:`u^{n+1}`:


.. math::
   :label: decay:FE
        
        u^{n+1} = u^n - a(t_{n+1} -t_n)u^n\thinspace .
        
        

We shall refer to :eq:`decay:FE` as the Forward Euler (FE) scheme
for our model problem. From a mathematical point of view,
equations of the form :eq:`decay:FE` are known as
*difference equations* since they express how differences in
:math:`u`, like :math:`u^{n+1}-u^n`, evolve with :math:`n`.
The finite difference method can be viewed as a method for turning
a differential equation into a difference equation.

Computation with :eq:`decay:FE` is straightforward:


.. math::
        
        u_0 &= I,\\ 
        u_1 & = u^0 - a(t_{1} -t_0)u^0 = I(1-a(t_1-t_0)),\\ 
        u_2 & = u^1 - a(t_{2} -t_1)u^1 = I(1-a(t_1-t_0))(1 - a(t_2-t_1)),\\ 
        u^3 &= u^2 - a(t_{3} -t_2)u^2 = I(1-a(t_1-t_0))(1 - a(t_2-t_1))(1 - a(t_3-t_2)),
        

and so on until we reach :math:`u^{N_t}`.
Very often, :math:`t_{n+1}-t_n` is constant for all :math:`n`, so we can introduce
the common symbol :math:`\Delta t` for the time step:
:math:`\Delta t = t_{n+1}-t_n`, :math:`n=0,1,\ldots,N_t-1`.
Using a constant time step :math:`\Delta t` in the above calculations gives


.. math::
        
        u_0 &= I,\\ 
        u_1 & = I(1-a\Delta t),\\ 
        u_2 & = I(1-a\Delta t)^2,\\ 
        u^3 &= I(1-a\Delta t)^3,\\ 
        &\vdots\\ 
        u^{N_t} &= I(1-a\Delta t)^{N_t}\thinspace .
        

This means that we have found a closed formula for :math:`u^n`, and there is
no need to let a computer generate the sequence :math:`u^1, u^2, u^3, \ldots`.
However, finding such a formula for :math:`u^n` is possible only for a few very
simple problems, so in general finite difference equations must be
solved on a computer.

As the next sections will show, the scheme :eq:`decay:FE` is just one
out of many alternative finite difference (and other) methods for
the model problem :eq:`decay:problem`.

.. _decay:schemes:BE:

The Backward Euler scheme
-------------------------

There are several choices of difference approximations in step 3 of
the finite difference method as presented in the previous section.
Another alternative is


.. math::
   :label: decay:BEdiff
        
        u'(t_n) \approx \frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}}\thinspace .
        
        

Since this difference is based on going backward in time (:math:`t_{n-1}`)
for information, it is known as the Backward Euler difference.
Figure :ref:`decay:sketch:BE` explains the idea.


.. _decay:sketch:BE:

.. figure:: fd_backward.png
   :width: 400

   *Illustration of a backward difference*



.. index::
   single: backward scheme, 1-step


.. index:: Backward Euler scheme


Inserting :eq:`decay:BEdiff` in :eq:`decay:step2` yields
the Backward Euler (BE) scheme:


.. math::
   :label: decay:BE0
        
        \frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}} = -a u^n\thinspace .
        
        

We assume, as explained under step 4 in the section :ref:`decay:schemes:FE`,
that we have computed :math:`u^0, u^1, \ldots, u^{n-1}` such that
:eq:`decay:BE0` can be used to compute :math:`u^n`.
For direct similarity with the Forward Euler scheme :eq:`decay:FE`
we replace :math:`n` by :math:`n+1` in :eq:`decay:BE0` and solve for the
unknown value :math:`u^{n+1}`:


.. math::
   :label: decay:BE
        
        u^{n+1} = \frac{1}{1+ a(t_{n+1}-t_n)} u^n\thinspace .
        
        


.. _decay:schemes:CN:

The Crank-Nicolson scheme
-------------------------


.. index:: Crank-Nicolson scheme


The finite difference approximations used to derive the schemes
:eq:`decay:FE` and :eq:`decay:BE` are both one-sided differences,
known to be less accurate than central (or midpoint)
differences. We shall now construct
a central difference at :math:`t_{n+1/2}=\frac{1}{2} (t_n + t_{n+1})`, or
:math:`t_{n+1/2}=(n+\frac{1}{2})\Delta t` if the mesh spacing is uniform in time.
The approximation reads


.. math::
   :label: decay:CNdiff
        
        u'(t_{n+\frac{1}{2}}) \approx \frac{u^{n+1}-u^n}{t_{n+1}-t_n}\thinspace .
        
        

Note that the fraction on the right-hand side is the same as for the
Forward Euler approximation :eq:`decay:FEdiff` and
the Backward Euler approximation :eq:`decay:BEdiff` (with
:math:`n` replaced by :math:`n+1`). The accuracy of this fraction as an approximation
to the derivative of :math:`u` depends on *where* we seek the derivative:
in the center of the interval :math:`[t_{n},t_{n+1}]` or at the end points.

With the formula :eq:`decay:CNdiff`, where :math:`u'` is evaluated at
:math:`t_{n+1/2}`, it is natural to demand the
ODE to be fulfilled at the time points between the mesh points:


.. math::
   :label: decay:step2m
        
        u'(t_{n+\frac{1}{2}}) = -au(t_{n+\frac{1}{2}}),\quad n=0,
        \ldots,N_t-1\thinspace .
        
        

Using :eq:`decay:CNdiff` in :eq:`decay:step2m` results in


.. math::
   :label: decay:CN0
        
        \frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -au^{n+\frac{1}{2}},
        
        

where :math:`u^{n+\frac{1}{2}}` is a short form for :math:`u(t_{n+\frac{1}{2}})`.
The problem is that we aim to compute :math:`u^n` for integer :math:`n`, implying that
:math:`u^{n+\frac{1}{2}}` is not a quantity computed by our method. It must
therefore be
expressed by the quantities that we actually produce, i.e.,
the numerical solution at the
mesh points. One possibility is to approximate :math:`u^{n+\frac{1}{2}}`
as an arithmetic average of the :math:`u` values at the neighboring mesh points:


.. index::
   single: averaging; arithmetic



.. math::
   :label: decay:uhalfavg
        
        u^{n+\frac{1}{2}} \approx \frac{1}{2} (u^n + u^{n+1})\thinspace .
        
        

Using :eq:`decay:uhalfavg` in :eq:`decay:CN0` results in


.. math::
   :label: decay:CN1
        
        \frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -a\frac{1}{2} (u^n + u^{n+1})\thinspace .
        
        

Figure :ref:`decay:sketch:BE` sketches the geometric interpretation of
such a centered difference.


.. _decay:sketch:BE:

.. figure:: fd_centered.png
   :width: 400

   *Illustration of a centered difference*


We assume that :math:`u^n` is already computed so that :math:`u^{n+1}` is the
unknown, which we can solve for:


.. math::
   :label: decay:CN
        
        u^{n+1} = \frac{1-\frac{1}{2} a(t_{n+1}-t_n)}{1 + \frac{1}{2} a(t_{n+1}-t_n)}u^n\thinspace .
        
        

The finite difference scheme :eq:`decay:CN` is often called
the Crank-Nicolson (CN) scheme or a midpoint or centered scheme.


.. _decay:schemes:theta:

The unifying :math:`\theta`-rule
--------------------------------


.. index:: weighted average

.. index:: theta-rule

.. index:: theta-rule


The Forward Euler, Backward Euler, and Crank-Nicolson schemes can be
formulated as one scheme with a varying parameter :math:`\theta`:


.. math::
   :label: decay:th0
        
        \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -a (\theta u^{n+1} + (1-\theta) u^{n})
        
        \thinspace .
        


 * :math:`\theta =0` gives the Forward Euler scheme

 * :math:`\theta =1` gives the Backward Euler scheme, and

 * :math:`\theta =1/2` gives the Crank-Nicolson scheme.

 * We may alternatively choose any other value of :math:`\theta` in :math:`[0,1]`.

As before, :math:`u^n` is considered known and :math:`u^{n+1}` unknown, so
we solve for the latter:


.. math::
   :label: decay:th
        
        u^{n+1} = \frac{1 - (1-\theta) a(t_{n+1}-t_n)}{1 + \theta a(t_{n+1}-t_n)}\thinspace .
        
        

This scheme is known as the :math:`\theta`-rule, or alternatively written as
the "theta-rule".


.. admonition:: Derivation

   We start with replacing :math:`u'` by the fraction
   
   
   .. math::
            \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n},
   
   in the Forward Euler, Backward Euler,
   and Crank-Nicolson schemes. Then we observe that
   the difference between the methods concerns which point this
   fraction approximates the derivative. Or in other words, at which point we
   sample the ODE. So far this has been the
   end points or the midpoint of :math:`[t_n,t_{n+1}]`. However, we may choose any point
   :math:`\tilde t \in [t_n,t_{n+1}]`.
   The difficulty
   is that evaluating the right-hand side :math:`-au` at an arbitrary point
   faces the same problem as in
   the section :ref:`decay:schemes:CN`: the point value must be expressed
   by the discrete :math:`u` quantities that we compute by the scheme, i.e.,
   :math:`u^n` and :math:`u^{n+1}`. Following the averaging idea from
   the section :ref:`decay:schemes:CN`,
   the value of :math:`u` at an arbitrary point :math:`\tilde t` can be
   calculated as a *weighted average*, which generalizes the arithmetic average
   :math:`\frac{1}{2}u^n + \frac{1}{2}u^{n+1}`.
   If we express :math:`\tilde t` as a weighted average
   
   .. math::
            t_{n+\theta} = \theta t_{n+1} + (1-\theta) t_{n},
   
   where :math:`\theta\in [0,1]` is the weighting factor, we can write
   
   
   .. math::
      :label: decay:thetaavg
           
           u(\tilde t) = u(\theta t_{n+1} + (1-\theta) t_{n}) \approx
           \theta u^{n+1} + (1-\theta) u^{n}\thinspace .
           
           
   
   
   
   .. index:: theta-rule
   
   
   We can now let the ODE hold at the point
   :math:`\tilde t\in [t_n,t_{n+1}]`, approximate :math:`u'` by the fraction
   :math:`(u^{n+1}-u^{n})/(t_{n+1}-t_n)`, and approximate the right-hand
   side :math:`-au` by the weighted average :eq:`decay:thetaavg`.
   The result is :eq:`decay:th0`.


Constant time step
------------------

All schemes up to now have been formulated for a general non-uniform
mesh in time: :math:`t_0,t_1,\ldots,t_{N_t}`. Non-uniform meshes are highly relevant
since one can use many points in regions where :math:`u` varies rapidly, and
save points in regions where :math:`u` is slowly varying. This is the key idea
of *adaptive* methods where the spacing of the mesh points
are determined as the computations proceed.

However, a uniformly distributed set of mesh points is very common and
sufficient for many applications. It therefore makes sense to
present the finite difference schemes for a uniform point distribution
:math:`t_n=n\Delta t`, where :math:`\Delta t` is the constant spacing between
the mesh points, also referred to as the *time step*.
The resulting formulas look simpler and are perhaps more
well known.



.. admonition:: Summary of schemes for constant time step

   
   .. math::
      :label: decay:FE:u
           
           u^{n+1} = (1 - a\Delta t )u^n  \quad (\hbox{FE})
           
           
   
   
   
   .. math::
      :label: decay:BE:u
             
           u^{n+1} = \frac{1}{1+ a\Delta t} u^n  \quad (\hbox{BE})
           
           
   
   
   
   .. math::
      :label: decay:CN:u
             
           u^{n+1} = \frac{1-\frac{1}{2} a\Delta t}{1 + \frac{1}{2} a\Delta t} u^n \quad (\hbox{CN})
           
           
   
   
   
   .. math::
      :label: decay:th:u
             
           u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n \quad (\theta-\hbox{rule})


Not surprisingly, we present these three alternative schemes
because they have different pros and cons, both for the simple ODE
in question (which can easily be solved as accurately as desired), and for
more advanced differential equation problems.



.. admonition:: Test the understanding

   At this point it can be good training to apply the explained
   finite difference discretization techniques to a slightly
   different equation. :ref:`decay:app:exer:cooling:schemes`
   is therefore highly recommended to check that the key concepts
   are understood.


.. _decay:fd:op:

Compact operator notation for finite differences
------------------------------------------------


.. index:: finite difference operator notation

.. index::
   single: operator notation, finite differences


Finite difference formulas can be tedious to write and read,
especially for differential equations with many terms and many
derivatives. To save space and help the reader of the scheme to quickly
see the nature of the difference approximations, we introduce a
compact notation. A forward difference approximation is denoted
by the :math:`D_t^+` operator:


.. math::
   :label: fd:D:f
        
        [D_t^+u]^n = \frac{u^{n+1} - u^{n}}{\Delta t}
        \approx \frac{d}{dt} u(t_n) 
        \thinspace .
        

The notation consists of an operator that approximates
differentiation with respect to an independent variable, here :math:`t`.
The operator is built of the symbol :math:`D`, with the variable as subscript
and a superscript denoting the type of difference. The superscript :math:`\,{}^+`
indicates a forward difference.
We place square brackets around the operator and the function it operates
on and specify the mesh point, where the operator is acting, by
a superscript.

The corresponding operator notation for a centered difference and
a backward difference reads


.. math::
   :label: fd:D:c
        
        [D_tu]^n = \frac{u^{n+\frac{1}{2}} - u^{n-\frac{1}{2}}}{\Delta t}
        \approx \frac{d}{dt} u(t_n), 
        

and

.. math::
   :label: fd:D:b
        
        [D_t^-u]^n = \frac{u^{n} - u^{n-1}}{\Delta t}
        \approx \frac{d}{dt} u(t_n) 
        \thinspace .
        

Note that the superscript :math:`\,{}^-` denotes the backward
difference, while no superscript implies a central difference.

An averaging operator is also convenient to have:


.. math::
   :label: fd:mean:a
        
        [\overline{u}^{t}]^n = \frac{1}{2} (u^{n-\frac{1}{2}} + u^{n+\frac{1}{2}} )
        \approx u(t_n) 
        

The superscript :math:`t` indicates that the average is taken along the time
coordinate. The common average :math:`(u^n + u^{n+1})/2` can now be
expressed as :math:`[\overline{u}^{t}]^{n+1/2}`. (When also spatial coordinates
enter the problem, we need the explicit specification of the coordinate
after the bar.)


The Backward Euler finite difference approximation to :math:`u'=-au` can be written
as follows utilizing the compact notation:


.. math::
        
        [D_t^-u]^n = -au^n \thinspace .
        

In difference equations we often place the square brackets around
the whole equation, to indicate at which mesh point the equation applies,
since each term is supposed to be approximated at the same point:


.. math::
        
        [D_t^- u  = -au]^n \thinspace .
        

The Forward Euler scheme takes the form


.. math::
        
        [D_t^+ u  = -au]^n,
        

while the Crank-Nicolson scheme is written as


.. math::
   :label: fd:compact:ex:CN
        
        [D_t u = -a\overline{u}^t]^{n+\frac{1}{2}}\thinspace .
        
        




.. admonition:: Question

   Apply :eq:`fd:D:c` and :eq:`fd:mean:a` and write out the
   expressions to see that :eq:`fd:compact:ex:CN` is indeed the
   Crank-Nicolson scheme.


The :math:`\theta`-rule can be specified by


.. math::
   :label: decay:fd1:op:theta
        
        [\bar D_t u = -a\overline{u}^{t,\theta}]^{n+\theta},
        
        

if we define a new time difference and a *weighted averaging operator*:


.. math::
        
        \lbrack\bar D_t u\rbrack^{n+\theta} = \frac{u^{n+1}-u^n}{t^{n+1}-t^n},
        
        
        
        \lbrack\overline{u}^{t,\theta}\rbrack^{n+\theta} = (1-\theta)u^{n} + \theta u^{n+1}
        \approx u(t_{n+\theta}),
        
        

where :math:`\theta\in [0,1]`. Note that for :math:`\theta =1/2` we recover
the standard centered difference and the standard arithmetic average.
The idea in :eq:`decay:fd1:op:theta` is to sample the equation at
:math:`t_{n+\theta}`, use a skew difference at that
point :math:`[\bar D_t u]^{n+\theta}`, and a skew mean value.
An alternative notation is

.. math::
         [D_t u]^{n+1/2} = \theta [-au]^{n+1} + (1-\theta)[-au]^{n}\thinspace .


Looking at the various examples above and comparing them with the
underlying differential equations, we see immediately which difference
approximations that have been used and at which point they
apply. Therefore, the compact notation effectively communicates the
reasoning behind turning a differential equation into a difference
equation.


.. !split


.. _decay:impl1:

Implementation  (1)
===================



.. admonition:: Goal

   We want make a computer program for solving
   
   .. math::
           
           u'(t) = -au(t),\quad t\in (0,T], \quad u(0)=I,
           
   
   by finite difference methods. The program should also display
   the numerical solution as a curve on the
   screen, preferably together with the
   exact solution. We shall also be concerned with program testing,
   user interfaces, and computing convergence rates.


.. index:: directory

.. index:: folder


All programs referred to in this section are found in the
`src/decay <http://tinyurl.com/jvzzcfn/decay>`_ directory (we use the classical
Unix term *directory* for what many others nowadays call *folder*).

*Mathematical problem.* We want to explore the Forward Euler scheme, the
Backward Euler, and the Crank-Nicolson schemes applied to our model problem.
From an implementational point of view, it is advantageous to
implement the :math:`\theta`-rule

.. math::
        
        u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n,
        

since it can generate the three other schemes by various of
choices of :math:`\theta`: :math:`\theta=0` for Forward Euler, :math:`\theta =1` for
Backward Euler, and :math:`\theta =1/2` for Crank-Nicolson.
Given :math:`a`, :math:`u^0=I`, :math:`T`, and :math:`\Delta t`,
our task is to use the :math:`\theta`-rule to
compute :math:`u^1, u^2,\ldots,u^{N_t}`, where :math:`t_{N_t}=N_t\Delta t`, and
:math:`N_t` the closest integer to :math:`T/\Delta t`.

*Computer Language: Python.* Any programming language can be used to generate the :math:`u^{n+1}` values from
the formula above. However, in this document we shall mainly make use of
Python of several reasons:

  * Python has a very clean, readable syntax (often known as
    "executable pseudo-code").

  * Python code is very similar to MATLAB code (and MATLAB has a
    particularly widespread use for scientific computing).

  * Python is a full-fledged, very powerful programming language.

  * Python is similar to, but much simpler to work with and
    results in more reliable code than C++.

  * Python has a rich set of modules for scientific computing, and its
    popularity in scientific computing is rapidly growing.

  * Python was made for being combined with compiled languages
    (C, C++, Fortran) to reuse existing numerical software and to
    reach high computational performance of new implementations.

  * Python has extensive support for administrative task
    needed when doing large-scale computational investigations.

  * Python has extensive support for graphics (visualization,
    user interfaces, web applications).

  * FEniCS, a very powerful tool for solving PDEs by
    the finite element method, is most human-efficient to operate
    from Python.

Learning Python is easy. Many newcomers to the language will probably
learn enough from the forthcoming examples to perform their own computer
experiments. The examples start with simple Python code and gradually
make use of more powerful constructs as we proceed. As long as it is
not inconvenient for the problem at hand, our Python code is made as
close as possible to MATLAB code for easy transition between the two
languages.

Readers who feel the Python examples are too hard to follow will probably
benefit from read a tutorial, e.g.,

  * `The Official Python Tutorial <http://docs.python.org/2/tutorial/>`_

  * `Python Tutorial on tutorialspoint.com <http://www.tutorialspoint.com/python/>`_

  * `Interactive Python tutorial site <http://www.learnpython.org/>`_

  * `A Beginner's Python Tutorial <http://en.wikibooks.org/wiki/A_Beginner's_Python_Tutorial>`_

The author also has a book [Ref1]_ that introduces
scientific programming with Python.


.. _decay:py1:

Making a solver function
------------------------

We choose to have an array ``u`` for storing the :math:`u^n` values, :math:`n=0,1,\ldots,N_t`.
The algorithmic steps are

 1. initialize :math:`u^0`

 2. for :math:`t=t_n`, :math:`n=1,2,\ldots,N_t`: compute :math:`u_n` using
    the :math:`\theta`-rule formula

Function for computing the numerical solution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following Python function takes the input data of the problem
(:math:`I`, :math:`a`, :math:`T`, :math:`\Delta t`, :math:`\theta`) as arguments and returns two arrays with
the solution :math:`u^0,\ldots,u^{N_t}` and the mesh points :math:`t_0,\ldots,t_{N_t}`,
respectively:


.. code-block:: python

        from numpy import *
        
        def solver(I, a, T, dt, theta):
            """Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt."""
            Nt = int(T/dt)            # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
            return u, t


The ``numpy`` library contains a lot of functions for array computing. Most
of the function names are similar to what is found
in the alternative scientific computing language MATLAB. Here
we make use of

 * ``zeros(Nt+1)`` for creating an array of a size ``Nt+1``
   and initializing the elements to zero

 * ``linspace(0, T, Nt+1)`` for creating an array with ``Nt+1`` coordinates uniformly
   distributed between ``0`` and ``T``

The ``for`` loop deserves a comment, especially for newcomers to Python.
The construction ``range(0, Nt, s)`` generates all integers from ``0`` to ``Nt``
in steps of ``s``, *but not including* ``Nt``. Omitting ``s`` means ``s=1``.
For example, ``range(0, 6, 3)``
gives ``0`` and ``3``, while ``range(0, Nt)`` generates ``0``, ``1``, ..., ``Nt-1``.
Our loop implies the following assignments to ``u[n+1]``: ``u[1]``, ``u[2]``, ...,
``u[Nt]``, which is what we want since ``u`` has length ``Nt+1``.
The first index in Python arrays or lists is *always* ``0`` and the
last is then ``len(u)-1``. The length of an array ``u`` is obtained by
``len(u)`` or ``u.size``.

To compute with the ``solver`` function, we need to *call* it. Here
is a sample call:

.. code-block:: python

        u, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)


Integer division
~~~~~~~~~~~~~~~~

The shown implementation of the ``solver`` may face problems and
wrong results if ``T``, ``a``, ``dt``, and ``theta`` are given as integers,
see :ref:`decay:exer:intdiv` and :ref:`decay:exer:decay1err`.
The problem is related to *integer division* in Python (as well as
in Fortran, C, C++, and many other computer languages): ``1/2`` becomes ``0``,
while ``1.0/2``, ``1/2.0``, or ``1.0/2.0`` all become ``0.5``. It is enough
that at least the nominator or the denominator is a real number
(i.e., a ``float`` object)
to ensure correct mathematical division. Inserting
a conversion ``dt = float(dt)``
guarantees that ``dt`` is
``float`` and avoids problems in :ref:`decay:exer:decay1err`.

Another problem with computing :math:`N_t=T/\Delta t` is that we should
round :math:`N_t` to the nearest integer. With ``Nt = int(T/dt)`` the ``int``
operation picks the largest integer smaller than ``T/dt``. Correct
mathematical rounding as known from school is obtained by

.. code-block:: python

        Nt = int(round(T/dt))

The complete version of our improved, safer ``solver`` function then becomes


.. code-block:: python

        from numpy import *
        
        def solver(I, a, T, dt, theta):
            """Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt."""
            dt = float(dt)            # avoid integer division
            Nt = int(round(T/dt))     # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
            return u, t



Doc strings
~~~~~~~~~~~

.. index:: doc strings


Right below the header line in the ``solver`` function there is a
Python string enclosed in triple double quotes ``"""``.
The purpose of this string object is to document what the function
does and what the arguments are. In this case the necessary
documentation do not span more than one line, but with triple double
quoted strings the text may span several lines:


.. code-block:: python

        def solver(I, a, T, dt, theta):
            """
            Solve
        
                u'(t) = -a*u(t),
        
            with initial condition u(0)=I, for t in the time interval
            (0,T]. The time interval is divided into time steps of
            length dt.
        
            theta=1 corresponds to the Backward Euler scheme, theta=0
            to the Forward Euler scheme, and theta=0.5 to the Crank-
            Nicolson method.
            """
            ...

Such documentation strings appearing right after the header of
a function are called *doc strings*. There are tools that can automatically
produce nicely formatted documentation by extracting the definition of
functions and the contents of doc strings.

It is strongly recommended to equip any function whose purpose
is not obvious with a doc string. Nevertheless, the forthcoming
text deviates from this rule if the function is explained in the text.


Formatting of numbers
~~~~~~~~~~~~~~~~~~~~~

Having computed the discrete solution ``u``, it is natural to look at
the numbers:

.. code-block:: python

        # Write out a table of t and u values:
        for i in range(len(t)):
            print t[i], u[i]

This compact ``print`` statement gives unfortunately quite ugly output
because the ``t`` and ``u`` values are not aligned in nicely formatted columns.
To fix this problem, we recommend to use the *printf format*, supported most
programming languages inherited from C. Another choice is
Python's recent *format string syntax*.


.. index:: printf format


Writing ``t[i]`` and ``u[i]`` in two nicely formatted columns is done like
this with the printf format:


.. code-block:: python

        print 't=%6.3f u=%g' % (t[i], u[i])

The percentage signs signify "slots" in the text where the variables
listed at the end of the statement are inserted. For each "slot" one
must specify a format for how the variable is going to appear in the
string: ``s`` for pure text, ``d`` for an integer, ``g`` for a real number
written as compactly as possible, ``9.3E`` for scientific notation with
three decimals in a field of width 9 characters (e.g., ``-1.351E-2``),
or ``.2f`` for standard decimal notation with two decimals
formatted with minimum width. The printf syntax provides a quick way
of formatting tabular output of numbers with full control of the
layout.


.. index:: format string syntax (Python)


The alternative *format string syntax* looks like

.. code-block:: python

        print 't={t:6.3f} u={u:g}'.format(t=t[i], u=u[i])

As seen, this format allows logical names in the "slots" where
``t[i]`` and ``u[i]`` are to be inserted. The "slots" are surrounded
by curly braces, and the logical name is followed by a colon and
then the printf-like specification of how to format real numbers,
integers, or strings.

Running the program
~~~~~~~~~~~~~~~~~~~

The function and main program shown above must be placed in a file,
say with name `decay_v1.py <http://tinyurl.com/jvzzcfn/decay/decay_v1.py>`_ (``v1`` stands for "version 1" - we shall make
numerous different versions of this program).  Make sure you
write the code with a suitable text editor (Gedit, Emacs, Vim,
Notepad++, or similar).  The program is run by executing the file this
way:


.. code-block:: console

        Terminal> python decay_v1.py

The text ``Terminal>`` just indicates a prompt in a
Unix/Linux or DOS terminal window. After this prompt, which will look
different in your terminal window, depending on the terminal application
and how it is set up, commands like ``python decay_v1.py`` can be issued.
These commands are interpreted by the operating system.

We strongly recommend to run Python programs within the IPython shell.
First start IPython by typing ``ipython`` in the terminal window.
Inside the IPython shell, our program ``decay_v1.py`` is run by the command
``run decay_v1.py``:


.. code-block:: console

        Terminal> ipython
        
        In [1]: run decay_v1.py
        t= 0.000 u=1
        t= 0.800 u=0.384615
        t= 1.600 u=0.147929
        t= 2.400 u=0.0568958
        t= 3.200 u=0.021883
        t= 4.000 u=0.00841653
        t= 4.800 u=0.00323713
        t= 5.600 u=0.00124505
        t= 6.400 u=0.000478865
        t= 7.200 u=0.000184179
        t= 8.000 u=7.0838e-05
        
        In [2]:


The advantage of running programs in IPython are many: previous commands
are easily recalled with the up arrow, ``%pdb`` turns on debugging so that
variables can be examined if the program
aborts due to an exception, output of commands are stored in variables,
programs and statements can be profiled,
any operating system command can be executed, modules can be loaded
automatically and other customizations can be performed when starting
IPython -- to mention a few of the most
useful features.

Although running programs in IPython is strongly recommended, most
execution examples in the forthcoming text use the standard
Python shell with prompt ``>>>`` and run programs through
a typesetting like


.. code-block:: console

        Terminal> python programname

The reason is that such typesetting
makes the text more compact in the vertical direction
than showing sessions with IPython syntax.

.. Explain running programs in IPython

.. Prompt: maybe just something with Unix to promote virtual Ubuntu


Verifying the implementation
----------------------------

It is easy to make mistakes while deriving and implementing numerical
algorithms, so we should never believe in the printed :math:`u` values before
they have been thoroughly verified. The most obvious idea is to compare
the computed solution with the exact solution, when that exists,
but there will always be a discrepancy between these two solutions
because of the numerical approximations. The challenging question is whether
we have the mathematically correct discrepancy or if we have another,
maybe small, discrepancy due to both an approximation error
and an error in the implementation.

The purpose of *verifying* a program is to bring evidence for the
property that there are no errors in the implementation. To avoid
mixing unavoidable approximation errors and undesired
implementation errors, we should
try to make tests where we have some exact computation of the
discrete solution or at least parts of it.
Examples will show how this can be done.

Running a few algorithmic steps by hand
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The simplest approach to produce a correct reference for the discrete
solution :math:`u` of finite difference equations is to compute a few
steps of the algorithm by hand. Then we can compare the hand
calculations with numbers produced by the program.

A straightforward approach is to use a calculator and
compute :math:`u^1`, :math:`u^2`, and :math:`u^3`. With :math:`I=0.1`, :math:`\theta=0.8`,
and :math:`\Delta t =0.8` we get


.. math::
         A\equiv \frac{1 - (1-\theta) a\Delta t}{1 + \theta a \Delta t} = 0.298245614035


.. math::
        
        u^1 &= AI=0.0298245614035,\\ 
        u^2 &= Au^1= 0.00889504462912,\\ 
        u^3 &=Au^2= 0.00265290804728
        


Comparison of these manual calculations with the result of the
``solver`` function is carried out in the function


.. code-block:: python

        def verify_three_steps():
            """Compare three steps with known manual computations."""
            theta = 0.8; a = 2; I = 0.1; dt = 0.8
            u_by_hand = array([I,
                               0.0298245614035,
                               0.00889504462912,
                               0.00265290804728])
        
            Nt = 3  # number of time steps
            u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)
        
            tol = 1E-15  # tolerance for comparing floats
            difference = abs(u - u_by_hand).max()
            success = difference <= tol
            return success


The main program, where we call the ``solver`` function and print ``u``,
is now put in a separate function ``main``:


.. code-block:: python

        def main():
            u, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)
            # Write out a table of t and u values:
            for i in range(len(t)):
                print 't=%6.3f u=%g' % (t[i], u[i])
                # or print 't={t:6.3f} u={u:g}'.format(t=t[i], u=u[i])


The main program in the file may now first run the verification test
and then go on with the real simulation (``main()``) only if the test is passed:


.. code-block:: python

        if verify_three_steps():
            main()
        else:
            print 'Bug in the implementation!'


Since the verification test is always done, future errors introduced
accidentally in the program have a good chance of being detected.

.. Note: the admon function needs a raw string to handle \theta (\t, \b, etc)




.. admonition:: Caution: choice of parameter values

   For the choice of values of parameters in verification tests one should
   stay away from integers, especially 0 and 1, as these can
   simplify formulas too much for test purposes. For example, with
   :math:`\theta =1` the nominator in the formula for :math:`u^n` will be the same for
   all :math:`a` and :math:`\Delta t` values. One should therefore choose more
   "arbitrary" values, say :math:`\theta =0.8` and :math:`I=0.1`.


It is essential that verification tests can be automatically run
at *any* time. For this purpose,
there are test frameworks and corresponding programming
rules that allow us to request running through a suite of test cases
(see the section :ref:`decay:prog:se:nose`),
but in this very early stage of program development we just implement
and run the verification in our own code so that every detail is
visible and understood.

The complete program including the ``verify_three_steps*`` functions is
found in the file `decay_verf1.py <http://tinyurl.com/jvzzcfn/decay/decay_verf1.py>`_ (``verf1`` is a short name for "verification,
version 1").

Comparison with an exact discrete solution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Sometimes it is possible to find a closed-form
*exact discrete solution* that fulfills the discrete finite
difference equations. The implementation can then be verified against
the exact discrete solution. This is usually the best technique for
verification.

.. Not so limited, will later guess that linear functions and MMS can

.. be used in the discrete eqs as well!


Define

.. math::
         A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a \Delta t}\thinspace . 

Manual computations with the :math:`\theta`-rule results in

.. math::
        
        u^0 &= I,\\ 
        u^1 &= Au^0 = AI,\\ 
        u^2 &= Au^1 = A^2I,\\ 
        &\vdots\\ 
        u^n &= A^nu^{n-1} = A^nI \thinspace .
        

We have then established the exact discrete solution as

.. math::
   :label: decay:un:exact
        
        u^n = IA^n
        
        \thinspace .
        




.. admonition:: Caution

   One should be conscious about the different meanings of the notation
   on the left- and right-hand side
   of :eq:`decay:un:exact`: on the left, :math:`n` in :math:`u^n`
   is a superscript reflecting a counter
   of mesh points (:math:`t_n`), while on the right, :math:`n`
   is the power in the exponentiation :math:`A^n`.


Comparison of the exact discrete solution and the computed
solution is done in the following function:


.. code-block:: python

        def verify_exact_discrete_solution():
        
            def exact_discrete_solution(n, I, a, theta, dt):
                A = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
                return I*A**n
        
            theta = 0.8; a = 2; I = 0.1; dt = 0.8
            Nt = int(8/dt)  # no of steps
            u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)
            u_de = array([exact_discrete_solution(n, I, a, theta, dt)
                          for n in range(Nt+1)])
            difference = abs(u_de - u).max()  # max deviation
            tol = 1E-15  # tolerance for comparing floats
            success = difference <= tol
            return success

The complete program is found in the file `decay_verf2.py <http://tinyurl.com/jvzzcfn/decay/decay_verf2.py>`_ (``verf2`` is a short name for "verification,
version 2").



.. admonition:: Local functions

   One can define a function inside another function, here called
   a *local function* (also known as *closure*) inside a *parent function*.
   A local function is invisible outside the parent function.
   A convenient property is that any local function has access to all
   variables defined in the parent function, also if we send the
   local function to some other function as argument (!).
   In the present example, it means that the local function
   ``exact_discrete_solution`` does not need its five arguments as the
   values can alternatively be accessed through the local variables defined
   in the parent function ``verify_exact_discrete_solution``. We can send
   such an ``exact_discrete_solution`` without arguments to any other
   function and ``exact_discrete_solution`` will still have access to
   ``n``, ``I``, ``a``, and so forth defined in its parent function.


.. _decay:computing:error:

Computing the numerical error as a mesh function
------------------------------------------------

Now that we have evidence for a correct implementation, we are in a
position to compare the computed :math:`u^n` values in the ``u`` array with
the exact :math:`u` values at the mesh points, in order to study the error
in the numerical solution.

Let us first make a function for the analytical solution :math:`u_{\small\mbox{e}}(t)=Ie^{-at}`
of the model problem:


.. code-block:: python

        def exact_solution(t, I, a):
            return I*exp(-a*t)



.. index:: representative (mesh function)


A natural way to compare the exact and discrete solutions is to
calculate their difference as a mesh function:


.. math::
        
        e^n = u_{\small\mbox{e}}(t_n) - u^n,\quad n=0,1,\ldots,N_t \thinspace .
        

We may view :math:`u_{\small\mbox{e}}^n = u_{\small\mbox{e}}(t_n)` as the representation of :math:`u_{\small\mbox{e}}(t)`
as a mesh function rather than a continuous function defined for all
:math:`t\in [0,T]` (:math:`u_{\small\mbox{e}}^n` is often called the *representative* of
:math:`u_{\small\mbox{e}}` on the mesh). Then, :math:`e^n = u_{\small\mbox{e}}^n - u^n` is clearly
the difference of two mesh functions. This interpretation of :math:`e^n`
is natural when programming.

The error mesh function :math:`e^n` can be computed by


.. code-block:: python

        u, t = solver(I, a, T, dt, theta)  # Numerical sol.
        u_e = exact_solution(t, I, a)      # Representative of exact sol.
        e = u_e - u

Note that the mesh functions ``u`` and ``u_e`` are represented by arrays
and associated with the points in the array ``t``.


.. index:: array arithmetics

.. index:: array computing




.. admonition:: Array arithmetics

   The last statements
   
   
   .. code-block:: python
   
           u_e = exact_solution(t, I, a)
           e = u_e - u
   
   are primary examples of array arithmetics: ``t`` is an
   array of mesh points that we pass to ``exact_solution``. This function
   evaluates ``-a*t``, which is a scalar times an array, meaning that
   the scalar is multiplied with each array element.
   The result is an array, let us call it ``tmp1``. Then
   ``exp(tmp1)`` means applying the exponential function to each element in
   ``tmp``, resulting an array, say ``tmp2``. Finally, ``I*tmp2`` is computed
   (scalar times array) and ``u_e`` refers to this array returned from
   ``exact_solution``. The expression ``u_e - u`` is the difference between
   two arrays, resulting in a new array referred to by ``e``.


.. _decay:computing:error:norm:

Computing the norm of the numerical error
-----------------------------------------


.. index:: continuous function norms


.. index::
   single: norm; continuous


Instead of working with the error :math:`e^n` on the entire mesh, we
often want one number expressing the size of the error.
This is obtained by taking the norm of the error function.

Let us first define norms of a function :math:`f(t)`
defined for all :math:`t\in [0,T]`.
Three common norms are


.. math::
   :label: decay:norms:L2
        
        ||f||_{L^2} = \left( \int_0^T f(t)^2 dt\right)^{1/2},
        
        



.. math::
   :label: decay:norms:L1
          
        ||f||_{L^1} = \int_0^T |f(t)| dt,
        
        



.. math::
   :label: decay:norms:Linf
          
        ||f||_{L^\infty} = \max_{t\in [0,T]}|f(t)|{\thinspace .}
        
        

The :math:`L^2` norm :eq:`decay:norms:L2` ("L-two norm")
has nice mathematical properties and
is the most popular norm. It is a generalization
of the well-known Eucledian norm of vectors to functions.
The :math:`L^\infty` is also called the max norm or the supremum norm.
In fact, there is a whole family of norms,


.. math::
        
        ||f||_{L^p} = \left(\int_0^T f(t)^pdt\right)^{1/p},
        

with :math:`p` real. In particular,
:math:`p=1` corresponds to the :math:`L^1` norm above while :math:`p=\infty` is the
:math:`L^\infty` norm.


.. index:: discrete function norms


.. index:: mesh function norms


.. index::
   single: norm; discrete (mesh function)


Numerical computations involving mesh functions need corresponding norms.
Given a set of function values, :math:`f^n`, and some associated mesh points, :math:`t_n`,
a numerical integration rule can be used to calculate the :math:`L^2` and
:math:`L^1` norms defined above. Imagining that the mesh function is extended
to vary linearly between the mesh points, the Trapezoidal rule is
in fact an exact integration rule. A possible modification of the :math:`L^2`
norm for a mesh function :math:`f^n` on a uniform mesh with spacing :math:`\Delta t`
is therefore the well-known Trapezoidal integration formula


.. math::
         ||f^n|| = \left(\Delta t\left(\frac{1}{2}(f^0)^2 + \frac{1}{2}(f^{N_t})^2
        + \sum_{n=1}^{N_t-1} (f^n)^2\right)\right)^{1/2} 

A common approximation of this expression, motivated by the
convenience of having a simpler formula, is


.. math::
         ||f^n||_{\ell^2} = \left(\Delta t\sum_{n=0}^{N_t} (f^n)^2\right)^{1/2} {\thinspace .}

This is called the discrete :math:`L^2` norm and denoted by :math:`\ell^2`.
The error in :math:`||f||_{\ell^2}^2` compared with the Trapezoidal
integration formula
is :math:`\Delta t((f^0)^2 + (f^{N_t})^2)/2`, which means perturbed weights
at the end points of the mesh function, and the error goes to zero as
:math:`\Delta t\rightarrow 0`. As long as we are consistent and
stick to one kind of integration
rule for the norm of a mesh function, the details and accuracy of this
rule is not of concern.

The three discrete norms for a mesh function :math:`f^n`, corresponding to
the :math:`L^2`, :math:`L^1`, and :math:`L^\infty` norms of :math:`f(t)` defined above, are
defined by


.. math::
   :label: decay:norms:l2
        
        ||f^n||_{\ell^2}  \left( \Delta t\sum_{n=0}^{N_t} (f^n)^2\right)^{1/2},
        
        



.. math::
   :label: decay:norms:l1
          
        ||f^n||_{\ell^1}  \Delta t\sum_{n=0}^{N_t} |f^n|
        
        



.. math::
   :label: decay:norms:linf
          
        ||f^n||_{\ell^\infty}  \max_{0\leq n\leq N_t}|f^n|{\thinspace .}
        
        


Note that the :math:`L^2`, :math:`L^1`, :math:`\ell^2`, and :math:`\ell^1` norms depend on the
length of the interval of interest (think of :math:`f=1`, then the
norms are proportional to :math:`\sqrt{T}` or :math:`T`). In some applications it
is convenient to think of a mesh function as just a vector of function
values and neglect the information of the mesh points. Then we can
replace :math:`\Delta t` by :math:`T/N_t` and drop :math:`T`. Moreover, it is convenient
to divide by the total length of the vector, :math:`N_t+1`, instead of :math:`N_t`.
This reasoning gives rise to the *vector norms* for a vector
:math:`f=(f_0,\ldots,f_{N})`:


.. math::
   :label: decay:norms:vl2
        
        ||f||_2 = \left( \frac{1}{N+1}\sum_{n=0}^{N} (f_n)^2\right)^{1/2},
        
        



.. math::
   :label: decay:norms:vl1
          
        ||f||_1 = \frac{1}{N+1}\sum_{n=0}^{N} |f_n|
        
        



.. math::
   :label: decay:norms:vlinf
          
        ||f||_{\ell^\infty} = \max_{0\leq n\leq N}|f_n|{\thinspace .}
        
        

Here we have used the common vector component notation with subscripts
(:math:`f_n`) and :math:`N` as length. We will mostly work with mesh functions
and use the discrete :math:`\ell^2`
norm :eq:`decay:norms:l2` or the max norm :math:`\ell^\infty`
:eq:`decay:norms:linf`, but the corresponding vector norms
:eq:`decay:norms:vl2`-:eq:`decay:norms:vlinf` are also much used
in numerical computations, so it is important to know the different
norms and the relations between them.

A single number that expresses the size of the numerical error
will be taken as :math:`||e^n||_{\ell^2}` and called :math:`E`:


.. math::
   :label: decay:E
        
        E = \sqrt{\Delta t\sum_{n=0}^{N_t} (e^n)^2}
        
        

The corresponding Python code, using array arithmetics, reads


.. code-block:: python

        E = sqrt(dt*sum(e**2))

The ``sum`` function comes from ``numpy`` and computes the sum of the elements
of an array. Also the ``sqrt`` function is from ``numpy`` and computes the
square root of each element in the array argument.


.. index:: scalar computing


Scalar computing
~~~~~~~~~~~~~~~~

Instead of doing array computing ``sqrt(dt*sum(e**2))`` we can compute with
one element at a time:

.. code-block:: python

        m = len(u)     # length of u array (alt: u.size)
        u_e = zeros(m)
        t = 0
        for i in range(m):
            u_e[i] = exact_solution(t, a, I)
            t = t + dt
        e = zeros(m)
        for i in range(m):
            e[i] = u_e[i] - u[i]
        s = 0  # summation variable
        for i in range(m):
            s = s + e[i]**2
        error = sqrt(dt*s)

Such element-wise computing, often called *scalar* computing, takes
more code, is less readable, and runs much slower than what we
can achieve with array computing.


.. _decay:plotting:

Plotting solutions
------------------

Having the ``t`` and ``u`` arrays, the approximate solution ``u`` is visualized
by the intuitive command ``plot(t, u)``:


.. code-block:: python

        from matplotlib.pyplot import *
        plot(t, u)
        show()


Plotting multiple curves
~~~~~~~~~~~~~~~~~~~~~~~~

It will be illustrative to also plot :math:`u_{\small\mbox{e}}(t)` for comparison. Doing a
``plot(t, u_e)`` is not exactly what we want: the ``plot`` function draws
straight lines between the discrete points ``(t[n], u_e[n])`` while
:math:`u_{\small\mbox{e}}(t)` varies as an exponential function between the mesh points.
The technique for showing the "exact" variation of :math:`u_{\small\mbox{e}}(t)` between
the mesh points is to introduce a very fine mesh for :math:`u_{\small\mbox{e}}(t)`:


.. code-block:: python

        t_e = linspace(0, T, 1001)      # fine mesh
        u_e = exact_solution(t_e, I, a)
        plot(t_e, u_e, 'b-')            # blue line for u_e
        plot(t,   u,   'r--o')          # red dashes w/circles


With more than one curve in the plot we need to associate each curve
with a legend. We also want appropriate names on the axis, a title,
and a file containing the plot as an image for inclusion in reports.
The Matplotlib package (``matplotlib.pyplot``) contains functions for
this purpose. The names of the functions are similar to the plotting
functions known from MATLAB.  A complete plot session then becomes


.. code-block:: python

        from matplotlib.pyplot import *
        
        figure()                          # create new plot
        t_e = linspace(0, T, 1001)        # fine mesh for u_e
        u_e = exact_solution(t_e, I, a)
        plot(t,   u,   'r--o')            # red dashes w/circles
        plot(t_e, u_e, 'b-')              # blue line for exact sol.
        legend(['numerical', 'exact'])
        xlabel('t')
        ylabel('u')
        title('theta=%g, dt=%g' % (theta, dt))
        savefig('%s_%g.png' % (theta, dt))
        show()

Note that ``savefig`` here creates a PNG file whose name reflects the
values of :math:`\theta` and :math:`\Delta t` so that we can easily distinguish
files from different runs with :math:`\theta` and :math:`\Delta t`.

A bit more sophisticated and easy-to-read filename can be generated
by mapping the :math:`\theta` value to acronyms for the three common
schemes: FE (Forward Euler, :math:`\theta=0`), BE (Backward Euler, :math:`\theta=1`),
CN (Crank-Nicolson, :math:`\theta=0.5`). A Python dictionary is ideal for such
a mapping from numbers to strings:


.. code-block:: python

        theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
        savefig('%s_%g.png' % (theta2name[theta], dt))


Experiments with computing and plotting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us wrap up the computation of the error measure and all the
plotting statements in a function ``explore``. This function
can be called for various :math:`\theta` and :math:`\Delta t` values
to see how the error varies with the method and the mesh resolution:


.. code-block:: python

        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            """
            Run a case with the solver, compute error measure,
            and plot the numerical and exact solutions (if makeplot=True).
            """
            u, t = solver(I, a, T, dt, theta)    # Numerical solution
            u_e = exact_solution(t, I, a)
            e = u_e - u
            E = sqrt(dt*sum(e**2))
            if makeplot:
                figure()                         # create new plot
                t_e = linspace(0, T, 1001)       # fine mesh for u_e
                u_e = exact_solution(t_e, I, a)
                plot(t,   u,   'r--o')           # red dashes w/circles
                plot(t_e, u_e, 'b-')             # blue line for exact sol.
                legend(['numerical', 'exact'])
                xlabel('t')
                ylabel('u')
                title('theta=%g, dt=%g' % (theta, dt))
                theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
                savefig('%s_%g.png' % (theta2name[theta], dt))
                savefig('%s_%g.pdf' % (theta2name[theta], dt))
                savefig('%s_%g.eps' % (theta2name[theta], dt))
                show()
            return E


The ``figure()`` call is key here: without it, a new ``plot`` command will
draw the new pair of curves in the same plot window, while we want
the different pairs to appear in separate windows and files.
Calling ``figure()`` ensures this.


.. index:: PNG plot

.. index:: PDF plot

.. index:: EPS plot


.. index:: viewing graphics files


The ``explore`` function stores the plot in three different image file formats:
PNG, PDF, and EPS (Encapsulated PostScript). The PNG format is aimed
at being included in HTML files, the PDF format in pdfLaTeX documents,
and the EPS format in LaTeX documents. Frequently used viewers for these
image files on Unix systems are ``gv`` (comes with Ghostscript)
for the PDF and EPS formats and
``display`` (from the ImageMagick) suite for PNG files:


.. code-block:: console

        Terminal> gv BE_0.5.pdf
        Terminal> gv BE_0.5.eps
        Terminal> display BE_0.5.png


The complete code containing the functions above
resides in the file `decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`_.
Running this program results in

.. code-block:: console

        Terminal> python decay_plot_mpl.py
        0.0   0.40:    2.105E-01
        0.0   0.04:    1.449E-02
        0.5   0.40:    3.362E-02
        0.5   0.04:    1.887E-04
        1.0   0.40:    1.030E-01
        1.0   0.04:    1.382E-02

We observe that reducing :math:`\Delta t` by a factor of 10 increases the
accuracy for all three methods (:math:`\theta` values). We also see that
the combination of :math:`\theta=0.5` and a small time step :math:`\Delta t =0.04`
gives a much more accurate solution, and that :math:`\theta=0` and :math:`\theta=0`
with :math:`\Delta t = 0.4` result in the least accurate solutions.

Figure :ref:`decay:fig:FE1` demonstrates that the numerical solution for
:math:`\Delta t=0.4` clearly lies below the exact curve, but that the
accuracy improves considerably by reducing the time step by a factor
of 10.


.. _decay:fig:FE1:

.. figure:: FE1.png
   :width: 600

   *The Forward Euler scheme for two values of the time step*



.. index:: cropping images


.. index:: montage program


Combining plot files  (1)
~~~~~~~~~~~~~~~~~~~~~~~~~

Mounting two PNG files, as done in the figure, is easily done by the
`montage <http://www.imagemagick.org/script/montage.php>`_ program
from the ImageMagick suite:


.. code-block:: console

        Terminal> montage -background white -geometry 100% -tile 2x1 \ 
                  FE_0.4.png FE_0.04.png FE1.png
        Terminal> convert -trim FE1.png FE1.png

The ``-geometry`` argument is used to specify the size of the image, and here
we preserve the individual sizes of the images. The ``-tile HxV`` option
specifies ``H`` images in the horizontal direction and ``V`` images in
the vertical direction. A series of image files to be combined are then listed,
with the name of the resulting combined image, here ``FE1.png`` at the end.
The ``convert -trim`` command removes surrounding white areas in the figure
(an operation usually known as *cropping* in image manipulation programs).


.. index:: pdftk program


.. index:: pdfnup program


.. index:: pdfcrop program


For \LaTeX{} reports it is not recommended to use ``montage`` and PNG files
as the result has too low resolution. Instead, plots should be made
in the PDF format and combined using the ``pdftk``, ``pdfnup``, and ``pdfcrop`` tools
(on Linux/Unix):


.. code-block:: console

        Terminal> pdftk FE_0.4.png FE_0.04.png output tmp.pdf
        Terminal> pdfnup --nup 2x1 tmp.pdf     # output in tmp-nup.pdf
        Terminal> pdfcrop tmp-nup.pdf FE1.png  # output in FE1.png

Here, ``pdftk`` combines images into a multi-page PDF file, ``pdfnup``
combines the images in individual pages to a table of images (pages),
and ``pdfcrop`` removes white margins in the resulting combined image file.

The behavior of the two other schemes is shown in Figures :ref:`decay:fig:BE1`
and :ref:`decay:fig:CN1`. Crank-Nicolson is obviously the most accurate
scheme from this visual point of view.


.. _decay:fig:BE1:

.. figure:: BE1.png
   :width: 600

   *The Backward Euler scheme for two values of the time step*



.. _decay:fig:CN1:

.. figure:: CN1.png
   :width: 600

   *The Crank-Nicolson scheme for two values of the time step*



Plotting with SciTools
~~~~~~~~~~~~~~~~~~~~~~

The `SciTools package <http://code.google.com/p/scitools>`_ provides a
unified plotting interface, called Easyviz, to many different plotting
packages, including Matplotlib, Gnuplot, Grace, MATLAB,
VTK, OpenDX, and VisIt. The syntax is very similar to that of
Matplotlib and MATLAB. In fact, the plotting commands shown above look
the same in SciTool's Easyviz interface, apart from the import
statement, which reads


.. code-block:: python

        from scitools.std import *

This statement performs a ``from numpy import *`` as well as an import
of the most common pieces of the Easyviz (``scitools.easyviz``) package,
along with some additional numerical functionality.

With Easyviz one can
merge several plotting commands into a single one
using keyword arguments:


.. code-block:: python

        plot(t,   u,   'r--o',           # red dashes w/circles
             t_e, u_e, 'b-',             # blue line for exact sol.
             legend=['numerical', 'exact'],
             xlabel='t',
             ylabel='u',
             title='theta=%g, dt=%g' % (theta, dt),
             savefig='%s_%g.png' % (theta2name[theta], dt),
             show=True)

The `decay_plot_st.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_st.py>`_ file
contains such a demo.

By default, Easyviz employs Matplotlib for plotting, but `Gnuplot <http://www.gnuplot.info/>`_ and `Grace <http://plasma-gate.weizmann.ac.il/Grace/>`_ are viable alternatives:


.. code-block:: console

        Terminal> python decay_plot_st.py --SCITOOLS_easyviz_backend gnuplot
        Terminal> python decay_plot_st.py --SCITOOLS_easyviz_backend grace

The backend used for creating plots (and numerous other options)
can be permanently set in SciTool's configuration file.

All the Gnuplot windows are launched without any need to kill one before
the next one pops up (as is the case with Matplotlib) and one can
press the key 'q' anywhere in a plot window to kill it.
Another advantage of Gnuplot is the automatic choice of sensible
and distinguishable line types in black-and-white PDF and PostScript
files.

Regarding functionality for annotating plots with title, labels on the
axis, legends, etc., we refer to the documentation of Matplotlib and
SciTools for more detailed information on the syntax. The hope is that
the programming syntax explained so far suffices for understanding the
code and learning more from a combination of the forthcoming examples
and other resources such as books and web pages.



.. admonition:: Test the understanding

   :ref:`decay:app:exer:cooling:py` asks you to implement
   a solver for a problem that is slightly different from the
   one above. You may use the ``solver`` and ``explore`` functions
   explained above as a starting point. Apply the new solver
   to :ref:`decay:app:exer:cooling:murder`.


Creating command-line interfaces
--------------------------------


.. index:: user interfaces to programs


.. index:: command-line arguments


It is good programming practice to let programs read input from the user
rather than require the user to edit the source code when trying out
new values of input parameters.
Reading input from the command line is a simple and flexible way of interacting
with the user. Python stores all the command-line arguments in
the list ``sys.argv``, and there are, in principle, two ways of programming with
command-line arguments in Python:

 * Decide upon a sequence of parameters on the command line and read
   their values directly from the ``sys.argv[1:]`` list (``sys.argv[0]`` is
   the just program name).

 * Use option-value pairs (``--option value``) on
   the command line to override default values of input parameters,
   and utilize the ``argparse.ArgumentParser`` tool to interact with
   the command line.

Both strategies will be illustrated next.

Reading a sequence of command-line arguments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: reading the command line


The `decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`_
program needs the following input data: :math:`I`, :math:`a`, :math:`T`, an option to
turn the plot on or off (``makeplot``), and a list of :math:`\Delta t` values.

The simplest way of reading this input from the command line is to say
that the first four command-line arguments correspond to the first
four points in the list above, in that order, and that the rest of the
command-line arguments are the :math:`\Delta t` values.  The input given for
``makeplot`` can be a string among ``'on'``, ``'off'``, ``'True'``, and
``'False'``. The code for reading this input is most conveniently put in
a function:


.. code-block:: python

        import sys
        
        def read_command_line():
            if len(sys.argv) < 6:
                print 'Usage: %s I a T on/off dt1 dt2 dt3 ...' % \ 
                      sys.argv[0]; sys.exit(1)  # abort
        
            I = float(sys.argv[1])
            a = float(sys.argv[2])
            T = float(sys.argv[3])
            makeplot = sys.argv[4] in ('on', 'True')
            dt_values = [float(arg) for arg in sys.argv[5:]]
        
            return I, a, T, makeplot, dt_values



.. index:: list comprehension


.. index:: sys.argv


One should note the following about the constructions in the program above:

  * Everything on the command line ends up in a *string* in
    the list ``sys.argv``. Explicit conversion to, e.g., a ``float`` object is
    required if the string as a number we want to compute with.

  * The value of ``makeplot`` is determined from a boolean expression,
    which becomes ``True`` if the command-line argument is either ``'on'`` or
    ``'True'``, and ``False`` otherwise.

  * It is easy to build the list of :math:`\Delta t` values: we simply run through
    the rest of the list, ``sys.argv[5:]``, convert each command-line argument
    to ``float``, and collect these ``float`` objects in a list, using the
    compact and convenient *list comprehension* syntax in Python.

The loops over :math:`\theta` and :math:`\Delta t` values can be coded in a ``main`` function:


.. code-block:: python

        def main():
            I, a, T, makeplot, dt_values = read_command_line()
            for theta in 0, 0.5, 1:
                for dt in dt_values:
                    E = explore(I, a, T, dt, theta, makeplot)
                    print '%3.1f %6.2f: %12.3E' % (theta, dt, E)

The complete program can be found in `decay_cml.py <http://tinyurl.com/jvzzcfn/decay/decay_cml.py>`_.

Working with an argument parser
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: argparse (Python module)


.. index:: ArgumentParser (Python class)


.. index:: option-value pairs (command line)


.. index:: command-line options and values


.. index:: reading the command line


Python's ``ArgumentParser`` tool in the ``argparse`` module makes it
easy to create a professional command-line interface to any
program. The documentation of `ArgumentParser <http://docs.python.org/library/argparse.html>`_ demonstrates its
versatile applications, so we shall here just list an example
containing basic features.  On the command line we want to
specify option-value pairs for :math:`I`, :math:`a`, and :math:`T`, e.g., ``--a 3.5 --I 2
--T 2``. Including ``--makeplot`` turns the plot on and excluding this
option turns the plot off.  The :math:`\Delta t` values can be given as
``--dt 1 0.5 0.25 0.1 0.01``.  Each parameter must have a sensible
default value so that we specify the option on the command line only
when the default value is not suitable.

We introduce a function for defining the mentioned command-line options:


.. code-block:: python

        def define_command_line_options():
            import argparse
            parser = argparse.ArgumentParser()
            parser.add_argument('--I', '--initial_condition', type=float,
                                default=1.0, help='initial condition, u(0)',
                                metavar='I')
            parser.add_argument('--a', type=float,
                                default=1.0, help='coefficient in ODE',
                                metavar='a')
            parser.add_argument('--T', '--stop_time', type=float,
                                default=1.0, help='end time of simulation',
                                metavar='T')
            parser.add_argument('--makeplot', action='store_true',
                                help='display plot or not')
            parser.add_argument('--dt', '--time_step_values', type=float,
                                default=[1.0], help='time step values',
                                metavar='dt', nargs='+', dest='dt_values')
            return parser


Each command-line option is defined through the ``parser.add_argument``
method. Alternative options, like the short ``--I`` and the more
explaining version ``--initial_condition`` can be defined. Other arguments
are ``type`` for the Python object type, a default value, and a help
string, which gets printed if the command-line argument ``-h`` or ``--help`` is
included. The ``metavar`` argument specifies the value associated with
the option when the help string is printed. For example, the option for
:math:`I` has this help output:


.. code-block:: console

        Terminal> python decay_argparse.py -h
          ...
          --I I, --initial_condition I
                                initial condition, u(0)
          ...

The structure of this output is


.. code-block:: text


          --I metavar, --initial_condition metavar
                                help-string


The ``--makeplot`` option is a pure flag without any value, implying a
true value if the flag is present and otherwise a false value. The
``action='store_true'`` makes an option for such a flag.

Finally, the ``--dt`` option demonstrates how to allow for more than one
value (separated by blanks) through the ``nargs='+'`` keyword argument.
After the command line is parsed, we get an object where the values of
the options are stored as attributes. The attribute name is specified
by the ``dist`` keyword argument, which for the ``--dt`` option is
``dt_values``. Without the ``dest`` argument, the value of an option ``--opt``
is stored as the attribute ``opt``.

The code below demonstrates how to read the command line and extract
the values for each option:


.. code-block:: python

        def read_command_line():
            parser = define_command_line_options()
            args = parser.parse_args()
            print 'I={}, a={}, T={}, makeplot={}, dt_values={}'.format(
                args.I, args.a, args.T, args.makeplot, args.dt_values)
            return args.I, args.a, args.T, args.makeplot, args.dt_values


The ``main`` function remains the same as in the ``decay_cml.py`` code based
on reading from ``sys.argv`` directly. A complete program featuring the
demo above of ``ArgumentParser`` appears in the file `decay_argparse.py <http://tinyurl.com/jvzzcfn/decay/decay_argparse.py>`_.

Creating a graphical web user interface
---------------------------------------

The Python package `Parampool <https://github.com/hplgit/parampool>`_
can be used to automatically generate a web-based graphical user interface
(GUI) for our simulation program.

Making a compute function
~~~~~~~~~~~~~~~~~~~~~~~~~

The first step is to identify a function
that performs the computations and that takes the necessary input
variables as arguments. This is called the *compute function* in
Parampool terminology. We may start with a copy of the basic file
`decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`_,
which has a ``main`` function displayed in
the section :ref:`decay:plotting` for carrying out simulations and plotting
for a series of :math:`\Delta t` values. Now we want to control and view the same
experiments from a web GUI.

To tell Parampool what type of input data we have,
we assign default values of the right type to all arguments in the
main function and call it ``main_GUI``:


.. code-block:: python

        def main_GUI(I=1.0, a=.2, T=4.0,
                 dt_values=[1.25, 0.75, 0.5, 0.1],
                 theta_values=[0, 0.5, 1]):


The compute function must return the HTML code we want for displaying
the result in a web page. Here we want to show plots of the numerical
and exact solution for different methods and :math:`\Delta t` values.
The plots can be organized in a table with :math:`\theta` (methods) varying
through the columns and :math:`\Delta t` varying through the rows.
Assume now that a new version of the ``explore`` function
not only returns the error ``E`` but also HTML code containing the
plot. Then we can write the ``main_GUI`` function as


.. code-block:: python

        def main_GUI(I=1.0, a=.2, T=4.0,
                 dt_values=[1.25, 0.75, 0.5, 0.1],
                 theta_values=[0, 0.5, 1]):
            # Build HTML code for web page. Arrange plots in columns
            # corresponding to the theta values, with dt down the rows
            theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
            html_text = '<table>\n'
            for dt in dt_values:
                html_text += '<tr>\n'
                for theta in theta_values:
                    E, html = explore(I, a, T, dt, theta, makeplot=True)
                    html_text += """
        <td>
        <center><b>%s, dt=%g, error: %s</b></center><br>
        %s
        </td>
        """ % (theta2name[theta], dt, E, html)
                html_text += '</tr>\n'
            html_text += '</table>\n'
            return html_text


Rather than creating plot files and showing the plot on the screen,
the new version of the ``explore`` function makes a string with the PNG code of
the plot and embeds that string in HTML code. This action is
conveniently performed by Parampool's ``save_png_to_str`` function:


.. code-block:: python

        import matplotlib.pyplot as plt
        ...
        # plot
        plt.plot(t, u, r-')
        plt.xlabel('t')
        plt.ylabel('u')
        ...
        from parampool.utils import save_png_to_str
        html_text = save_png_to_str(plt, plotwidth=400)

Note that we now write ``plt.plot``, ``plt.xlabel``, etc.
The ``html_text`` string is long and contains all the characters that
build up the PNG file of the current plot. The new ``explore``
function can make use of the above code snippet and return
``html_text`` along with ``E``.

Generating the user interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The web GUI is automatically generated by
the following code, placed in a file `decay_GUI_generate.py <http://tinyurl.com/jvzzcfn/decay/decay_GUI_generate.py>`_


.. code-block:: python

        from parampool.generator.flask import generate
        from decay_GUI import main
        generate(main,
                 output_controller='decay_GUI_controller.py',
                 output_template='decay_GUI_view.py',
                 output_model='decay_GUI_model.py')

Running the ``decay_GUI_generate.py`` program results in three new
files whose names are specified in the call to ``generate``:

 1. ``decay_GUI_model.py`` defines HTML widgets to be used to set
    input data in the web interface,

 2. ``templates/decay_GUI_views.py`` defines the layout of the web page,

 3. ``decay_GUI_controller.py`` runs the web application.

We only need to run the last program, and there is no need to look into
these files.

Running the web application
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The web GUI is started by


.. code-block:: console

        Terminal> python decay_GUI_controller.py

Open a web browser at the location ``127.0.0.1:5000``. Input fields for
``I``, ``a``, ``T``, ``dt_values``, and ``theta_values`` are presented.
Setting the latter two to ``[1.25, 0.5]`` and ``[1, 0.5]``,
respectively, and pressing *Compute* results in four plots, see
Figure :ref:`decay:fig:GUI`. We the techniques demonstrated here, one
can easily create a tailored web GUI for a particular type of application
and use it to interactively explore physical and numerical effects.


.. _decay:fig:GUI:

.. figure:: decay_GUI.png
   :width: 800

   *Automatically generated graphical web interface*



.. _decay:convergence:rate:

Computing convergence rates
---------------------------


.. index:: convergence rate


We expect that the error :math:`E` in the numerical solution is
reduced if the mesh size :math:`\Delta t` is decreased. More specifically,
many numerical methods obey a power-law relation between :math:`E` and
:math:`\Delta t`:


.. math::
   :label: decay:E:dt
        
        E = C\Delta t^r,
        
        

where :math:`C` and :math:`r` are (usually unknown) constants independent of :math:`\Delta t`.
The formula :eq:`decay:E:dt` is viewed as an asymptotic model valid for
sufficiently small :math:`\Delta t`. How small is normally hard to estimate
without doing numerical estimations of :math:`r`.

The parameter :math:`r` is known as the *convergence rate*. For example,
if the convergence rate is 2, halving :math:`\Delta t` reduces the error by
a factor of 4. Diminishing :math:`\Delta t` then has a greater impact on
the error compared with methods that have :math:`r=1`. For a given value of :math:`r`,
we refer to the method as of :math:`r`-th order. First- and second-order
methods are most common in scientific computing.

Estimating :math:`r`
~~~~~~~~~~~~~~~~~~~~

There are two alternative ways of estimating :math:`C` and :math:`r` based on a set of
:math:`m` simulations with corresponding pairs :math:`(\Delta t_i, E_i)`, :math:`i=0,\ldots,m-1`,
and :math:`\Delta t_{i} < \Delta t_{i-1}` (i.e., decreasing cell size).

 1. Take the logarithm of :eq:`decay:E:dt`, :math:`\ln E = r\ln \Delta t + \ln C`,
    and fit a straight line to the data points :math:`(\Delta t_i, E_i)`,
    :math:`i=0,\ldots,m-1`.

 2. Consider two consecutive experiments, :math:`(\Delta t_i, E_i)` and
    :math:`(\Delta t_{i-1}, E_{i-1})`. Dividing the equation
    :math:`E_{i-1}=C\Delta t_{i-1}^r` by :math:`E_{i}=C\Delta t_{i}^r` and solving
    for :math:`r` yields


.. math::
   :label: decay:conv:rate
        
        r_{i-1} = \frac{\ln (E_{i-1}/E_i)}{\ln (\Delta t_{i-1}/\Delta t_i)}
        
        

for :math:`i=1,\ldots,m-1`.

The disadvantage of method 1 is that :eq:`decay:E:dt` might not be valid
for the coarsest meshes (largest :math:`\Delta t` values). Fitting a line
to all the data points is then misleading.  Method 2 computes
convergence rates for pairs of experiments and allows us to see
if the sequence :math:`r_i` converges to some value as :math:`i\rightarrow m-2`.
The final :math:`r_{m-2}` can then be taken as the convergence rate.
If the coarsest meshes have a differing rate, the corresponding
time steps are probably too large for :eq:`decay:E:dt` to be valid.
That is, those time steps lie outside the asymptotic range of
:math:`\Delta t` values where the error behaves like :eq:`decay:E:dt`.


Implementation  (2)
~~~~~~~~~~~~~~~~~~~

It is straightforward to extend the ``main`` function in the program
``decay_argparse.py`` with statements for computing :math:`r_0, r_1, \ldots, r_{m-2}`
from :eq:`decay:E:dt`:


.. code-block:: python

        from math import log
        
        def main():
            I, a, T, makeplot, dt_values = read_command_line()
            r = {}  # estimated convergence rates
            for theta in 0, 0.5, 1:
                E_values = []
                for dt in dt_values:
                    E = explore(I, a, T, dt, theta, makeplot=False)
                    E_values.append(E)
        
                # Compute convergence rates
                m = len(dt_values)
                r[theta] = [log(E_values[i-1]/E_values[i])/
                            log(dt_values[i-1]/dt_values[i])
                            for i in range(1, m, 1)]
        
            for theta in r:
                print '\nPairwise convergence rates for theta=%g:' % theta
                print ' '.join(['%.2f' % r_ for r_ in r[theta]])
            return r

The program containing this ``main`` function is called `decay_convrate.py <http://tinyurl.com/jvzzcfn/decay/decay_convrate.py>`_.


.. index:: dictionary


The ``r`` object is a *dictionary of lists*. The keys in this
dictionary are the :math:`\theta` values. For example,
``r[1]`` holds the list of the :math:`r_i` values corresponding to
:math:`\theta=1`. In the loop ``for theta in r``, the loop variable ``theta``
takes on the values of the keys in the dictionary ``r`` (in an
undetermined ordering). We could simply do a ``print r[theta]``
inside the loop, but this would typically yield output of
the convergence rates with 16 decimals:


.. code-block:: python

        [1.331919482274763, 1.1488178494691532, ...]


Instead, we format each number with 2 decimals, using a list
comprehension to turn the list of numbers, ``r[theta]``, into
a list of formatted strings. Then we join these strings
with a space in between to get a sequence of rates on one line
in the terminal window. More generally, ``d.join(list)`` joins the
strings in the list ``list`` to one string, with ``d``
as delimiter between ``list[0]``, ``list[1]``, etc.

Here is an example on the outcome of the convergence rate computations:

.. code-block:: console

        Terminal> python decay_convrate.py --dt 0.5 0.25 0.1 0.05 0.025 0.01
        ...
        Pairwise convergence rates for theta=0:
        1.33 1.15 1.07 1.03 1.02
        
        Pairwise convergence rates for theta=0.5:
        2.14 2.07 2.03 2.01 2.01
        
        Pairwise convergence rates for theta=1:
        0.98 0.99 0.99 1.00 1.00

The Forward and Backward Euler methods seem to have an :math:`r` value which
stabilizes at 1, while the Crank-Nicolson seems to be a second-order
method with :math:`r=2`.


.. index:: verification


Very often, we have some theory that predicts what :math:`r` is for a numerical
method. Various theoretical error measures for the :math:`\theta`-rule point to
:math:`r=2` for :math:`\theta =0.5` and :math:`r=1` otherwise. The computed estimates of :math:`r` are
in very good agreement with these theoretical values.



.. admonition:: Why convergence rates are important

   The strong practical application of computing convergence rates is for
   verification: wrong convergence rates point to errors in the code, and
   correct convergence rates brings evidence that the implementation is
   correct. Experience shows that bugs in the code easily destroy the
   expected convergence rate.


Debugging via convergence rates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us experiment with bugs and see the implication on the convergence
rate. We may, for instance, forget to multiply by ``a`` in the denominator
in the updating formula for ``u[n+1]``:


.. code-block:: python

        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt)*u[n]

Running the same ``decay_convrate.py`` command as above gives the expected
convergence rates (!). Why? The reason is that we just specified
the :math:`\Delta t` values are relied on default values for other
parameters. The default value of :math:`a` is 1. Forgetting the factor
``a`` has then no effect. This example shows how important it is to
avoid parameters that are 1 or 0 when verifying implementations.
Running the code ``decay_v0.py`` with :math:`a=2.1` and :math:`I=0.1` yields


.. code-block:: console

        Terminal> python decay_convrate.py --a 2.1 --I 0.1  \ 
                  --dt 0.5 0.25 0.1 0.05 0.025 0.01
        ...
        Pairwise convergence rates for theta=0:
        1.49 1.18 1.07 1.04 1.02
        
        Pairwise convergence rates for theta=0.5:
        -1.42 -0.22 -0.07 -0.03 -0.01
        
        Pairwise convergence rates for theta=1:
        0.21 0.12 0.06 0.03 0.01

This time we see that the expected convergence rates for the Crank-Nicolson and
Backward Euler methods are not obtained, while :math:`r=1` for the Forward Euler
method. The reason for correct rate in the latter case is that :math:`\theta=0`
and the wrong ``theta*dt`` term in the denominator vanishes anyway.

The error


.. code-block:: python

        u[n+1] = ((1-theta)*a*dt)/(1 + theta*dt*a)*u[n]

manifests itself through wrong rates :math:`r\approx 0` for all three methods.
About the same results arise from an erroneous initial condition, ``u[0] = 1``,
or wrong loop limits, ``range(1,Nt)``. It seems that in this simple
problem, most bugs we can think of are detected by the convergence rate
test, provided the values of the input data do not hide the bug.

A ``verify_convergence_rate`` function could compute the dictionary of
list via ``main`` and check if the final rate estimates (:math:`r_{m-2}`)
are sufficiently close to the expected ones. A tolerance of 0.1
seems appropriate, given the uncertainty in estimating :math:`r`:


.. code-block:: python

        def verify_convergence_rate():
            r = main()
            tol = 0.1
            expected_rates = {0: 1, 1: 1, 0.5: 2}
            for theta in r:
                r_final = r[theta][-1]
                diff = abs(expected_rates[theta] - r_final)
                if diff > tol:
                    return False
            return True  # all tests passed

We remark that ``r[theta]`` is a list and the last element in any list
can be extracted by the index ``-1``.



Memory-saving implementation
----------------------------

The computer memory requirements of our implementations so far consists
mainly of the ``u`` and ``t`` arrays, both of length :math:`N_t+1`, plus some other
temporary arrays that Python needs for intermediate results if we do
array arithmetics in our program (e.g., ``I*exp(-a*t)`` needs to store
``a*t`` before ``-`` can be applied to it and then ``exp``).  The
extremely modest storage requirements of simple ODE problems put no
restrictions on the formulations of the algorithm and implementation.
Nevertheless, when the methods for ODEs used here are applied to
three-dimensional partial differential equation (PDE) problems,
memory storage requirements
suddenly become an issue.

The PDE counterpart to our model problem
:math:`u'=-a` is a diffusion equation :math:`u_t = a\nabla^2 u` posed on a
space-time domain. The discrete representation of this domain may in
3D be a spatial mesh of :math:`M^3` points and a time mesh of :math:`N_t` points. A
typical desired value for :math:`M` is 100 in many applications, or even
:math:`1000`.  Storing all the computed :math:`u` values, like we have done in the
programs so far, demands storage of some arrays of size :math:`M^3N_t`, giving
a factor of :math:`M^3` larger storage demands compared to our ODE
programs. Each real number in the array for :math:`u` requires 8 bytes (b) of
storage. With :math:`M=100` and :math:`N_t=1000`, there is a storage demand of
:math:`(10^3)^3\cdot 1000\cdot 8 = 8` Gb for the solution array.
Fortunately, we can usually get rid of the :math:`N_t`
factor, resulting in 8 Mb of storage.
Below we explain how this is done, and the technique is almost
always applied in implementations of PDE problems.

.. Fortunately, the methods we use to solve ODEs

.. and PDEs were mostly developed in a time where the size of a computer's

.. memory was very small compared to today's standards, and researchers

.. were therefore forced to always minimize the memory usage. As a result of

.. these circumstances, there is still a very strong focus on reducing

.. memory requirements in scientific computing algorithms.


Let us critically evaluate how much we really need to store in the
computer's memory in our implementation of the :math:`\theta` method. To
compute a new :math:`u^{n+1}`, all we need is :math:`u^n`. This implies that the
previous :math:`u^{n-1},u^{n-2},\dots,u^0` values do not need to be stored
in an array, although this is convenient for plotting and data
analysis in the program.  Instead of the ``u`` array we can work with
two variables for real numbers, ``u`` and ``u_1``, representing
:math:`u^{n+1}` and :math:`u^n` in the algorithm, respectively.  At each time
level, we update ``u`` from ``u_1`` and then set ``u_1 = u`` so that the
computed :math:`u^{n+1}` value becomes the "previous" value :math:`u^n` at the
next time level. The downside is that we cannot plot the solution
after the simulation is done since only the last two numbers are
available.  The remedy is to store computed values in a file and use
the file for visualizing the solution later.

We have implemented this memory saving idea in the file `decay_memsave.py <http://tinyurl.com/jvzzcfn/decay/decay_memsave.py>`_, which is a
merge of the `decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`_ and
`decay_argparse.py <http://tinyurl.com/jvzzcfn/decay/decay_argparse.py>`_
programs, using module prefixes ``np`` for ``numpy`` and ``plt`` for
``matplotlib.pyplot``.

The following function demonstrates how we work with the two most
recent values of the unknown:


.. code-block:: python

        def solver_memsave(I, a, T, dt, theta, filename='sol.dat'):
            """
            Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt.
            Minimum use of memory. The solution is stored in a file
            (with name filename) for later plotting.
            """
            dt = float(dt)         # avoid integer division
            Nt = int(round(T/dt))  # no of intervals
        
            outfile = open(filename, 'w')
            # u: time level n+1, u_1: time level n
            t = 0
            u_1 = I
            outfile.write('%.16E  %.16E\n' % (t, u_1))
            for n in range(1, Nt+1):
                u = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u_1
                u_1 = u
                t += dt
                outfile.write('%.16E  %.16E\n' % (t, u))
            outfile.close()
            return u, t

This code snippet serves as a quick introduction to file writing in Python.
Reading the data in the file into arrays ``t`` and ``u`` are done by the
function


.. code-block:: python

        def read_file(filename='sol.dat'):
            infile = open(filename, 'r')
            u = [];  t = []
            for line in infile:
                words = line.split()
                if len(words) != 2:
                    print 'Found more than two numbers on a line!', words
                    sys.exit(1)  # abort
                t.append(float(words[0]))
                u.append(float(words[1]))
            return np.array(t), np.array(u)


This type of file with numbers in rows and columns is very common, and
``numpy`` has a function ``loadtxt`` which loads such tabular data into a
two-dimensional array, say with name ``data``. The number in row ``i`` and
column ``j`` is then ``data[i,j]``.  The whole column number ``j`` can be
extracted by ``data[:,j]``.  A version of ``read_file`` using ``np.loadtxt``
reads


.. code-block:: python

        def read_file_numpy(filename='sol.dat'):
            data = np.loadtxt(filename)
            t = data[:,0]
            u = data[:,1]
            return t, u


The present counterpart to the ``explore`` function from
`decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`_ must run
``solver_memsave`` and then load data from file before we can compute
the error measure and make the plot:


.. code-block:: python

        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            filename = 'u.dat'
            u, t = solver_memsave(I, a, T, dt, theta, filename)
        
            t, u = read_file(filename)
            u_e = exact_solution(t, I, a)
            e = u_e - u
            E = np.sqrt(dt*np.sum(e**2))
            if makeplot:
                plt.figure()
                ...


The `decay_memsave.py <http://tinyurl.com/jvzzcfn/decay/decay_memsave.py>`_
file also includes command-line options ``--I``, ``--a``, ``--T``, ``--dt``,
``--theta``, and ``--makeplot`` for controlling input parameters and
making a single run.  For example,


.. code-block:: console

        Terminal> python decay_memsave.py --T 10 --theta 1 --dt 2

results in the output


.. code-block:: text


        I=1.0, a=1.0, T=10.0, makeplot=True, theta=1.0, dt=2.0
        theta=1.0 dt=2 Error=3.136E-01





.. !split

Software engineering
====================



.. admonition:: Goal

   Efficient use of differential equation models requires software that is easy to
   test and flexible for setting up extensive numerical experiments.
   This section introduces three important concepts:
   
     * Modules
   
     * Testing frameworks
   
     * Implementation with classes
   
   The concepts are introduced using the differential equation
   problem :math:`u'=-au`, :math:`u(0)=I`, as example.


.. _decay:prog:se:module:

Making a module
---------------


.. index:: modules




.. admonition:: The DRY principle

   The previous sections have outlined numerous different programs, all of
   them having their own copy of the ``solver`` function.  Such copies
   of the same piece of code is against the important *Don't Repeat
   Yourself* (DRY) principle in programming.  If we want to change the
   ``solver`` function there should be *one and only one* place where the
   change needs to be performed.


To clean up the repetitive code snippets scattered among the
``decay_*.py`` files, we start by collecting the
various functions we want to keep for the future in one file,
now called `decay_mod.py <http://tinyurl.com/jvzzcfn/decay/decay_mod.py>`_ (``mod`` stands for "module").
The following functions are copied to this file:

 * ``solver`` for computing the numerical solution

 * ``verify_three_steps`` for verifying the first three solution
   points against hand calculations

 * ``verify_discrete_solution`` for verifying the entire computed solution
   against an exact formula for the numerical solution

 * ``explore`` for computing and plotting the solution

 * ``define_command_line_options`` for defining option-value pairs
   on the command line

 * ``read_command_line`` for reading input from the command line,
   now extended to work both with ``sys.argv`` directly
   and with an ``ArgumentParser`` object

 * ``main`` for running experiments with :math:`\theta=0,0.5,1` and a series of
   :math:`\Delta t` values, and computing convergence rates

 * ``main_GUI`` for doing the same as the ``main`` function, but modified
   for automatic GUI generation

 * ``verify_convergence_rate`` for verifying the computed convergence
   rates against the theoretically expected values

We use Matplotlib for plotting. A sketch of the ``decay_mod.py``
file, with complete versions of the modified functions, looks as follows:


.. code-block:: python

        from numpy import *
        from matplotlib.pyplot import *
        import sys
        
        def solver(I, a, T, dt, theta):
            ...
        
        def verify_three_steps():
            ...
        
        def verify_exact_discrete_solution():
            ...
        
        def exact_solution(t, I, a):
            ...
        
        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            ...
        
        def define_command_line_options():
            ...
        
        def read_command_line(use_argparse=True):
            if use_argparse:
                parser = define_command_line_options()
                args = parser.parse_args()
                print 'I={}, a={}, makeplot={}, dt_values={}'.format(
                    args.I, args.a, args.makeplot, args.dt_values)
                return args.I, args.a, args.makeplot, args.dt_values
            else:
                if len(sys.argv) < 6:
                    print 'Usage: %s I a on/off dt1 dt2 dt3 ...' % \ 
                          sys.argv[0]; sys.exit(1)
        
                I = float(sys.argv[1])
                a = float(sys.argv[2])
                T = float(sys.argv[3])
                makeplot = sys.argv[4] in ('on', 'True')
                dt_values = [float(arg) for arg in sys.argv[5:]]
        
                return I, a, makeplot, dt_values
        
        def main():
            ...


This ``decay_mod.py`` file is already a module such that we can import
desired functions in other programs. For example, we can in a file do


.. code-block:: python

        from decay_mod import solver
        u, t = solver(I=1.0, a=3.0, T=3, dt=0.01, theta=0.5)



.. index:: test block (in modules)


However, it should also be possible to both use ``decay_mod.py`` as
a module *and* execute the file as a program that runs ``main()``. This is
accomplished by ending the file with a *test block*:


.. code-block:: python

        if __name__ == '__main__':
            main()

When ``decay_mod.py`` is used as a module, ``__name__`` equals the module
name ``decay_mod``, while ``__name__`` equals ``'__main__'`` when the
file is run as a program.
Optionally, we could run the verification tests if the word ``verify``
is present on the command line and ``verify_convergence_rate`` could
be tested if ``verify_rates`` is found on the command line. The
``verify_rates`` argument must be removed before we read parameter values from
the command line, otherwise the ``read_command_line`` function (called by ``main``)
will not work properly.


.. code-block:: python

        if __name__ == '__main__':
            if 'verify' in sys.argv:
                if verify_three_steps() and verify_discrete_solution():
                    pass # ok
                else:
                    print 'Bug in the implementation!'
            elif 'verify_rates' in sys.argv:
                sys.argv.remove('verify_rates')
                if not '--dt' in sys.argv:
                    print 'Must assign several dt values'
                    sys.exit(1)  # abort
                if verify_convergence_rate():
                    pass
                else:
                    print 'Bug in the implementation!'
            else:
                # Perform simulations
                main()


.. _decay:prog:se:import:

Prefixing imported functions by the module name
-----------------------------------------------


.. index:: importing modules


Import statements of the form ``from module import *`` import
functions and variables in ``module.py`` into the current file.
For example, when doing


.. code-block:: python

        from numpy import *
        from matplotlib.pyplot import *

we get mathematical functions like ``sin`` and ``exp``
as well as MATLAB-style functions like ``linspace`` and ``plot``,
which can be called by these well-known names.
Unfortunately, it sometimes becomes confusing to
know where a particular function comes from. Is it from ``numpy``? Or
``matplotlib.pyplot``?
Or is it our own function?

An alternative import is


.. code-block:: python

        import numpy
        import matplotlib.pyplot

and such imports require functions to be prefixed by the module name, e.g.,


.. code-block:: python

        t = numpy.linspace(0, T, Nt+1)
        u_e = I*numpy.exp(-a*t)
        matplotlib.pyplot.plot(t, u_e)

This is normally regarded as a better habit because it is explicitly stated
from which module a function comes from.

The modules ``numpy`` and ``matplotlib.pyplot`` are so frequently used,
and their full names quite tedious to write, so two standard abbreviations
have evolved in the Python scientific computing community:


.. code-block:: python

        import numpy as np
        import matplotlib.pyplot as plt
        
        t = np.linspace(0, T, Nt+1)
        u_e = I*np.exp(-a*t)
        plt.plot(t, u_e)

A version of the ``decay_mod`` module where we use the ``np`` and ``plt``
prefixes is found in the file
`decay_mod_prefix.py <http://tinyurl.com/jvzzcfn/decay/decay_mod_prefix.py>`_.

The downside of prefixing functions by the module name is that
mathematical expressions like :math:`e^{-at}\sin(2\pi t)` get
cluttered with module names,

.. code-block:: python

        numpy.exp(-a*t)*numpy.sin(2(numpy.pi*t)
        # or
        np.exp(-a*t)*np.sin(2*np.pi*t)

Such an expression looks like ``exp(-a*t)*sin(2*pi*t)`` in most
other programming languages. Similarly,
``np.linspace`` and ``plt.plot`` look less familiar to people who are
used to MATLAB and who have not adopted Python's prefix style.
Whether to do ``from module import *`` or ``import module`` depends
on personal taste and the problem at hand. In these writings we use
``from module import`` in shorter programs where similarity with
MATLAB could be an advantage, and where a one-to-one correspondence between
mathematical formulas and Python expressions is important.
The style ``import module`` is preferred inside Python modules (see
:ref:`decay:exer:module1` for a demonstration).


.. _decay:prog:se:doctest:

Doctests
--------


.. index:: doctests


.. index::
   single: software testing; doctests


We have emphasized how important it is to be able to run tests in the
program at any time. This was solved by calling various ``verify*``
functions in the previous examples. However, there exists
well-established procedures and corresponding tools for automating
the execution of tests. We shall briefly demonstrate two important
techniques: *doctest* and *unit testing*. The corresponding files are
the modules `decay_mod_doctest.py <http://tinyurl.com/jvzzcfn/decay/decay_mod_doctest.py>`_
and `decay_mod_unittest.py <http://tinyurl.com/jvzzcfn/decay/decay_mod_unittest.py>`_.


Doc strings (the first string after the function header) are used to
document the purpose of functions and their arguments. Very often it
is instructive to include an example on how to use the function.
Interactive examples in the Python shell are most illustrative as
we can see the output resulting from function calls. For example,
we can in the ``solver`` function include an example on calling
this function and printing the computed ``u`` and ``t`` arrays:


.. code-block:: python

        def solver(I, a, T, dt, theta):
            """
            Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt.
        
        
            >>> u, t = solver(I=0.8, a=1.2, T=4, dt=0.5, theta=0.5)
            >>> for t_n, u_n in zip(t, u):
            ...     print 't=%.1f, u=%.14f' % (t_n, u_n)
            t=0.0, u=0.80000000000000
            t=0.5, u=0.43076923076923
            t=1.0, u=0.23195266272189
            t=1.5, u=0.12489758761948
            t=2.0, u=0.06725254717972
            t=2.5, u=0.03621291001985
            t=3.0, u=0.01949925924146
            t=3.5, u=0.01049960113002
            t=4.0, u=0.00565363137770
            """
            ...


When such interactive demonstrations are inserted in doc strings,
Python's `doctest <http://docs.python.org/library/doctest.html>`_
module can be used to automate running all commands
in interactive sessions and compare new output with the output
appearing in the doc string.  All we have to do in the current example
is to write


.. code-block:: python

        Terminal> python -m doctest decay_mod_doctest.py

This command imports the ``doctest`` module, which runs all tests.
No additional command-line argument is allowed when running doctests.
If any test fails, the problem is reported, e.g.,


.. code-block:: console

        Terminal> python -m doctest decay_mod_doctest.py
        ********************************************************
        File "decay_mod_doctest.py", line 12, in decay_mod_doctest....
        Failed example:
            for t_n, u_n in zip(t, u):
                print 't=%.1f, u=%.14f' % (t_n, u_n)
        Expected:
            t=0.0, u=0.80000000000000
            t=0.5, u=0.43076923076923
            t=1.0, u=0.23195266272189
            t=1.5, u=0.12489758761948
            t=2.0, u=0.06725254717972
        Got:
            t=0.0, u=0.80000000000000
            t=0.5, u=0.43076923076923
            t=1.0, u=0.23195266272189
            t=1.5, u=0.12489758761948
            t=2.0, u=0.06725254718756
        ********************************************************
        1 items had failures:
           1 of   2 in decay_mod_doctest.solver
        ***Test Failed*** 1 failures.


Note that in the output of ``t`` and ``u`` we write ``u`` with 14 digits.
Writing all 16 digits is not a good idea: if the tests are run on
different hardware, round-off errors might be different, and
the ``doctest`` module detects that the numbers are not precisely the same
and reports failures. In the present application, where :math:`0 < u(t) \leq 0.8`,
we expect round-off errors to be of size :math:`10^{-16}`, so comparing 15
digits would probably be reliable, but we compare 14 to be on the
safe side.

Doctests are highly encouraged as they do two things: 1) demonstrate
how a function is used and 2) test that the function works.

Here is an example on a doctest in the ``explore`` function:


.. code-block:: python

        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            """
            Run a case with the solver, compute error measure,
            and plot the numerical and exact solutions (if makeplot=True).
        
            >>> for theta in 0, 0.5, 1:
            ...    E = explore(I=1.9, a=2.1, T=5, dt=0.1, theta=theta,
            ...                makeplot=False)
            ...    print '%.10E' % E
            ...
            7.3565079236E-02
            2.4183893110E-03
            6.5013039886E-02
            """
            ...

This time we limit the output to 10 digits.



.. warning::
    Doctests are not straightforward to construct for
    functions that rely on command-line input and that print results to
    the terminal window.


.. _decay:prog:se:nose:

Unit testing with nose
----------------------


.. index:: nose tests


.. index:: unit testing


.. index::
   single: software testing; nose


The unit testing technique consists of identifying small units
of code, usually functions (or classes), and write one or more tests for
each unit. One test should, ideally, not depend on the outcome of
other tests. For example, the doctest in function ``solver`` is a
unit test, and the doctest in function ``explore`` as well, but the
latter depends on a working ``solver``. Putting the error computation
and plotting in ``explore`` in two separate functions would allow
independent unit tests. In this way, the design of unit tests impacts
the design of functions. The recommended practice is actually to
design and write the unit tests first and *then* implement the functions!

In scientific computing it is not always obvious how to best perform
unit testing. The units is naturally larger than in non-scientific
software. Very often the solution procedure of a mathematical problem
identifies a unit.

Basic use of nose
~~~~~~~~~~~~~~~~~

The ``nose`` package is a versatile tool for implementing unit tests
in Python. Here is a recommended way of structuring tests:

 1. Implement tests in functions with names starting with ``test_``.

 2. The test functions perform assertions on computed results
    using ``assert`` functions from the ``nose.tools`` module.

 3. The test functions can be in the source code files or be
    collected in separate files, usually with names starting with ``test_``.

Here comes a very simple illustration of the three points.
Assume that we have this function in a module ``mymod``:


.. code-block:: python

        def double(n):
            return 2*n


Either in this file, or in a separate file ``test_mymod.py``, we
implement a test function whose purpose is
to test that the function ``double`` works as intended:


.. code-block:: python

        import nose.tools as nt
        
        def test_double():
            result = mymod.double(4)
            nt.assert_equal(result, 8)

We need to do an ``import mymod`` if this test is in a separate file, and
Python needs to be able to find ``mymod`` (it must be avaliable in
the same directory, installed in a
registered system library or in a directory listed in ``PYTHONPATH``,
or the directory containing ``mymod.py``
must explicitly be added to the ``sys.path`` list).

Running


.. code-block:: console

        Terminal> nosetests -s mymod

makes the ``nose`` tool run all functions with names matching ``test_*()``
in ``mymod.py``.
Alternatively, if the test functions are in some ``test_mymod.py`` file,
we can just write ``nosetests -s``. The nose tool will then look
for all files with names mathching ``test_*.py`` and run all
functions ``test_*()`` in these files.

When you have nose tests in files ``test_*.py`` it is common to collect
these files in a subdirectory ``tests``, or ``*_tests`` if
you have several test subdirectories. Running ``nosetests -s`` will
then recursively look for all ``tests`` and ``*_tests`` subdirectories
and run all functions ``test_*()`` in all files ``test_*.py`` in these
directories. Just one command can then launch a series of tests in
a directory tree!

An example of a ``tests`` directory with different types of ``test_*.py``
files are found in `src/decay/tests <http://tinyurl.com/jvzzcfn/decay/tests>`_.



.. admonition:: Tip

   The ``-s`` option to ``nosetests`` assures that any print statement
   in the ``test_*`` functions appears in the output. Without this
   option, ``nosetests`` suppressed whatever the tests writes to
   the terminal window (standard output). Such behavior is annoying,
   especially when developing and testing tests.


The number of failed tests and their details are
reported, or an ``OK`` is printed if all tests passed.

The advantage with the ``nose`` package is two-fold:

1. tests are written and collected
   in a structured way, and

2. large collections of tests, scattered
   throughout a tree of directories,
   can be executed with one command ``nosetests -s``.

Alternative assert statements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In case the ``nt.assert_equal`` function
finds that the two arguments are equal, the test is a success, otherwise
it is a failure and an exception of type ``AssertionError`` is raised.
The particular exception is the indicator that a test has failed.

Instead of calling the convenience function ``nt.assert_equal``, we
can use Python's plain ``assert`` statement, which tests if a boolean
expression is true and raises an ``AssertionError`` otherwise.
Here, the statement is ``assert result == 8``.

A completely manual alternative is to explicitly raise an ``AssertionError``
exception if the computed result is wrong:


.. code-block:: python

        if result != 8:
            raise AssertionError()



Applying nose
~~~~~~~~~~~~~

Let us illustrate how to use the ``nose`` tool for testing key functions
in the ``decay_mod`` module. Or more precisely, the module is called
``decay_mod_unittest`` with all the ``verify*`` functions removed
as these now are outdated by the unit tests.

We design three unit tests:

 1. A comparison between the computed :math:`u^n` values and the
    exact discrete solution.

 2. A comparison between the computed :math:`u^n` values and precomputed,
    verified reference values.

 3. A comparison between observed and expected convergence rates.

These tests follow very closely the code in the previously shown
``verify*`` functions. We start with comparing :math:`u^n`, as computed by
the function ``solver``, to the formula
for the exact discrete solution:


.. code-block:: python

        import nose.tools as nt
        import decay_mod_unittest as decay_mod
        import numpy as np
        
        def exact_discrete_solution(n, I, a, theta, dt):
            """Return exact discrete solution of the theta scheme."""
            dt = float(dt)  # avoid integer division
            factor = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
            return I*factor**n
        
        def test_against_discrete_solution():
            """
            Compare result from solver against
            formula for the discrete solution.
            """
            theta = 0.8; a = 2; I = 0.1; dt = 0.8
            N = int(8/dt)  # no of steps
            u, t = decay_mod.solver(I=I, a=a, T=N*dt, dt=dt, theta=theta)
            u_de = np.array([exact_discrete_solution(n, I, a, theta, dt)
                             for n in range(N+1)])
            diff = np.abs(u_de - u).max()
            nt.assert_almost_equal(diff, 0, delta=1E-14)


The ``nt.assert_almost_equal`` is the relevant function for comparing two
real numbers. The ``delta`` argument specifies a tolerance for the
comparison. Alternatively, one can specify a ``places`` argument
for the number of decimal places to be used in the comparison.

After having carefully verified the implementation, we may
store correctly computed numbers in the test program or in files for
use in future tests. Here is an example on how the outcome from the
``solver`` function can be compared to what is considered to be
correct results:


.. code-block:: python

        def test_solver():
            """
            Compare result from solver against
            precomputed arrays for theta=0, 0.5, 1.
            """
            I=0.8; a=1.2; T=4; dt=0.5  # fixed parameters
            precomputed = {
                't': np.array([ 0. ,  0.5,  1. ,  1.5,  2. ,  2.5,
                                3. ,  3.5,  4. ]),
                0.5: np.array(
                    [ 0.8       ,  0.43076923,  0.23195266, 0.12489759,
                      0.06725255,  0.03621291,  0.01949926, 0.0104996 ,
                      0.00565363]),
                0: np.array(
                    [  8.00000000e-01,   3.20000000e-01,
                       1.28000000e-01,   5.12000000e-02,
                       2.04800000e-02,   8.19200000e-03,
                       3.27680000e-03,   1.31072000e-03,
                       5.24288000e-04]),
                1: np.array(
                    [ 0.8       ,  0.5       ,  0.3125    ,  0.1953125 ,
                      0.12207031,  0.07629395,  0.04768372,  0.02980232,
                      0.01862645]),
                }
            for theta in 0, 0.5, 1:
                u, t = decay_mod.solver(I, a, T, dt, theta=theta)
                diff = np.abs(u - precomputed[theta]).max()
                # Precomputed numbers are known to 8 decimal places
                nt.assert_almost_equal(diff, 0, places=8,
                                       msg='theta=%s' % theta)

The ``precomputed`` object is a dictionary with four keys: ``'t'`` for the
time mesh, and three :math:`\theta` values for :math:`u^n` solutions corresponding
to :math:`\theta=0,0.5,1`.

Testing for special type of input data that may cause trouble constitutes
a common way of constructing unit tests.
For example, the updating formula for
:math:`u^{n+1}` may be incorrectly evaluated because of unintended integer
divisions. With

.. code-block:: python

        theta = 1; a = 1; I = 1; dt = 2

the nominator and denominator in the updating expression,

.. code-block:: python

        (1 - (1-theta)*a*dt)
        (1 + theta*dt*a)

evaluate to 1 and 3, respectively, and the fraction ``1/3`` will
call up integer division and consequently lead to ``u[n+1]=0``.
We construct a unit test to make sure ``solver`` is smart
enough to avoid this problem:


.. code-block:: python

        def test_potential_integer_division():
            """Choose variables that can trigger integer division."""
            theta = 1; a = 1; I = 1; dt = 2
            N = 4
            u, t = decay_mod.solver(I=I, a=a, T=N*dt, dt=dt, theta=theta)
            u_de = np.array([exact_discrete_solution(n, I, a, theta, dt)
                             for n in range(N+1)])
            diff = np.abs(u_de - u).max()
            nt.assert_almost_equal(diff, 0, delta=1E-14)


The final test is to see that the convergence rates corresponding to
:math:`\theta=0,0.5, 1` are 1, 2, and 1, respectively:


.. code-block:: python

        def test_convergence_rates():
            """Compare empirical convergence rates to exact ones."""
            # Set command-line arguments directly in sys.argv
            import sys
            sys.argv[1:] = '--I 0.8 --a 2.1 --T 5 '\ 
                           '--dt 0.4 0.2 0.1 0.05 0.025'.split()
            r = decay_mod.main()
            for theta in r:
                nt.assert_true(r[theta])  # check for non-empty list
        
            expected_rates = {0: 1, 1: 1, 0.5: 2}
            for theta in r:
                r_final = r[theta][-1]
                # Compare to 1 decimal place
                nt.assert_almost_equal(expected_rates[theta], r_final,
                                       places=1, msg='theta=%s' % theta)


Nothing more is needed in the `test_decay_nose.py <http://tinyurl.com/jvzzcfn/decay/tests/test_decay_nose.py>`_
file where the tests reside.
Running ``nosetests -s`` will report ``Ran 3 tests`` and an ``OK`` for
success.  Everytime we modify the ``decay_mod_unittest`` module we can
run ``nosetests`` to quickly see if the edits have any impact on the
verification tests.

Installation of nose
~~~~~~~~~~~~~~~~~~~~

The ``nose`` package does not come with a standard Python distribution and must
therefore be installed separately. The procedure is standard and
described on `Nose's web pages <http://nose.readthedocs.org/en/latest/>`_.  On Debian-based Linux
systems the command is ``sudo apt-get install python-nose``, and
with MacPorts you run ``sudo port install py27-nose``.


.. index:: nose testing of doctests


.. index::
   single: software testing; nose w/doctests


Using nose to test modules with doctests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Assume that ``mod`` is the name of some module that contains doctests.
We may let ``nose`` run these doctests and report errors in the
standard way using the code set-up


.. code-block:: python

        import doctest
        import mod
        
        def test_mod():
            failure_count, test_count = doctest.testmod(m=mod)
            nt.assert_equal(failure_count, 0,
                            msg='%d tests out of %d failed' %
                            (failure_count, test_count))

The call to ``doctest.testmod`` runs all doctests in the module file
``mod.py`` and returns the number of failures (``failure_count``)
and the total number of tests (``test_count``). A real example is
found in the file
`test_decay_doctest.py <http://tinyurl.com/jvzzcfn/decay/tests/test_decay_doctest.py>`_.


.. _decay:prog:se:unittest:

Classical class-based unit testing
----------------------------------


.. index:: unit testing


.. index:: unittest


.. index::
   single: software testing; unit testing (class-based)


The classical way of implementing unit tests derives from the JUnit
tool in Java where all tests are methods in a class for testing.
Python comes with a module ``unittest`` for doing this type of unit tests.
While ``nose`` allows simple functions for unit tests, ``unittest``
requires deriving a class ``Test*`` from ``unittest.TestCase`` and
implementing each test as methods with names ``test_*`` in that class.
I strongly recommend to use ``nose`` over ``unittest``, because it is
much simpler and more convenient, but class-based unit testing
is a very classical subject that computational scientists should
have some knowledge about. That is why a short introduction
to ``unittest`` is included below.

Basic use of unittest
~~~~~~~~~~~~~~~~~~~~~

.. index:: unittest

.. index:: TestCase (class in unittest)


We apply the ``double`` function in the ``mymod`` module introduced in the
previous section as example.
Unit testing with the aid of the ``unittest`` module
consists of writing a file ``test_mymod.py`` with the content


.. code-block:: python

        import unittest
        import mymod
        
        class TestMyCode(unittest.TestCase):
            def test_double(self):
                result = mymod.double(4)
                self.assertEqual(result, 8)
        
        if __name__ == '__main__':
            unittest.main()

The test is run by executing the test file ``test_mymod.py`` as a standard
Python program. There is no support in ``unittest`` for automatically
locating and running all tests in all test files in a directory tree.

Those who have experience with object-oriented programming will see that
the difference between using ``unittest`` and ``nose`` is minor.

Demonstration of unittest
~~~~~~~~~~~~~~~~~~~~~~~~~

The same tests as shown for the nose framework are reimplemented
with the ``TestCase`` classes in the file `test_decay_unittest.py <http://tinyurl.com/jvzzcfn/decay/tests/test_decay_nose.py>`_.
The tests are identical, the only difference being that with
``unittest`` we must write the tests as methods in
a class and the assert functions have
slightly different names.


.. code-block:: python

        import unittest
        import decay_mod_unittest as decay
        import numpy as np
        
        def exact_discrete_solution(n, I, a, theta, dt):
            factor = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
            return I*factor**n
        
        class TestDecay(unittest.TestCase):
        
            def test_against_discrete_solution(self):
                ...
                diff = np.abs(u_de - u).max()
                self.assertAlmostEqual(diff, 0, delta=1E-14)
        
            def test_solver(self):
                ...
                for theta in 0, 0.5, 1:
                    ...
                    self.assertAlmostEqual(diff, 0, places=8,
                                           msg='theta=%s' % theta)
        
            def test_potential_integer_division():
                ...
                self.assertAlmostEqual(diff, 0, delta=1E-14)
        
            def test_convergence_rates(self):
                ...
                for theta in r:
                    ...
                    self.assertAlmostEqual(...)
        
        if __name__ == '__main__':
            unittest.main()


.. @@@CODE src-decay/tests/test_decay_unittest.py fromto: def test_conv@




.. _decay:prog:se:class:

Implementing simple problem and solver classes
----------------------------------------------

The :math:`\theta`-rule was compactly and conveniently implemented in
a function ``solver`` in the section :ref:`decay:py1`.
In more complicated problems it might
be beneficial to use classes and introduce a class ``Problem`` to
hold the definition of the physical problem, a class ``Solver``
to hold the data and methods needed to numerically solve the problem,
and a class ``Visualizer`` to make plots.
This idea will now be illustrated, resulting in code that represents
an alternative to the ``solver`` and ``explore`` functions found
in the ``decay_mod`` module.

Explaining the details of class programming in Python is considered
beyond the scope of this text.  Readers who are unfamiliar with Python
class programming should first consult one of the many electronic
Python tutorials or textbooks to come up to speed with concepts and
syntax of Python classes before reading on. The author has a gentle
introduction to class programming for scientific applications
in [Ref1]_, see Chapter 7 and 9 and Appendix E.
Other useful resources are

 * The Python Tutorial: `<http://docs.python.org/2/tutorial/classes.html>`_

 * Wiki book on Python Programming: `<http://en.wikibooks.org/wiki/Python_Programming/Classes>`_

 * tutorialspoint.com: `<http://www.tutorialspoint.com/python/python_classes_objects.htm>`_

The problem class  (1)
~~~~~~~~~~~~~~~~~~~~~~

.. index:: problem class


The purpose of the problem class is to store all information about
the mathematical model. This usually means all the physical parameters
in the problem. In the current example with exponential decay we may
also add the exact solution of the ODE to the problem class.
The simplest form of a problem class is therefore


.. code-block:: python

        from numpy import exp
        
        class Problem:
            def __init__(self, I=1, a=1, T=10):
                self.T, self.I, self.a = I, float(a), T
        
            def exact_solution(self, t):
                I, a = self.I, self.a
                return I*exp(-a*t)

We could in the ``exact_solution`` method have written
``self.I*exp(-self.a*t)``, but using local variables ``I`` and ``a`` allows
the formula ``I*exp(-a*t)`` which looks closer to the mathematical
expression :math:`Ie^{-at}`.  This is not an important issue with the
current compact formula, but is beneficial in more complicated
problems with longer formulas to obtain the closest possible
relationship between code and mathematics. My coding style is to strip
off the ``self`` prefix when the code expresses mathematical formulas.

The class data can be set either as arguments in the constructor or
at any time later, e.g.,


.. code-block:: python

        problem = Problem(T=5)
        problem.T = 8
        problem.dt = 1.5

(Some programmers prefer ``set`` and ``get`` functions for setting and getting
data in classes, often implemented via *properties* in Python, but
I consider that overkill when we just have a few data items in a class.)

It would be convenient if class ``Problem`` could also initialize
the data from the command line. To this end, we add a method for
defining a set of command-line options and a method that sets the
local attributes equal to what was found on the command line.
The default values associated with the command-line options are taken
as the values provided to the constructor. Class ``Problem`` now becomes


.. code-block:: python

        class Problem:
            def __init__(self, I=1, a=1, T=10):
                self.T, self.I, self.a = I, float(a), T
        
            def define_command_line_options(self, parser=None):
                if parser is None:
                    import argparse
                    parser = argparse.ArgumentParser()
        
                parser.add_argument(
                    '--I', '--initial_condition', type=float,
                    default=self.I, help='initial condition, u(0)',
                    metavar='I')
                parser.add_argument(
                    '--a', type=float, default=self.a,
                    help='coefficient in ODE', metavar='a')
                parser.add_argument(
                    '--T', '--stop_time', type=float, default=self.T,
                    help='end time of simulation', metavar='T')
                return parser
        
            def init_from_command_line(self, args):
                self.I, self.a, self.T = args.I, args.a, args.T
        
            def exact_solution(self, t):
                I, a = self.I, self.a
                return I*exp(-a*t)

Observe that if the user already has an ``ArgumentParser`` object it can be
supplied, but if she does not have any, class ``Problem`` makes one.
Python's ``None`` object is used to indicate that a variable is not
initialized with a proper value.

The solver class  (1)
~~~~~~~~~~~~~~~~~~~~~

.. index:: solver class


.. index:: wrapper (code)


The solver class stores data related to the numerical solution method
and provides a function ``solve`` for solving the problem.
A problem object must be given to the constructor so that the solver
can easily look up physical data. In the present example, the
data related to the numerical solution method consists of :math:`\Delta t`
and :math:`\theta`. We add, as in the problem class, functionality for
reading :math:`\Delta t` and :math:`\theta` from the command line:


.. code-block:: python

        class Solver:
            def __init__(self, problem, dt=0.1, theta=0.5):
                self.problem = problem
                self.dt, self.theta = float(dt), theta
        
            def define_command_line_options(self, parser):
                parser.add_argument(
                    '--dt', '--time_step_value', type=float,
                    default=0.5, help='time step value', metavar='dt')
                parser.add_argument(
                    '--theta', type=float, default=0.5,
                    help='time discretization parameter', metavar='dt')
                return parser
        
            def init_from_command_line(self, args):
                self.dt, self.theta = args.dt, args.theta
        
            def solve(self):
                from decay_mod import solver
                self.u, self.t = solver(
                    self.problem.I, self.problem.a, self.problem.T,
                    self.dt, self.theta)
        
            def error(self):
                u_e = self.problem.exact_solution(self.t)
                e = u_e - self.u
                E = sqrt(self.dt*sum(e**2))
                return E

Note that we here simply reuse the implementation of the numerical method
from the ``decay_mod`` module. The ``solve`` function is just a *wrapper*
of the previously developed stand-alone ``solver`` function.

The visualizer class  (1)
~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: visualizer class


The purpose of the visualizer class is to plot the numerical solution
stored in class ``Solver``. We also add the possibility to plot the
exact solution. Access to the problem and solver objects is required
when making plots so the constructor must hold references to these objects:


.. code-block:: python

        class Visualizer:
            def __init__(self, problem, solver):
                self.problem, self.solver = problem, solver
        
            def plot(self, include_exact=True, plt=None):
                """
                Add solver.u curve to the plotting object plt,
                and include the exact solution if include_exact is True.
                This plot function can be called several times (if
                the solver object has computed new solutions).
                """
                if plt is None:
                    import scitools.std  as plt # can use matplotlib as well
        
                plt.plot(self.solver.t, self.solver.u, '--o')
                plt.hold('on')
                theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
                name = theta2name.get(self.solver.theta, '')
                legends = ['numerical %s' % name]
                if include_exact:
                    t_e = linspace(0, self.problem.T, 1001)
                    u_e = self.problem.exact_solution(t_e)
                    plt.plot(t_e, u_e, 'b-')
                    legends.append('exact')
                plt.legend(legends)
                plt.xlabel('t')
                plt.ylabel('u')
                plt.title('theta=%g, dt=%g' %
                          (self.solver.theta, self.solver.dt))
                plt.savefig('%s_%g.png' % (name, self.solver.dt))
                return plt


The ``plt`` object in the ``plot`` method is worth a comment. The idea is
that ``plot`` can add a numerical solution curve to an existing
plot. Calling ``plot`` with a ``plt`` object (which has to be a
``matplotlib.pyplot`` or ``scitools.std`` object in this implementation),
will just add the curve
``self.solver.u`` as a dashed line with circles at the mesh points
(leaving the color of the curve up to the plotting tool). This
functionality allows plots with several solutions: just make a loop
where new data is set in the problem and/or solver classes, the
solver's ``solve()`` method is called, and the most recent numerical
solution is plotted by the ``plot(plt)`` method in the visualizer object
:ref:`decay:exer:decay_class:exper` describes a problem setting
where this functionality is explored.

Combing the objects
~~~~~~~~~~~~~~~~~~~

Eventually we need to show how the classes ``Problem``, ``Solver``, and
``Visualizer`` play together:


.. code-block:: python

        def main():
            problem = Problem()
            solver = Solver(problem)
            viz = Visualizer(problem, solver)
        
            # Read input from the command line
            parser = problem.define_command_line_options()
            parser = solver. define_command_line_options(parser)
            args = parser.parse_args()
            problem.init_from_command_line(args)
            solver. init_from_command_line(args)
        
            # Solve and plot
            solver.solve()
            import matplotlib.pyplot as plt
            #import scitools.std as plt
            plt = viz.plot(plt=plt)
            E = solver.error()
            if E is not None:
                print 'Error: %.4E' % E
            plt.show()


The file `decay_class.py <http://tinyurl.com/jvzzcfn/decay/decay_class.py>`_
constitutes a module with the three classes and the ``main`` function.



.. admonition:: Test the understanding

   Implement the problem in
   :ref:`decay:app:exer:drag:prog` in terms of problem, solver,
   and visualizer classes. Equip the classes and their methods with
   doc strings with tests. Also include nose tests.


.. _decay:prog:se:class2:

Improving the problem and solver classes
----------------------------------------

The previous ``Problem`` and ``Solver`` classes containing parameters
soon get much repetitive code when the number of parameters increases.
Much of this code can be parameterized and be made more compact.
For this purpose, we decide to collect all parameters in a dictionary,
``self.prms``, with two associated dictionaries ``self.types`` and
``self.help`` for holding associated object types and help strings.
Provided a problem, solver, or visualizer class defines these three
dictionaries in the constructor, using default or user-supplied values
of the parameters, we can create a super class ``Parameters`` with general code
for defining command-line options and reading them as well as
methods for setting and getting a parameter. A ``Problem`` or ``Solver`` class will
then inherit command-line functionality and the set/get methods from
the ``Parameters`` class.

A generic class for parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A simplified version of the parameter class looks as follows:


.. code-block:: python

        class Parameters:
            def set(self, **parameters):
                for name in parameters:
                    self.prms[name] = parameters[name]
        
            def get(self, name):
                return self.prms[name]
        
            def define_command_line_options(self, parser=None):
                if parser is None:
                    import argparse
                    parser = argparse.ArgumentParser()
        
                for name in self.prms:
                    tp = self.types[name] if name in self.types else str
                    help = self.help[name] if name in self.help else None
                    parser.add_argument(
                        '--' + name, default=self.get(name), metavar=name,
                        type=tp, help=help)
        
                return parser
        
            def init_from_command_line(self, args):
                for name in self.prms:
                    self.prms[name] = getattr(args, name)

The file `class_decay_oo.py <http://tinyurl.com/jvzzcfn/decay/class_decay_oo.py>`_ contains
a slightly more advanced version of class ``Parameters`` where we
in the ``set`` and ``get`` functions test for valid parameter names and
raise exceptions with informative messages if any name is not registered.

The problem class  (2)
~~~~~~~~~~~~~~~~~~~~~~

.. index:: problem class


A class ``Problem`` for the problem :math:`u'=-au`, :math:`u(0)=I`, :math:`t\in (0,T]`, with
parameters input :math:`a`, :math:`I`, and :math:`T` can now be coded as


.. code-block:: python

        class Problem(Parameters):
            """
            Physical parameters for the problem u'=-a*u, u(0)=I,
            with t in [0,T].
            """
            def __init__(self):
                self.prms = dict(I=1, a=1, T=10)
                self.types = dict(I=float, a=float, T=float)
                self.help = dict(I='initial condition, u(0)',
                                 a='coefficient in ODE',
                                 T='end time of simulation')
        
            def exact_solution(self, t):
                I, a = self.get('I'), self.get('a')
                return I*np.exp(-a*t)


The solver class  (2)
~~~~~~~~~~~~~~~~~~~~~

.. index:: solver class


Also the solver class is derived from class ``Parameters`` and works with
the ``prms``, ``types``, and ``help`` dictionaries in the same way as class
``Problem``. Otherwise, the code is very similar to class ``Solver`` in
the ``decay_class.py`` file:


.. code-block:: python

        class Solver(Parameters):
            def __init__(self, problem):
                self.problem = problem
                self.prms = dict(dt=0.5, theta=0.5)
                self.types = dict(dt=float, theta=float)
                self.help = dict(dt='time step value',
                                 theta='time discretization parameter')
        
            def solve(self):
                from decay_mod import solver
                self.u, self.t = solver(
                    self.problem.get('I'),
                    self.problem.get('a'),
                    self.problem.get('T'),
                    self.get('dt'),
                    self.get('theta'))
        
            def error(self):
                try:
                    u_e = self.problem.exact_solution(self.t)
                    e = u_e - self.u
                    E = np.sqrt(self.get('dt')*np.sum(e**2))
                except AttributeError:
                    E = None
                return E


The visualizer class  (2)
~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: visualizer class


Class ``Visualizer`` can be identical to the one in the ``decay_class.py`` file
since the class does not need any parameters. However, a few
adjustments in the ``plot`` method is necessary since parameters are
accessed as, e.g., ``problem.get('T')`` rather than ``problem.T``.
The details are found in the file ``class_decay_oo.py``.

Finally, we need a function that solves a real problem using the
classes ``Problem``, ``Solver``, and ``Visualizer``. This function can
be just like ``main`` in the ``decay_class.py`` file.

The advantage with the ``Parameters`` class is that it scales to problems
with a large number of physical and numerical parameters:
as long as the parameters are defined once via a dictionary,
the compact code in class ``Parameters`` can handle any collection of
parameters of any size.

.. !split


.. _decay:experiments:

Performing scientific experiments
=================================


.. index:: numerical experiments

.. index:: scientific experiments




.. admonition:: Goal

   This section explores the behavior of a numerical
   method for a differential equation through computer experiments.
   In particular, it is shown how scientific experiments
   can be set up and reported. We address the ODE problem
   
   
   .. math::
      :label: decay:experiments:model
           
           u'(t) = -au(t),\quad u(0)=I,\quad t\in (0,T],
           
           
   
   numerically discretized by the :math:`\theta`-rule:
   
   
   .. math::
           
           u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n,
           \quad u^0=I\thinspace .
           
   
   Our aim is to plot :math:`u^0,u^1,\ldots,u^N`
   together with the exact solution :math:`u_{\small\mbox{e}} = Ie^{-at}`
   for various choices of the parameters in this numerical problem:
   :math:`I`, :math:`a`, :math:`\Delta t`, and :math:`\theta`. We are especially interested
   in how the discrete solution compares with the exact solution
   when the :math:`\Delta t` parameter is varied and :math:`\theta` takes on the three values
   corresponding to the Forward Euler, Backward Euler, and Crank-Nicolson
   schemes (:math:`\theta=0,1,0.5`, respectively).


Software
--------

A verified implementation for computing the numerical
solution :math:`u^n` and plotting it together
with the exact solution :math:`u_{\small\mbox{e}}` is found in the file
`decay_mod.py <http://tinyurl.com/jvzzcfn/decay/decay_mod.py>`_.
This program admits command-line arguments to specify a series of
:math:`\Delta t` values and will run a loop over these values and
:math:`\theta=0,0.5,1`. We make a slight edit of how the plots are
designed: the numerical solution is specified with line type ``'r--o'``
(dashed red lines with dots at the mesh points), and the ``show()``
command is removed to avoid a lot of plot windows popping up on
the computer screen (but hardcopies of the plot are still stored
in files via ``savefig``). The slightly
modified program has the name
`experiments/decay_mod.py <http://tinyurl.com/jvzzcfn/decay/experiments/decay_mod.py>`_.
All files associated with the scientific investigation are collected
in a subdirectory ``experiments``.

Running the experiments is easy since the ``decay_mod.py`` program
already has the loops over :math:`\theta` and :math:`\Delta t` implemented.
An experiment with :math:`I=1`, :math:`a=2`, :math:`T=5`, and :math:`dt=0.5, 0.25, 0.1, 0.05`
is run by


.. code-block:: console

        Terminal> python decay_mod.py --I 1 --a 2 --makeplot \ 
                  --T 5 --dt 0.5 0.25 0.1 0.05


Combining plot files  (2)
-------------------------

The ``decay_mod.py`` program generates a lot of image files, e.g.,
``FE_*.png``, ``BE_*.png``, and ``CN_*.png``.
We want to combine all the ``FE_*.png`` files in a table
fashion in one file, with two images in each row,
starting with the largest :math:`\Delta t` in the upper
left corner and decreasing the value as we go to the right and down.
This can be done using the `montage <http://www.imagemagick.org/script/montage.php>`_ program. The often occurring white areas around the plots can
be cropped away by the ``convert -trim`` command.
The remaining white can be made transparent for HTML pages with a
non-white background by the command ``convert -transparent white``.

Also plot files in the PDF format with names ``FE_*.pdf``, ``BE_*.pdf``,
and ``CN_*.pdf`` are generated and these should be combined using other
tools: ``pdftk`` to combine individual plots into one file with one plot
per page, and ``pdfnup`` to combine the pages into a table with multiple
plots per page. The resulting image often has some extra surrounding
white space that can be removed by the ``pdfcrop`` program.
The code snippets below contain all details about the
usage of ``montage``, ``convert``, ``pdftk``, ``pdfnup``, and ``pdfcrop``.



.. index:: script


Running manual commands is boring, and errors may easily
sneak in. Both for automating manual work and documenting the
operating system commands we actually issued in the experiment,
we should write a *script* (little program). An alternative is
to write the commands into an IPython notebook and use the
notebook as the script. A plain script as a standard Python
program in a separate text file will be used here.



.. admonition:: Reproducible science

   A script that automates running our computer experiments
   will ensure
   that the experiments can easily be rerun by ourselves or others in
   the future, either to check the results or redo the experiments with
   other input data. Also, whatever we did to produce the results is
   documented in every detail in the script.
   Automating scripts are therefore essential to making our
   research *reproducible*, which is a fundamental principle in science.


The script takes
a list of :math:`\Delta t` values on the command line as input and
makes three combined images, one for each :math:`\theta` value,
displaying the quality of the numerical solution as :math:`\Delta t`
varies. For example,

.. code-block:: console

        Terminal> python decay_exper0.py 0.5 0.25 0.1 0.05

results in images ``FE.png``, ``CN.png``, ``BE.png``,
``FE.pdf``, ``CN.pdf``, and ``BE.pdf``,
each with four plots corresponding to the four :math:`\Delta t` values.
Each plot compares the numerical solution with the exact one.
The latter image is shown in Figure :ref:`decay:experiments:fig:BE4a`.


.. _decay:experiments:fig:BE4a:

.. figure:: BE4a.png
   :width: 600

   *Illustration of the Backward Euler method for four time step values*


Ideally, the script should be scalable in the sense that it works for
any number of :math:`\Delta t` values, which is the case for this particular
implementation:


.. code-block:: python

        import os, sys
        
        def run_experiments(I=1, a=2, T=5):
            # The command line must contain dt values
            if len(sys.argv) > 1:
                dt_values = [float(arg) for arg in sys.argv[1:]]
            else:
                print 'Usage: %s dt1 dt2 dt3 ...' %  sys.argv[0]
                sys.exit(1)  # abort
        
            # Run module file as a stand-alone application
            cmd = 'python decay_mod.py --I %g --a %g --makeplot --T %g' % \
                  (I, a, T)
            dt_values_str = ' '.join([str(v) for v in dt_values])
            cmd += ' --dt %s' % dt_values_str
            print cmd
            failure = os.system(cmd)
            if failure:
                print 'Command failed:', cmd; sys.exit(1)
        
            # Combine images into rows with 2 plots in each row
            image_commands = []
            for method in 'BE', 'CN', 'FE':
                pdf_files = ' '.join(['%s_%g.pdf' % (method, dt)
                                      for dt in dt_values])
                png_files = ' '.join(['%s_%g.png' % (method, dt)
                                      for dt in dt_values])
                image_commands.append(
                    'montage -background white -geometry 100%' +
                    ' -tile 2x %s %s.png' % (png_files, method))
                image_commands.append(
                    'convert -trim %s.png %s.png' % (method, method))
                image_commands.append(
                    'convert %s.png -transparent white %s.png' %
                    (method, method))
                image_commands.append(
                    'pdftk %s output tmp.pdf' % pdf_files)
                num_rows = int(round(len(dt_values)/2.0))
                image_commands.append(
                    'pdfnup --nup 2x%d tmp.pdf' % num_rows)
                image_commands.append(
                    'pdfcrop tmp-nup.pdf %s.pdf' % method)
        
            for cmd in image_commands:
                print cmd
                failure = os.system(cmd)
                if failure:
                    print 'Command failed:', cmd; sys.exit(1)
        
            # Remove the files generated above and by decay_mod.py
            from glob import glob
            filenames = glob('*_*.png') + glob('*_*.pdf') + \
                        glob('*_*.eps') + glob('tmp*.pdf')
            for filename in filenames:
                os.remove(filename)
        
        if __name__ == '__main__':
            run_experiments()

This file is available as `experiments/decay_exper0.py <http://tinyurl.com/jvzzcfn/decay/experiments/decay_exper0.py>`_.


.. index:: Unix wildcard notation

.. index:: wildcard notation (Unix)


.. index:: os.system


We may comment upon many useful constructs in this script:

 * ``[float(arg) for arg in sys.argv[1:]]`` builds a list of real numbers
   from all the command-line arguments.

 * ``failure = os.system(cmd)`` runs an operating system command, e.g.,
   another program. The execution is successful only if ``failure`` is zero.

 * Unsuccessful execution usually makes it meaningless to continue
   the program, and therefore we abort the program with ``sys.exit(1)``.
   Any argument different from 0 signifies to the computer's operating system
   that our program stopped with a failure.

 * ``['%s_%s.png' % (method, dt) for dt in dt_values]`` builds a list of
   filenames from a list of numbers (``dt_values``).

 * All ``montage``, ``convert``, ``pdftk``, ``pdfnup``, and ``pdfcrop``
   commands for creating
   composite figures are stored in a
   list and later executed in a loop.

 * ``glob('*_*.png')`` returns a list of the names of all files in the
   current directory where the filename matches the `Unix wildcard notation <http://en.wikipedia.org/wiki/Glob_(programming)>`_
   ``*_*.png`` (meaning any text, underscore, any text, and then ``.png``).

 * ``os.remove(filename)`` removes the file with name ``filename``.

Interpreting output from other programs
---------------------------------------

Programs that run other programs, like ``decay_exper0.py`` does, will often
need to interpret output from those programs. Let us demonstrate how
this is done in Python by extracting the relations between :math:`\theta`,
:math:`\Delta t`, and the error :math:`E` as written to the terminal window
by the ``decay_mod.py`` program, when being executed by
``decay_exper0.py``. We will

  * read the output from the ``decay_mod.py`` program

  * interpret this output and store the :math:`E` values in arrays for each
    :math:`\theta` value

  * plot :math:`E` versus :math:`\Delta t`, for each :math:`\theta`, in a log-log plot

.. index:: subprocess (Python module)


.. index:: Popen (in subprocess module)


The simple ``os.system(cmd)`` call does not allow us to read the
output from running ``cmd``. Instead we need to invoke a bit more
involved procedure:


.. code-block:: python

        from subprocess import Popen, PIPE, STDOUT
        p = Popen(cmd, shell=True, stdout=PIPE, stderr=STDOUT)
        output, dummy = p.communicate()
        failure = p.returncode
        if failure:
            print 'Command failed:', cmd; sys.exit(1)

The command stored in ``cmd`` is run and all text that is written to
the standard output *and* the standard error is available in the
string ``output``. Or in other words, the text in ``output`` is what appeared in the
terminal window while running ``cmd``.

Our next task is to run through the ``output`` string, line by line,
and if the current line prints :math:`\theta`, :math:`\Delta t`, and :math:`E`,
we split the line into these three pieces and store the data.
The chosen storage structure is a dictionary ``errors`` with keys ``dt``
to hold the :math:`\Delta t` values in a list, and three :math:`\theta` keys to hold
the corresponding :math:`E` values in a list. The relevant code lines are


.. code-block:: python

        errors = {'dt': dt_values, 1: [], 0: [], 0.5: []}
        for line in output.splitlines():
            words = line.split()
            if words[0] in ('0.0', '0.5', '1.0'):  # line with E?
                # typical line: 0.0   1.25:    7.463E+00
                theta = float(words[0])
                E = float(words[2])
                errors[theta].append(E)

Note that we do not bother to store the :math:`\Delta t` values as we
read them from ``output``, because we already have these values in
the ``dt_values`` list.

We are now ready to plot :math:`E` versus :math:`\Delta t` for :math:`\theta=0,0.5,1`:


.. code-block:: python

        import matplotlib.pyplot as plt
        plt.loglog(errors['dt'], errors[0], 'ro-')
        plt.hold('on')
        plt.loglog(errors['dt'], errors[0.5], 'b+-')
        plt.loglog(errors['dt'], errors[1], 'gx-')
        plt.legend(['FE', 'CN', 'BE'], loc='upper left')
        plt.xlabel('log(time step)')
        plt.ylabel('log(error)')
        plt.title('Error vs time step')
        plt.savefig('error.png')
        plt.savefig('error.pdf')

Plots occasionally need some manual adjustments. Here, the axis of
the log-log plot look nicer if we adapt them strictly to the data,
see Figure :ref:`decay:exper:Evsdt`.
To this end, we need to compute :math:`\min E` and :math:`\max E`, and later
specify the extent of the axes:


.. code-block:: python

        # Find min/max for the axis
        E_min = 1E+20; E_max = -E_min
        for theta in 0, 0.5, 1:
            E_min = min(E_min, min(errors[theta]))
            E_max = max(E_max, max(errors[theta]))
        
        plt.loglog(errors['dt'], errors[0], 'ro-')
        ...
        plt.axis([min(dt_values), max(dt_values), E_min, E_max])
        ...



.. _decay:exper:Evsdt:

.. figure:: error_plot_improvement.png
   :width: 800

   *Default plot (left) and manually adjusted axes (right)*


The complete program, incorporating the code snippets above, is found
in `experiments/decay_exper1.py <http://tinyurl.com/jvzzcfn/decay/experiments/decay_exper1.py>`_.
This example can hopefully act as template for numerous
other occasions
where one needs to run experiments, extract data from the output
of programs, make plots, and combine several plots in a figure file.
The ``decay_exper1.py`` program
is organized as a module, and other files can then easily extend
the functionality, as illustrated in the next section.


.. _decay:exper:report:

Making a report
---------------

The results of running computer experiments are best documented in a
little report containing the problem to be solved, key code segments,
and the plots from a series of experiments. At least the part of the
report containing the plots should be automatically generated by the
script that performs the set of experiments, because in that script we
know exactly which input data that were used to generate a specific
plot, thereby ensuring that each figure is connected to the
right data. Take a look at an
example at `<http://tinyurl.com/k3sdbuv/writing_reports//sphinx-cloud/>`_  to see what we have in
mind.

Plain HTML
~~~~~~~~~~

Scientific reports can be written in a variety of formats. Here we
begin with the `HTML <http://en.wikipedia.org/wiki/HTML>`_ format
which allows efficient viewing of all the experiments in any web
browser. The program
`decay_exper1_html.py <http://tinyurl.com/jvzzcfn/decay/experiments/decay_exper1_html.py>`_ calls
``decay_exper1.py`` to perform the experiments and then runs
statements for creating an HTML file with a summary, a
section on the mathematical problem, a section on the numerical
method, a section on the ``solver`` function implementing the
method, and a section with subsections containing figures that show
the results of experiments where :math:`\Delta t` is varied for
:math:`\theta=0,0.5,1`. The mentioned
Python file contains all the details for writing
this `HTML report <http://tinyurl.com/k3sdbuv/writing_reports//_static/report_html.html.html>`_.
You can view the report on `<http://tinyurl.com/k3sdbuv/writing_reports//_static/report_html.html>`_.

HTML with MathJax
~~~~~~~~~~~~~~~~~

Scientific reports usually need mathematical formulas and hence
mathematical typesetting. In plain HTML, as used in the
``decay_exper1_html.py`` file, we have to use just the keyboard
characters to write mathematics. However, there is an extension to
HTML, called `MathJax <http://www.mathjax.org/>`_, which allows
formulas and equations to be typeset with LaTeX syntax and nicely
rendered in web browsers, see Figure
:ref:`decay:exper:report:fig:mathjax`.  A relatively small subset of
LaTeX environments is supported, but the syntax for formulas is quite
rich. Inline formulas are look like ``\( u'=-au \)`` while equations are
surrounded by ``$$`` signs.  Inside such signs, one can use ``\[ u'=-au
\]`` for unnumbered equations, or ``\begin{equation}`` and
``\end{equation}`` surrounding ``u'=-au`` for numbered equations, or
``\begin{equation}`` and ``\end{equation}`` for multiple aligned equations.  You
need to be familiar with `mathematical typesetting in LaTeX <http://en.wikibooks.org/wiki/LaTeX/Mathematics>`_.

The file `decay_exper1_mathjax.py <http://tinyurl.com/jvzzcfn/decay/experiments/decay_exper1_html.py>`_ contains all the
details for turning the previous plain HTML report into `web pages
with nicely typeset mathematics <http://tinyurl.com/k3sdbuv/writing_reports//_static/report_mathjax.html>`_.  The
`corresponding HTML code <http://tinyurl.com/k3sdbuv/writing_reports//_static/report_mathjax.html.html>`_ be studied
to see all details of the mathematical typesetting.


.. _decay:exper:report:fig:mathjax:

.. figure:: report_mathjax.png
   :width: 600

   *Report in HTML format with MathJax*


LaTeX
~~~~~

.. "http://en.wikibooks.org/wiki/LaTeX"


The *de facto* language for mathematical typesetting and scientific
report writing is `LaTeX <http://en.wikipedia.org/wiki/LaTeX>`_. A
number of very sophisticated packages have been added to the language
over a period of three decades, allowing very fine-tuned layout and
typesetting. For output in the `PDF format <http://tinyurl.com/k3sdbuv/writing_reports//_static/report.pdf>`_, see Figure
:ref:`decay:exper:report:fig:latex` for an example, LaTeX is the
definite choice when it comes to quality. The LaTeX language used to
write the reports has typically a lot of commands involving
`backslashes and braces <http://tinyurl.com/k3sdbuv/writing_reports//_static/report.tex.html>`_.  For output on
the web, using HTML (and not the PDF directly in the browser window),
LaTeX struggles with delivering high quality typesetting. Other tools,
especially Sphinx, give better results and can also produce
nice-looking PDFs.  The file ``decay_exper1_latex.py`` shows how to
generate the LaTeX source from a program.


.. _decay:exper:report:fig:latex:

.. figure:: report_latexpdf.png
   :width: 600

   *Report in PDF format generated from LaTeX source*


Sphinx
~~~~~~

.. give pointers to source pages


`Sphinx <http://sphinx.pocoo.org/>`_ is a typesetting language with
similarities to HTML and LaTeX, but with much less tagging. It has
recently become very popular for software documentation and
mathematical reports. Sphinx can utilize LaTeX for mathematical
formulas and equations (via MathJax or PNG images). Unfortunately, the
subset of LaTeX mathematics supported is less than in full MathJax (in
particular, numbering of multiple equations in an ``align`` type
environment is not supported).  The `Sphinx syntax <http://tinyurl.com/k3sdbuv/writing_reports//_static/report_sphinx.rst.html>`_ is an extension of
the reStructuredText language. An attractive feature of Sphinx is its
rich support for `fancy layout of web pages <http://tinyurl.com/k3sdbuv/writing_reports//_static/sphinx-cloud/index.html>`_. In particular,
Sphinx can easily be combined with various layout *themes* that give a
certain look and feel to the web site and that offers table of
contents, navigation, and search facilities, see Figure
:ref:`decay:exper:report:fig:sphinx`.


.. _decay:exper:report:fig:sphinx:

.. figure:: report_sphinx.png
   :width: 600

   *Report in HTML format generated from Sphinx source*


Markdown
~~~~~~~~

A recently popular format for easy writing of web pages is
`Markdown <http://daringfireball.net/projects/markdown/>`_.
Text is written very much like one would do in email, using
spacing and special characters to naturally format the code
instead of heavily tagging the text as in LaTeX and HTML.
With the tool `Pandoc <http://johnmacfarlane.net/pandoc/>`_ one
can go from Markdown to a variety of formats.
HTML is a common output format, but LaTeX, epub, XML,
OpenOffice, MediaWiki, and MS Word are some other possibilities.

Wiki formats
~~~~~~~~~~~~

A range of wiki formats are popular for creating notes on the web,
especially documents which allow groups of people to edit and add
content. Apart from `MediaWiki <http://www.mediawiki.org/wiki/MediaWiki>`_ (the wiki format used for
Wikipedia), wiki formats have no support for mathematical typesetting
and also limited tools for displaying computer code in nice ways.
Wiki formats are therefore less suitable for scientific reports compared
to the other formats mentioned here.

Doconce
~~~~~~~

Since it is difficult to choose the right tool or format for writing
a scientific report, it is advantageous to write the content in a
format that easily translates to LaTeX, HTML, Sphinx, Markdown,
and various wikis. `Doconce <https://github.com/hplgit/doconce>`_ is such
a tool. It is similar to Pandoc, but offers some special convenient
features for writing about mathematics and programming.
The `tagging is modest <http://tinyurl.com/k3sdbuv/writing_reports//_static/report.do.txt.html>`_,
somewhere between LaTeX and Markdown.
The program ``decay_exper_do.py`` demonstrates how to generate (and write)
Doconce code for a report.

Worked example
~~~~~~~~~~~~~~

The HTML, LaTeX (PDF), Sphinx, and Doconce formats for the scientific
report whose content is outlined above, are exemplified
with source codes and results at the
web pages associated with this teaching material:
`<http://tinyurl.com/k3sdbuv/writing_reports/>`_.

.. project with exploring instability (help with matplotlib contour plots, and maybe show such a plot)


.. _decay:exper:github:

Publishing a complete project
-----------------------------

A report documenting scientific investigations should be accompanied by
all the software and data used for the investigations so that others
have a possibility to redo the work and assess the qualify of the results.
This possibility is important for *reproducible research* and
hence reaching reliable scientific conclusions.

One way of documenting a complete project is to make a directory tree
with all relevant files. Preferably, the tree is published at
some project hosting site like `Bitbucket, GitHub, or Googlecode <http://hplgit.github.com/teamods/bitgit/html/>`_ so that others can download it
as a tarfile, zipfile, or clone the files directly using a version control
system like Mercurial or Git.
For the investigations outlined in the section :ref:`decay:exper:report`,
we can create a directory tree with files

.. code-block:: text


        setup.py
        ./src:
           decay_mod.py
        ./doc:
           ./src:
              decay_exper1_mathjax.py
              make_report.sh
              run.sh
           ./pub:
              report.html

The ``src`` directory holds source code (modules) to be reused in other projects,
the ``setup.py`` builds and installs such software,
the ``doc`` directory contains the documentation, with ``src`` for the
source of the documentation and ``pub`` for ready-made, published documentation.
The ``run.sh`` file is a simple Bash script listing the ``python`` command
we used to run ``decay_exper1_mathjax.py`` to generate the experiments and
the ``report.html`` file.

.. Point to Doconce version



.. !split

Exercises  (1)
==============



.. --- begin exercise ---


.. _decay:app:exer:cooling:schemes:

Exercise 1: Derive schemes for Newton's law of cooling
------------------------------------------------------

Show in detail how we can apply the ideas of the Forward Euler,
Backward Euler, and Crank-Nicolson discretizations to derive explicit
computational formulas for new temperature values in Newton's law of
cooling (see the section :ref:`decay:app:Newton:cooling`):


.. math::
   :label: decay:Newton:cooling
        
        {dT\over dt} = -k(T-T_s),\quad T(0)=T_0,
        
        

where :math:`T` is the temperature of the body, :math:`T_s` is the temperature
of the surroundings, :math:`t` is time, :math:`k` is the heat transfer
coefficient, and :math:`T_0` is the initial temperature of the body.

.. Can introduce :math:`u=T-T_s`, or much better for illustration of the

.. thinking: operate directly on the T equation

Filename: ``schemes_cooling.pdf``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:cooling:py:

Exercise 2: Implement schemes for Newton's law of cooling
---------------------------------------------------------

Formulate a :math:`\theta`-rule for the three schemes in :ref:`decay:app:exer:cooling:schemes` such that you can get the three
schemes from a single formula by varying the :math:`\theta` parameter.
Implement the :math:`\theta` scheme in a function ``cooling(T0, k, T_s,
t_end, dt, theta=0.5)``, where ``T0`` is the initial temperature, ``k`` is
the heat transfer coefficient, ``T_s`` is the temperature of the
surroundings, ``t_end`` is the end time of the simulation, ``dt`` is the
time step, and ``theta`` corresponds to :math:`\theta`.  The ``cooling``
function should return the temperature as an array ``T`` of values at
the mesh points and the time mesh ``t``.  Construct verification
examples to check that the implementation works.

.. --- begin hint in exercise ---


*Hint.* For verification, try to find an exact solution of the
discrete equations. A trick is to introduce :math:`u=T-T_s`, observe
that :math:`u^{n}=(T_0-T_s)A^n` for some amplification factor :math:`A`,
and then express this formula in terms of :math:`T^n`.

.. --- end hint in exercise ---

Filename: ``cooling.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:cooling:murder:

Exercise 3: Find time of murder from body temperature
-----------------------------------------------------

.. `<http://www.biology.arizona.edu/BioMath/tutorials/Applications/Cooling.html>`_


A detective measures the temperature of a dead body to be
26.7 C at 2 pm. One hour later
the temperature is 25.8 C. The question is when
death occurred.

Assume that Newton's law of cooling :eq:`decay:Newton:cooling`
is an appropriate mathematical
model for the evolution of the temperature in the body.
First, determine :math:`k` in :eq:`decay:Newton:cooling` by
formulating a Forward Euler approximation with one time steep
from time 2 am to time 3 am, where knowing the two temperatures
allows for finding :math:`k`. Thereafter, simulate the temperature evolution
from the time of murder, taken as :math:`t=0`, when :math:`T=37\hbox{ C}`,
until the temperature
reaches 25.8 C. The corresponding time allows for answering when
death occurred.
Filename: ``detective.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:intdiv:

Exercise 4: Experiment with integer division
--------------------------------------------

Explain what happens in the following computations, where
some are mathematically unexpected:

        >>> dt = 3
        >>> T = 8
        >>> Nt = T/dt
        >>> Nt
        2
        >>> theta = 1; a = 1
        >>> (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
        0

Filename: ``pyproblems.txt``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:decay1err:

Exercise 5: Experiment with wrong computations
----------------------------------------------

Consider the ``solver`` function in the `decay_v1.py <http://tinyurl.com/jvzzcfn/decay/decay_v1.py>`_ file
and the following call:

.. code-block:: python

        u, t = solver(I=1, a=1, T=7, dt=2, theta=1)

The output becomes

.. code-block:: python

        t= 0.000 u=1
        t= 2.000 u=0
        t= 4.000 u=0
        t= 6.000 u=0

Print out the result of all intermediate computations and use
``type(v)`` to see the object type of the result stored in ``v``.
Examine the intermediate calculations and explain
why ``u`` is wrong and why we compute up to :math:`t=6` only even though we
specified :math:`T=7`.
Filename: ``decay_v1_err.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:plot:error:

Exercise 6: Plot the error function
-----------------------------------

Solve the problem :math:`u'=-au`, :math:`u(0)=I`, using the Forward Euler, Backward
Euler, and Crank-Nicolson schemes. For each scheme, plot the error function
:math:`e^n = u_{\small\mbox{e}}(t_n)-u^n` for :math:`\Delta t`, :math:`\frac{1}{4}\Delta t`, and
:math:`\frac{1}{8}\Delta t`, where :math:`u_{\small\mbox{e}}` is the exact solution of the ODE and
:math:`u^n` is the numerical solution at mesh point :math:`t_n`.
Filename: ``decay_plot_error.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:plot:dtconst:

Exercise 7: Compare methods for a given time mesh
-------------------------------------------------

Make a program that imports the ``solver`` function from the
``decay_mod`` module and offers a function ``compare(dt, I, a)`` for
comparing, in a plot, the methods corresponding to :math:`\theta=0,0.5,1`
and the exact solution.  This plot shows the accuracy of the methods
for a given time mesh. Read input data for the problem from the
command line using appropriate functions in the ``decay_mod`` module
(the ``--dt`` option for giving several time step values can be reused:
just use the first time step value for the computations).
Filename: ``decay_compare_theta.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:inexact:output:

Exercise 8: Change formatting of numbers and debug
--------------------------------------------------

The `decay_memsave.py <http://tinyurl.com/jvzzcfn/decay/decay_memsave.py>`_ program
writes the time values and solution values to a file which looks
like

.. code-block:: text


        0.0000000000000000E+00  1.0000000000000000E+00
        2.0000000000000001E-01  8.3333333333333337E-01
        4.0000000000000002E-01  6.9444444444444453E-01
        6.0000000000000009E-01  5.7870370370370383E-01
        8.0000000000000004E-01  4.8225308641975323E-01
        1.0000000000000000E+00  4.0187757201646102E-01
        1.2000000000000000E+00  3.3489797668038418E-01
        1.3999999999999999E+00  2.7908164723365347E-01

Modify the file output such that it looks like

.. code-block:: python

        0.000  1.00000
        0.200  0.83333
        0.400  0.69444
        0.600  0.57870
        0.800  0.48225
        1.000  0.40188
        1.200  0.33490
        1.400  0.27908

Run the modified program

.. code-block:: console

        Terminal> python decay_memsave_v2.py --T 10 --theta 1 \ 
                  --dt 0.2 --makeplot

The program just prints ``Bug in the implementation!`` and does not
show the plot. What went wrong?
Filename: ``decay_memsave_v2.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:doctest1:

Problem 1: Write a doctest
--------------------------

Type in the following program and equip the ``roots`` function with a doctest:


.. code-block:: python

        import sys
        # This sqrt(x) returns real if x>0 and complex if x<0
        from numpy.lib.scimath import sqrt
        
        def roots(a, b, c):
            """
            Return the roots of the quadratic polynomial
            p(x) = a*x**2 + b*x + c.
        
            The roots are real or complex objects.
            """
            q = b**2 - 4*a*c
            r1 = (-b + sqrt(q))/(2*a)
            r2 = (-b - sqrt(q))/(2*a)
            return r1, r2
        
        a, b, c = [float(arg) for arg in sys.argv[1:]]
        print roots(a, b, c)

Make sure to test both real and complex roots.
Write out numbers with 14 digits or less.
Filename: ``doctest_roots.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:nosetest1:

Problem 2: Write a nose test
----------------------------

Make a nose test for the ``roots`` function in :ref:`decay:exer:doctest1`.
Filename: ``test_roots.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:module1:

Problem 3: Make a module
------------------------

Let

.. math::
         q(t) = \frac{RAe^{at}}{R + A(e^{at} - 1)}
        \thinspace .
        

Make a Python module ``q_module`` containing two functions ``q(t)`` and
``dqdt(t)`` for computing :math:`q(t)` and :math:`q'(t)`, respectively. Perform a
``from numpy import *`` in this module. Import ``q`` and ``dqdt`` in another
file using the "star import" construction ``from q_module import
*``. All objects available in this file is given by ``dir()``. Print
``dir()`` and ``len(dir())``.  Then change the import of ``numpy`` in
``q_module.py`` to ``import numpy as np``. What is the effect of this
import on the number of objects in ``dir()`` in a file that does ``from
q_module import *``?

.. \frac{du}{dt}=au\left(1-\frac{u}{R}\right),\quad u(0)=A,

Filename: ``q_module.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:decay_class:exper:

Exercise 9: Make use of a class implementation
----------------------------------------------

We want to solve the exponential decay problem :math:`u'=-au`, :math:`u(0)=I`,
for several :math:`\Delta t` values and :math:`\theta=0,0.5,1`.
For each :math:`\Delta t` value, we want to make a plot where the
three solutions corresponding to :math:`\theta=0,0.5,1` appear along with
the exact solution.
Write a function ``experiment`` to accomplish this. The function should
import the classes ``Problem``, ``Solver``, and ``Visualizer`` from the
`decay_class <http://tinyurl.com/jvzzcfn/decay/decay_class.py>`_
module and make use of these. A new command-line option ``--dt_values``
must be added to allow the user to specify the :math:`\Delta t` values on
the command line (the options ``--dt`` and ``--theta`` implemented
by the ``decay_class`` module have then no effect
when running the ``experiment`` function).
Note that the classes in the ``decay_class`` module should *not* be
modified.
Filename: ``decay_class_exper.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:decay_class2:

Exercise 10: Generalize a class implementation
----------------------------------------------

Consider the file `decay_class.py <http://tinyurl.com/jvzzcfn/decay/decay_class.py>`_
where the exponential decay problem :math:`u'=-au`, :math:`u(0)=I`, is implemented
via the classes ``Problem``, ``Solver``, and ``Visualizer``.
Extend the classes to handle the more general problem

.. math::
         u'(t) = -a(t)u(t) + b(t),\quad u(0)=I,\ t\in (0,T],

using the :math:`\theta`-rule for discretization.

In the case with arbitrary functions :math:`a(t)` and :math:`b(t)` the problem class
is no longer guaranteed to provide an exact solution. Let
the ``exact_solution`` in class ``Problem`` return ``None`` if the exact
solution for the particular problem is not available. Modify classes
``Solver`` and ``Visualizer`` accordingly.

Add test functions ``test_*()`` for the nose testing tool in the module.
Also add a demo example where the environment suddenly changes
(modeled as an abrupt change in the decay rate :math:`a`):

.. math::
         a(t) =\left\lbrace\begin{array}{ll}
        1, & 0\leq t\leq t_p,\\ 
        k, & t> t_p,\end{array}\right.
        

where :math:`t_p` is the point of time the environment changes. Take :math:`t_p=1`
and make plots that illustrate the effect of having :math:`k\gg 1` and :math:`k\ll 1`.
Filename: ``decay_class2.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:decay_class3:

Exercise 11: Generalize an advanced class implementation
--------------------------------------------------------

Solve :ref:`decay:exer:decay_class2` by utilizing the
class implementations in
`decay_class_oo.py <http://tinyurl.com/jvzzcfn/decay/decay_class_oo.py>`_.
Filename: ``decay_class3.py``.

.. --- end exercise ---



.. !split


.. _decay:analysis:

Analysis of finite difference equations
=======================================

We address the ODE for exponential decay,

.. math::
        
        u'(t) = -au(t),\quad u(0)=I,
        

where :math:`a` and :math:`I` are given constants. This problem is solved
by the :math:`\theta`-rule finite difference scheme, resulting in
the recursive equations

.. math::
   :label: decay:analysis:scheme
        
        u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
        
        

for the numerical solution :math:`u^{n+1}`, which approximates the exact
solution :math:`u_{\small\mbox{e}}` at time point :math:`t_{n+1}`. For constant mesh spacing,
which we assume here, :math:`t_{n+1}=(n+1)\Delta t`.


Discouraging numerical solutions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Choosing :math:`I=1`, :math:`a=2`, and running experiments with :math:`\theta =1,0.5, 0`
for :math:`\Delta t=1.25, 0.75, 0.5, 0.1`, gives the results in
Figures :ref:`decay:analysis:BE4c`, :ref:`decay:analysis:CN4c`, and
:ref:`decay:analysis:FE4c`.


.. _decay:analysis:BE4c:

.. figure:: BE4c.png
   :width: 600

   *Backward Euler*



.. _decay:analysis:CN4c:

.. figure:: CN4c.png
   :width: 600

   *Crank-Nicolson*



.. _decay:analysis:FE4c:

.. figure:: FE4c.png
   :width: 600

   *Forward Euler*


The characteristics of the displayed curves can be summarized as follows:

  * The Backward Euler scheme always gives a monotone solution, lying above
    the exact curve.

  * The Crank-Nicolson scheme gives the most accurate results, but for
    :math:`\Delta t=1.25` the solution oscillates.

  * The Forward Euler scheme gives a growing, oscillating solution for
    :math:`\Delta t=1.25`; a decaying, oscillating solution for :math:`\Delta t=0.75`;
    a strange solution :math:`u^n=0` for :math:`n\geq 1` when :math:`\Delta t=0.5`; and
    a solution seemingly as accurate as the one by the Backward Euler
    scheme for :math:`\Delta t = 0.1`, but the curve lies below the exact
    solution.

Since the exact solution of our model problem is a monotone function,
:math:`u(t)=Ie^{-at}`, some of these qualitatively wrong results are indeed alarming!



.. admonition:: Goal

   We ask the question
   
     * Under what circumstances, i.e., values of
       the input data :math:`I`, :math:`a`, and :math:`\Delta t` will the Forward Euler and
       Crank-Nicolson schemes result in undesired oscillatory solutions?
   
   The question will be investigated both by numerical experiments and
   by precise mathematical theory. The latter will help establish
   general critera on :math:`\Delta t` for avoiding non-physical oscillatory
   or growing solutions.
   
   Another question to be raised is
   
    * How does :math:`\Delta t` impact the error in the numerical solution?
   
   For our simple model problem we can answer this question very precisely, but
   we will also look at simplified formulas for small :math:`\Delta t`
   and touch upon important concepts such as *convergence rate* and
   *the order of a scheme*. Other fundamental concepts mentioned are
   stability, consistency, and convergence.


Experimental investigation of oscillatory solutions
---------------------------------------------------

To address the first question above,
we may set up an experiment where we loop over values of :math:`I`, :math:`a`,
and :math:`\Delta t`. For each experiment, we flag the solution as
oscillatory if

.. math::
         u^{n} > u^{n-1},

for some value of :math:`n`,
since we expect :math:`u^n` to decay with :math:`n`, but oscillations make
:math:`u` increase over a time step. We will quickly see that
oscillations are independent of :math:`I`, but do depend on :math:`a` and
:math:`\Delta t`. Therefore, we introduce a two-dimensional
function :math:`B(a,\Delta t)` which is 1 if oscillations occur
and 0 otherwise. We can visualize :math:`B` as a contour plot
(lines for which :math:`B=\hbox{const}`). The contour :math:`B=0.5`
corresponds to the borderline between oscillatory regions with :math:`B=1`
and monotone regions with :math:`B=0` in the :math:`a,\Delta t` plane.

The :math:`B` function is defined at discrete :math:`a` and :math:`\Delta t` values.
Say we have given :math:`P` $a$ values, :math:`a_0,\ldots,a_{P-1}`, and
:math:`Q` $\Delta t$ values, :math:`\Delta t_0,\ldots,\Delta t_{Q-1}`.
These :math:`a_i` and :math:`\Delta t_j` values, :math:`i=0,\ldots,P-1`,
:math:`j=0,\ldots,Q-1`, form a rectangular mesh of :math:`P\times Q` points
in the plane. At each point :math:`(a_i, \Delta t_j)`, we associate
the corresponding value of :math:`B(a_i,\Delta t_j)`, denoted :math:`B_{ij}`.
The :math:`B_{ij}` values are naturally stored in a two-dimensional
array. We can thereafter create a plot of the
contour line :math:`B_{ij}=0.5` dividing the oscillatory and monotone
regions. The file `decay_osc_regions.py <http://tinyurl.com/jvzzcfn/decay/decay_osc_regions.py>`_  ``osc_regions`` stands for "oscillatory regions") contains all nuts and
bolts to produce the :math:`B=0.5` line in Figures :ref:`decay:analysis:B:FE`
and :ref:`decay:analysis:B:CN`. The oscillatory region is above this line.


.. code-block:: python

        from decay_mod import solver
        import numpy as np
        import scitools.std as st
        
        def non_physical_behavior(I, a, T, dt, theta):
            """
            Given lists/arrays a and dt, and numbers I, dt, and theta,
            make a two-dimensional contour line B=0.5, where B=1>0.5
            means oscillatory (unstable) solution, and B=0<0.5 means
            monotone solution of u'=-au.
            """
            a = np.asarray(a); dt = np.asarray(dt)  # must be arrays
            B = np.zeros((len(a), len(dt)))         # results
            for i in range(len(a)):
                for j in range(len(dt)):
                    u, t = solver(I, a[i], T, dt[j], theta)
                    # Does u have the right monotone decay properties?
                    correct_qualitative_behavior = True
                    for n in range(1, len(u)):
                        if u[n] > u[n-1]:  # Not decaying?
                            correct_qualitative_behavior = False
                            break  # Jump out of loop
                    B[i,j] = float(correct_qualitative_behavior)
            a_, dt_ = st.ndgrid(a, dt)  # make mesh of a and dt values
            st.contour(a_, dt_, B, 1)
            st.grid('on')
            st.title('theta=%g' % theta)
            st.xlabel('a'); st.ylabel('dt')
            st.savefig('osc_region_theta_%s.png' % theta)
            st.savefig('osc_region_theta_%s.pdf' % theta)
        
        non_physical_behavior(
            I=1,
            a=np.linspace(0.01, 4, 22),
            dt=np.linspace(0.01, 4, 22),
            T=6,
            theta=0.5)



.. _decay:analysis:B:FE:

.. figure:: osc_region_FE.png
   :width: 500

   *Forward Euler scheme: oscillatory solutions occur for points above the curve*



.. _decay:analysis:B:CN:

.. figure:: osc_region_CN.png
   :width: 500

   *Crank-Nicolson scheme: oscillatory solutions occur for points above the curve*


By looking at the curves in the figures one may guess that :math:`a\Delta t`
must be less than a critical limit to avoid the undesired
oscillations.  This limit seems to be about 2 for Crank-Nicolson and 1
for Forward Euler.  We shall now establish a precise mathematical
analysis of the discrete model that can explain the observations in
our numerical experiments.

Exact numerical solution
------------------------

Starting with :math:`u^0=I`, the simple recursion :eq:`decay:analysis:scheme`
can be applied repeatedly :math:`n` times, with the result that

.. math::
   :label: decay:analysis:unex
        
        u^{n+1} = IA^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\thinspace .
        
        


<function admon at 0x256c578>('notice', 'Solving difference equations', r"""
Difference equations where all terms are linear in
:math:`u^{n+1}`, :math:`u^n`, and maybe :math:`u^{n-1}`, :math:`u^{n-2}`, etc., are
called *homogeneous, linear* difference equations, and their solutions
are generally of the form :math:`u^n=A^n`. Inserting this expression
and dividing by :math:`A^{n+1}` gives
a polynomial equation in :math:`A`. In the present case we get

.. math::
         A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\thinspace .

This is a solution technique of wider applicability than repeated use of
the recursion :eq:`decay:analysis:scheme`.
""")

Regardless of the solution approach, we have obtained a formula for
:math:`u^n`.  This formula can explain everything what we see in the figures
above, but it also gives us a more general insight into accuracy and
stability properties of the three schemes.

Stability
---------


.. index:: stability


Since :math:`u^n` is a factor :math:`A`
raised to an integer power :math:`n`, we realize that :math:`A<0`
will for odd powers imply :math:`u^n<0` and for even power result in :math:`u^n>0`.
That is, the solution oscillates between the mesh points.
We have oscillations due to :math:`A<0` when


.. math::
   :label: decay:th:stability
        
        (1-\theta)a\Delta t > 1 \thinspace .
        
        

Since :math:`A>0` is a requirement for having a numerical solution with the
same basic property (monotonicity) as the exact solution, we may say
that :math:`A>0` is a *stability criterion*. Expressed in terms of :math:`\Delta t`
the stability criterion reads


.. math::
        
        \Delta t < \frac{1}{(1-\theta)a}\thinspace .
        


The Backward
Euler scheme is always stable since :math:`A<0` is impossible for :math:`\theta=1`, while
non-oscillating solutions for Forward Euler and Crank-Nicolson
demand :math:`\Delta t\leq 1/a` and :math:`\Delta t\leq 2/a`, respectively.
The relation between :math:`\Delta t` and :math:`a` look reasonable: a smaller
:math:`a` means faster decay and hence a need for smaller time steps.

Looking at Figure :ref:`decay:analysis:FE4c`, we see that with :math:`a\Delta
t= 2\cdot 1.25=2.5`, :math:`A=-1.5`, and the solution :math:`u^n=(-1.5)^n`
oscillates *and* grows. With :math:`a\Delta t = 2\cdot 0.75=1.5`, :math:`A=-0.5`,
:math:`u^n=(-0.5)^n` decays but oscillates. The peculiar case :math:`\Delta t =
0.5`, where the Forward Euler scheme produces a solution that is stuck
on the :math:`t` axis, corresponds to :math:`A=0` and therefore :math:`u^0=I=1` and
:math:`u^n=0` for :math:`n\geq 1`.  The decaying oscillations in the Crank-Nicolson scheme
for :math:`\Delta t=1.25` are easily explained by the fact that :math:`A=-0.25`.


.. index:: amplification factor


The factor :math:`A` is called the *amplification factor* since the solution
at a new time level is :math:`A` times the solution at the previous time
level. For a decay process, we must obviously have :math:`|A|\leq 1`, which
is fulfilled for all :math:`\Delta t` if :math:`\theta \geq 1/2`. Arbitrarily
large values of :math:`u` can be generated when :math:`|A|>1` and :math:`n` is large
enough. The numerical solution is in such cases totally irrelevant to
an ODE modeling decay processes!


.. index:: A-stable methods


.. index:: L-stable methods




.. admonition:: Stability properties

   We may summarize the stability investigations as follows:
   
   1. The Forward Euler method is a *conditionally stable* scheme because
      it requires :math:`\Delta t < 2/a` for avoiding growing solutions
      and :math:`\Delta t < 1/a` for avoiding oscillatory solutions.
   
   2. The Crank-Nicolson is *unconditionally stable* with respect to
      growing solutions, while it is conditionally stable with
      the criterion :math:`\Delta t < 2/a` for avoiding oscillatory solutions.
   
   3. The Backward Euler method is unconditionally stable with respect
      to growing and oscillatory solutions - any :math:`\Delta t` will work.
   
   Much literature on ODEs speaks about L-stable and A-stable methods.
   In our case A-stable methods ensures non-growing solutions, while
   L-stable methods also avoids oscillatory solutions.


Comparing amplification factors
-------------------------------

After establishing how :math:`A` impacts the qualitative features of the
solution, we shall now look more into how well the numerical amplification
factor approximates the exact one. The exact solution reads
:math:`u(t)=Ie^{-at}`, which can be rewritten as

.. math::
        
        {u_{\small\mbox{e}}}(t_n) = Ie^{-a n\Delta t} = I(e^{-a\Delta t})^n \thinspace .
        

From this formula we see that the exact amplification factor is

.. math::
        
        A_{\small\mbox{e}} = e^{-a\Delta t} \thinspace .
        


We realize that the exact and numerical amplification factors depend
on :math:`a` and :math:`\Delta t` through the product :math:`a\Delta t`. Therefore, it
is convenient to introduce a symbol for this product, :math:`p=a\Delta t`,
and view :math:`A` and :math:`A_{\small\mbox{e}}` as functions of :math:`p`. Figure
:ref:`decay:analysis:fig:A` shows these functions. Crank-Nicolson is
clearly closest to the exact amplification factor, but that method has
the unfortunate oscillatory behavior when :math:`p>2`.


.. _decay:analysis:fig:A:

.. figure:: A_factors.png
   :width: 500

   *Comparison of amplification factors*



Series expansion of amplification factors
-----------------------------------------

As an alternative to the visual understanding inherent in Figure
:ref:`decay:analysis:fig:A`, there is a strong tradition in numerical
analysis to establish formulas for the approximation errors when the
discretization parameter, here :math:`\Delta t`, becomes small. In the
present case we let :math:`p` be our small discretization parameter, and it
makes sense to simplify the expressions for :math:`A` and :math:`A_{\small\mbox{e}}` by using
Taylor polynomials around :math:`p=0`.  The Taylor polynomials are accurate
for small :math:`p` and greatly simplifies the comparison of the analytical
expressions since we then can compare polynomials, term by term.

Calculating the Taylor series for :math:`A_{\small\mbox{e}}` is easily done by hand, but
the three versions of :math:`A` for :math:`\theta=0,1,\frac{1}{2}` lead to more
cumbersome calculations.
Nowadays, analytical computations can benefit greatly by
symbolic computer algebra software. The Python package ``sympy``
represents a powerful computer algebra system, not yet as sophisticated as
the famous Maple and Mathematica systems, but free and
very easy to integrate with our numerical computations in Python.

When using ``sympy``, it is convenient to enter the interactive Python
mode where we can write expressions and statements and immediately see
the results.  Here is a simple example. We strongly recommend to use
``isympy`` (or ``ipython``) for such interactive sessions.

Let us write ``sympy`` (or ``isympy``) in a standard Python shell
(or ``ipython``) and show how
we can find the Taylor series for :math:`e^{-p}`:


        >>> from sympy import *
        >>> # Create p as a mathematical symbol with name 'p'
        >>> p = Symbol('p')
        >>> # Create a mathematical expression with p
        >>> A_e = exp(-p)
        >>>
        >>> # Find the first 6 terms of the Taylor series of A_e
        >>> A_e.series(p, 6)
        1 + (1/2)*p**2 - p - 1/6*p**3 - 1/120*p**5 + (1/24)*p**4 + O(p**6)

Lines with ``>>>`` represent input lines and lines without
this prompt represents the result of computations (note that
``isympy`` and ``ipython`` apply other prompts, but in this text
we always apply ``>>>`` for interactive Python computing).
Apart from the order of the powers, the computed formula is easily
recognized as the beginning of the Taylor series for :math:`e^{-p}`.

Let us define the numerical amplification factor where :math:`p` and :math:`\theta`
enter the formula as symbols:

        >>> theta = Symbol('theta')
        >>> A = (1-(1-theta)*p)/(1+theta*p)

To work with the factor for the Backward Euler scheme we
can substitute the value 1 for ``theta``:

        >>> A.subs(theta, 1)
        1/(1 + p)

Similarly, we can replace ``theta`` by 1/2 for Crank-Nicolson,
preferably using an exact rational representation of 1/2 in ``sympy``:

        >>> half = Rational(1,2)
        >>> A.subs(theta, half)
        1/(1 + (1/2)*p)*(1 - 1/2*p)


The Taylor series of the amplification factor for the Crank-Nicolson
scheme can be computed as

        >>> A.subs(theta, half).series(p, 4)
        1 + (1/2)*p**2 - p - 1/4*p**3 + O(p**4)

We are now in a position to compare Taylor series:

        >>> FE = A_e.series(p, 4) - A.subs(theta, 0).series(p, 4)
        >>> BE = A_e.series(p, 4) - A.subs(theta, 1).series(p, 4)
        >>> CN = A_e.series(p, 4) - A.subs(theta, half).series(p, 4)
        >>> FE
        (1/2)*p**2 - 1/6*p**3 + O(p**4)
        >>> BE
        -1/2*p**2 + (5/6)*p**3 + O(p**4)
        >>> CN
        (1/12)*p**3 + O(p**4)

From these expressions we see that the error :math:`A-A_{\small\mbox{e}}\sim {{\cal O}(p^2)}`
for the Forward and Backward Euler schemes, while
:math:`A-A_{\small\mbox{e}}\sim {{\cal O}(p^3)}` for the Crank-Nicolson scheme.
It is the *leading order term*,
i.e., the term of the lowest order (polynomial degree),
that is of interest, because as :math:`p\rightarrow 0`, this term is
(much) bigger than the higher-order terms (think of :math:`p=0.01`:
:math:`p` is a hundred times larger than :math:`p^2`).

Now, :math:`a` is a given parameter in the problem, while :math:`\Delta t` is
what we can vary. One therefore usually writes the error expressions in
terms :math:`\Delta t`. When then have

.. math::
        
        A-A_{\small\mbox{e}} = \left\lbrace\begin{array}{ll}
        {{\cal O}(\Delta t^2)}, & \hbox{Forward and Backward Euler},\\ 
        {{\cal O}(\Delta t^3)}, & \hbox{Crank-Nicolson}
        \end{array}\right.
        


We say that the Crank-Nicolson scheme has an error in the amplification
factor of order :math:`\Delta t^3`, while the two other schemes are
of order :math:`\Delta t^2` in the same quantity.
What is the significance of the order expression? If we halve :math:`\Delta t`,
the error in amplification factor at a time level will be reduced
by a factor of 4 in the Forward and Backward Euler schemes, and by
a factor of 8 in the Crank-Nicolson scheme. That is, as we
reduce :math:`\Delta t` to obtain more accurate results, the Crank-Nicolson
scheme reduces the error more efficiently than the other schemes.


The fraction of numerical and exact amplification factors
---------------------------------------------------------

An alternative comparison of the schemes is to look at the
ratio :math:`A/A_{\small\mbox{e}}`, or the error :math:`1-A/A_{\small\mbox{e}}` in this ratio:

        >>> FE = 1 - (A.subs(theta, 0)/A_e).series(p, 4)
        >>> BE = 1 - (A.subs(theta, 1)/A_e).series(p, 4)
        >>> CN = 1 - (A.subs(theta, half)/A_e).series(p, 4)
        >>> FE
        (1/2)*p**2 + (1/3)*p**3 + O(p**4)
        >>> BE
        -1/2*p**2 + (1/3)*p**3 + O(p**4)
        >>> CN
        (1/12)*p**3 + O(p**4)

The leading-order terms have the same powers as
in the analysis of :math:`A-A_{\small\mbox{e}}`.

.. _decay:analysis:gobal:error:

The true error at a point
-------------------------

The error in the amplification factor reflects the error when
progressing from time level :math:`t_n` to :math:`t_{n-1}`.
To investigate the real error at a point, known as the *global error*,
we look at :math:`e^n = u^n-u_{\small\mbox{e}}(t_n)` for some :math:`n` and Taylor expand the
mathematical expressions as functions of :math:`p=a\Delta t`:

        >>> n = Symbol('n')
        >>> u_e = exp(-p*n)
        >>> u_n = A**n
        >>> FE = u_e.series(p, 4) - u_n.subs(theta, 0).series(p, 4)
        >>> BE = u_e.series(p, 4) - u_n.subs(theta, 1).series(p, 4)
        >>> CN = u_e.series(p, 4) - u_n.subs(theta, half).series(p, 4)
        >>> FE
        (1/2)*n*p**2 - 1/2*n**2*p**3 + (1/3)*n*p**3 + O(p**4)
        >>> BE
        (1/2)*n**2*p**3 - 1/2*n*p**2 + (1/3)*n*p**3 + O(p**4)
        >>> CN
        (1/12)*n*p**3 + O(p**4)

For a fixed time :math:`t`, the parameter :math:`n` in these expressions increases
as :math:`p\rightarrow 0` since :math:`t=n\Delta t =\mbox{const}` and hence
:math:`n` must increase like :math:`\Delta t^{-1}`. With :math:`n` substituted by
:math:`t/\Delta t` in
the leading-order error terms, these become :math:`\frac{1}{2}na^2\Delta
t^2 = \frac{1}{2}ta^2t\Delta t` for the Forward and Backward Euler
scheme, and :math:`\frac{1}{12}na^3\Delta t^3 = \frac{1}{12}ta^3\Delta t^2`
for the Crank-Nicolson scheme.  The global error is therefore of
second order (in :math:`\Delta t`) for the latter scheme and of first order for
the former schemes.

When the global error :math:`e^n\rightarrow 0` as :math:`\Delta t\rightarrow 0`,
we say that the scheme is *convergent*. It means that the numerical
solution approaches the exact solution as the mesh is refined, and
this is a much desired property of a numerical method.

Integrated errors
-----------------

It is common to study the norm of the numerical error, as
explained in detail in the section :ref:`decay:computing:error:norm`.
Let us calculate the discrete :math:`\ell^2` norm of the error mesh
function :math:`e^n`:


.. math::
         ||e^n||_{\ell^2} = \sqrt{\Delta t\sum_{n=0}^{N_t} ({u_{\small\mbox{e}}}(t_n) - u^n)^2}
        {\thinspace .} 

We have obtained an exact analytical expressions for the error at :math:`t=t_n`,
but here we use the leading-order error term only. For the Forward Euler scheme,
:math:`u_{\small\mbox{e}}(t_n) - u^n \approx \frac{1}{2}np^2`, giving


.. math::
         ||e^n||_{\ell^2}^2 = \Delta t\sum_{n=0}^{N_t} \frac{1}{4}n^2p^4
        =\Delta t\frac{1}{4}p^4 \sum_{n=0}^{N_t} n^2{\thinspace .}

Now, :math:`\sum_{n=0}^{N_t} n^2\approx \frac{1}{3}N_t^3`. Using this approximation
and :math:`N_t =T/\Delta_t` gives the expression


.. math::
         ||e^n||_{\ell^2} = \frac{1}{4}\sqrt{\frac{T^3}{3}} a^2\Delta t{\thinspace .}

Calculations for the Backward Euler scheme are very similar and provide
the same result, while the Crank-Nicolson scheme leads to


.. math::
         ||e^n||_{\ell^2} = \frac{1}{12}\sqrt{\frac{T^3}{3}}a^3\Delta t^2{\thinspace .}




.. admonition:: Summary of errors

   Both the pointwise and the time-integrated true errors are of
   second order in :math:`\Delta t` for the Crank-Nicolson scheme and of
   first order in :math:`\Delta t` for the Forward Euler and Backward Euler schemes.


Truncation error
----------------

The truncation error is a very frequently used error measure for
finite difference methods. It is defined as *the error
in the difference equation that arises when inserting the exact
solution*. Contrary to many other error measures, e.g., the
true error :math:`e^n=u_{\small\mbox{e}}(t_n)-u^n`, the truncation error is a quantity that
is easily computable.

Let us illustrate the calculation of the truncation error
for the Forward Euler scheme.
We start with the difference equation on operator form,


.. math::
         \lbrack D_t u = -au\rbrack^n,

i.e.,


.. math::
         \frac{u^{n+1}-u^n}{\Delta t} = -au^n{\thinspace .}

The idea is to see how well the exact solution :math:`u_{\small\mbox{e}}(t)` fulfills
this equation. Since :math:`u_{\small\mbox{e}}(t)` in general will not fullfill the
discrete equation, error in the discrete equation, called
a *residual*, denoted here by :math:`R^n`:


.. math::
   :label: decay:analysis:trunc:Req
        
        R^n = \frac{u_{\small\mbox{e}}(t_{n+1})-u_{\small\mbox{e}}(t_n)}{\Delta t} + au_{\small\mbox{e}}(t_n)
        {\thinspace .}
        
        

The residual is defined at each mesh point and is therefore a mesh
function with a superscript :math:`n`.

The interesting feature of :math:`R^n` is to see how it
depends on the discretization parameter :math:`\Delta t`.
The tool for reaching
this goal is to Taylor expand :math:`u_{\small\mbox{e}}` around the point where the
difference equation is supposed to hold, here :math:`t=t_n`.
We have that


.. math::
         u_{\small\mbox{e}}(t_{n+1}) = u_{\small\mbox{e}}(t_n) + u_{\small\mbox{e}}'(t_n)\Delta t + \frac{1}{2}u_{\small\mbox{e}}''(t_n)
        \Delta t^2 + \cdots 

Inserting this Taylor series in :eq:`decay:analysis:trunc:Req` gives


.. math::
         R^n = u_{\small\mbox{e}}'(t_n) + \frac{1}{2}u_{\small\mbox{e}}''(t_n)\Delta t + \ldots + au_{\small\mbox{e}}(t_n){\thinspace .}

Now, :math:`u_{\small\mbox{e}}` fulfills the ODE :math:`u_{\small\mbox{e}}'=-au_{\small\mbox{e}}` such that the first and last
term cancels and we have


.. math::
         R^n \approx \frac{1}{2}u_{\small\mbox{e}}''(t_n)\Delta t {\thinspace .} 

This :math:`R^n` is the *truncation error*, which for the Forward Euler is seen
to be of first order in :math:`\Delta t`.

The above procedure can be repeated for the Backward Euler and the
Crank-Nicolson schemes. We start with the scheme in operator notation,
write it out in detail, Taylor expand :math:`u_{\small\mbox{e}}` around the point :math:`\tilde t`
at which the difference equation is defined, collect terms that
correspond to the ODE (here :math:`u_{\small\mbox{e}}' + au_{\small\mbox{e}}`), and identify the remaining
terms as the residual :math:`R`, which is the truncation error.
The Backward Euler scheme leads to


.. math::
         R^n \approx -\frac{1}{2}u_{\small\mbox{e}}''(t_n)\Delta t, 

while the Crank-Nicolson scheme gives


.. math::
         R^{n+1/2} \approx \frac{1}{24}u_{\small\mbox{e}}'''(t_{n+\frac{1}{2}})\Delta t^2{\thinspace .}


The *order* :math:`r` of a finite difference scheme is often defined through
the leading term :math:`\Delta t^r` in the trunction error. The above
expressions point out that the Forward and Backward Euler schemes are
of first order, while Crank-Nicolson is of second order.  We have
looked at other error measures in other sections, like the error in
amplification factor and the error :math:`e^n=u_{\small\mbox{e}}(t_n)-u^n`, and expressed
these error measures in terms of :math:`\Delta t` to see the order of the
method. Normally, calculating the truncation error is more
straightforward than deriving the expressions for other error measures
and therefore the easiest way to establish the order of a scheme.

Consistency, stability, and convergence
---------------------------------------


.. index:: consistency

.. index:: stability

.. index:: convergence


Three fundamental concepts when solving differential equations by
numerical methods are consistency, stability, and convergence.  We
shall briefly touch these concepts below in the context of the present
model problem.

Consistency means that the error in the difference equation, measured
through the truncation error, goes to zero as :math:`\Delta t\rightarrow
0`. Since the truncation error tells how well the exact solution
fulfills the difference equation, and the exact solution fulfills the
differential equation, consistency ensures that the difference
equation approaches the differential equation in the limit. The
expressions for the truncation errors in the previous section are all
proportional to :math:`\Delta t` or :math:`\Delta t^2`, hence they vanish as
:math:`\Delta t\rightarrow 0`, and all the schemes are consistent.  Lack of
consistency implies that we actually solve a different differential
equation in the limit :math:`\Delta t\rightarrow 0` than we aim at.

Stability means that the numerical solution exhibits the same
qualitative properties as the exact solution. This is obviously a
feature we want the numerical solution to have. In the present
exponential decay model, the exact solution is monotone and
decaying. An increasing numerical solution is not in accordance with
the decaying nature of the exact solution and hence unstable. We can
also say that an oscillating numerical solution lacks the property of
monotonicity of the exact solution and is also unstable. We have seen
that the Backward Euler scheme always leads to monotone and decaying
solutions, regardless of :math:`\Delta t`, and is hence stable. The Forward
Euler scheme can lead to increasing solutions and oscillating
solutions if :math:`\Delta t` is too large and is therefore unstable unless
:math:`\Delta t` is sufficiently small.  The Crank-Nicolson can never lead
to increasing solutions and has no problem to fulfill that stability
property, but it can produce oscillating solutions and is unstable in
that sense, unless :math:`\Delta t` is sufficiently small.

Convergence implies that the global (true) error mesh function :math:`e^n =
u_{\small\mbox{e}}(t_n)-u^n\rightarrow 0` as :math:`\Delta t\rightarrow 0`. This is really
what we want: the numerical solution gets as close to the exact
solution as we request by having a sufficiently fine mesh.

Convergence is hard to establish theoretically, except in quite simple
problems like the present one. Stability and consistency are much
easier to calculate. A major breakthrough in the understanding of
numerical methods for differential equations came in 1956 when Lax and
Richtmeyer established equivalence between convergence on one hand and
consistency and stability on the other (the `Lax equivalance theorem <http://en.wikipedia.org/wiki/Lax_equivalence_theorem>`_).  In practice
it meant that one can first establish that a method is stable and
consistent, and then it is automatically convergent (which is much
harder to establish).  The result holds for linear problems only, and
in the world of nonlinear differential equations the relations between
consistency, stability, and convergence are much more complicated.

We have seen in the previous analysis that the Forward Euler,
Backward Euler, and Crank-Nicolson schemes are convergent (:math:`e^n\rightarrow 0`),
that they are consistent (:math:`R^n\rightarrow 0`, and that they are
stable under certain conditions on the size of :math:`\Delta t`.
We have also derived explicit mathematical expressions for :math:`e^n`,
the truncation error, and the stability criteria.

.. Look in Asher and Petzold, p 40


Exercises  (2)
==============



.. --- begin exercise ---


.. _decay:analysis:exer:fd:exp:plot:

Exercise 12: Visualize the accuracy of finite differences :math:`u=e^{-at}`
---------------------------------------------------------------------------

The purpose of this exercise is to visualize the accuracy of finite difference
approximations of the derivative of a given function.
For any finite difference approximation, take the Forward Euler difference
as an example, and any specific function, take  :math:`u=e^{-at}`,
we may introduce an error fraction
specific

.. math::
         E = \frac{[D_t^+ u]^n}{u'(t_n)} = \frac{\exp{(-a(t_n+\Delta t))} - \exp{(-at_n)}}{-a\exp{(-at_n)}} = -\frac{1}{a\Delta t}\left(\exp{(-a\Delta t)}  - 1\right),
        

and view :math:`E` as a function of :math:`\Delta t`. We expect that
:math:`\lim_{\Delta t\rightarrow 0}E=1`, while :math:`E` may deviate significantly from
unit for large :math:`\Delta t`. How the error depends on :math:`\Delta t` is best
visualized in a graph where we use a logarithmic scale on for :math:`\Delta t`,
so we can cover many orders of magnitude of that quantity. Here is
a code segment creating an array of 100 intervals, on the logarithmic
scale, ranging from :math:`10^{-6}` to :math:`1` and then plotting :math:`E` versus
:math:`p=a\Delta t` with logarithmic scale on the :math:`\Delta t` axis:


.. code-block:: python

        from numpy import logspace, exp
        from matplotlib.pyplot import plot
        p = logspace(-6, 1, 101)
        y = -(exp(-p)-1)/p
        semilog(p, y)

Illustrate such errors for the finite difference operators :math:`[D_t^+u]^n`
(forward), :math:`[D_t^-u]^n` (backward), and :math:`[D_t u]^n` (centered).

Perform a Taylor series expansions of the error fractions and find
the leading order :math:`r` in the expressions of type
:math:`1 + C\Delta t^r + {{\cal O}(\Delta t^{r+1)}}`, where :math:`C` is some constant.
Filename: ``decay_plot_fd_exp_error.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:analysis:exer:growth:

Exercise 13: Explore the :math:`\theta`-rule for exponential growth
-------------------------------------------------------------------

This exercise asks you to solve the ODE :math:`u'=-au` with :math:`a<0`
such that the ODE models
exponential growth instead of exponential decay.
A central theme is to investigate numerical artifacts and non-physical
solution behavior.


*a)* Run experiments with :math:`\theta` and :math:`\Delta t` to uncover numerical
artifacts (the exact solution is a monotone, growing function).
Use the insight to design a set of experiments that aims to
demonstrate all types of numerical artifacts for different choices
of :math:`\Delta t` while :math:`a` is fixed.

.. --- begin hint in exercise ---


*Hint.* Modify the ``decay_exper1.py`` code to suit your needs.

.. --- end hint in exercise ---

Filename: ``growth_exper.py``.

*b)* Write a scientific report about the findings.

.. --- begin hint in exercise ---


*Hint.* Use examples from the section :ref:`decay:exper:report` to
see how scientific reports can be written.

.. --- end hint in exercise ---

Filenames: ``growth_exper.pdf``, ``growth_exper.html``.

*c)* Plot the amplification factors for the various schemes together with
the exact one for :math:`a<0` and use the plot to explain the observations
made in the experiments.

.. --- begin hint in exercise ---


*Hint.* Modify the `decay_ampf_plot.py <http://tinyurl.com/jvzzcfn/decay/decay_ampf_plot.py>`_ code.

.. --- end hint in exercise ---

Filename: ``growth_ampf.py``.

.. --- end exercise ---



.. !split

Model extensions
================

It is time to consider generalizations of the simple decay model
:math:`u=-au` and also to look at additional numerical solution methods.

Generalization: including a variable coefficient
------------------------------------------------

In the ODE for decay, :math:`u'=-au`, we now consider the case where :math:`a`
depends on time:


.. math::
   :label: decay:problem:a
        
        u'(t) = -a(t)u(t),\quad t\in (0,T],\quad u(0)=I \thinspace .
        
        


A Forward Euler scheme consist of evaluating :eq:`decay:problem:a`
at :math:`t=t_n` and approximating the derivative with a forward
difference :math:`[D^+_t u]^n`:


.. math::
        
        \frac{u^{n+1} - u^n}{\Delta t} = -a(t_n)u^n
        \thinspace .
        

The Backward Euler scheme becomes

.. math::
        
        \frac{u^{n} - u^{n-1}}{\Delta t} = -a(t_n)u^n
        \thinspace .
        

The Crank-Nicolson method builds on sampling the ODE at
:math:`t_{n+\frac{1}{2}}`. We can evaluate :math:`a` at :math:`t_{n+\frac{1}{2}}`
and use an average for :math:`u` at
times :math:`t_n` and :math:`t_{n+1}`:

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -a(t_{n+\frac{1}{2}})\frac{1}{2}(u^n + u^{n+1})
        \thinspace .
        

Alternatively, we can use an average for the product :math:`au`:


.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -\frac{1}{2}(a(t_n)u^n + a(t_{n+1})u^{n+1})
        \thinspace .
        

The :math:`\theta`-rule unifies the three mentioned schemes. One version is to
have :math:`a` evaluated at :math:`t_{n+\theta}`,


.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -a((1-\theta)t_n + \theta t_{n+1})((1-\theta) u^n + \theta u^{n+1})
        \thinspace .
        

Another possibility is to apply a weighted average for the product :math:`au`,

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -(1-\theta) a(t_n)u^n - \theta
        a(t_{n+1})u^{n+1}
        \thinspace .
        


With the finite difference operator notation the Forward Euler and Backward
Euler schemes can be summarized as


.. math::
        
        \lbrack D^+_t u = -au\rbrack^n,
        



.. math::
          
        \lbrack D^-_t u = -au\rbrack^n
        \thinspace .
        

The Crank-Nicolson and :math:`\theta` schemes depend on whether we evaluate
:math:`a` at the sample point for the ODE or if we use an average. The
various versions are written as


.. math::
        
        \lbrack D_t u = -a\overline{u}^t\rbrack^{n+\frac{1}{2}},
        



.. math::
          
        \lbrack D_t u = -\overline{au}^t\rbrack^{n+\frac{1}{2}},
        



.. math::
          
        \lbrack D_t u = -a\overline{u}^{t,\theta}\rbrack^{n+\theta},
        



.. math::
          
        \lbrack D_t u = -\overline{au}^{t,\theta}\rbrack^{n+\theta}
        \thinspace .
        



.. _decay:source:

Generalization: including a source term
---------------------------------------

A further extension of the model ODE is to include a source term :math:`b(t)`:


.. math::
   :label: decay:problem:ab
        
        u'(t) = -a(t)u(t) + b(t),\quad t\in (0,T],\quad u(0)=I
        \thinspace .
        
        


Schemes
~~~~~~~

The time point where we sample the ODE determines where :math:`b(t)` is
evaluated. For the Crank-Nicolson scheme and the :math:`\theta`-rule we
have a choice of whether to evaluate :math:`a(t)` and :math:`b(t)` at the
correct point or use an average. The chosen strategy becomes
particularly clear if we write up the schemes in the operator notation:


.. math::
        
        \lbrack D^+_t u = -au + b\rbrack^n,
        



.. math::
          
        \lbrack D^-_t u = -au + b\rbrack^n,
        



.. math::
          
        \lbrack D_t u   = -a\overline{u}^t + b\rbrack^{n+\frac{1}{2}},
        



.. math::
          
        \lbrack D_t u   = \overline{-au+b}^t\rbrack^{n+\frac{1}{2}},
        



.. math::
          
        \lbrack D_t u   = -a\overline{u}^{t,\theta} + b\rbrack^{n+\theta},
        



.. math::
   :label: decay:problem:ab:theta:avg:all:op
          
        \lbrack D_t u   = \overline{-au+b}^{t,\theta}\rbrack^{n+\theta}
        
        \thinspace .
        


.. _decay:general:

Implementation of the generalized model problem
-----------------------------------------------

Deriving the :math:`\theta`-rule formula
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Writing out the :math:`\theta`-rule in :eq:`decay:problem:ab:theta:avg:all:op`,
using :eq:`decay:fd1:Du:theta`
and :eq:`decay:fd1:wmean:a`, we get

.. math::
   :label: decay:problem:ab:theta:avg:all
        
        \frac{u^{n+1}-u^n}{\Delta t} = \theta(-a^{n+1}u^{n+1} + b^{n+1}))
        + (1-\theta)(-a^nu^{n} + b^n)),
        
        

where :math:`a^n` means evaluating :math:`a` at :math:`t=t_n` and similar for
:math:`a^{n+1}`, :math:`b^n`, and :math:`b^{n+1}`.
We solve for :math:`u^{n+1}`:

.. math::
        
        u^{n+1} = ((1 - \Delta t(1-\theta)a^n)u^n
        + \Delta t(\theta b^{n+1} + (1-\theta)b^n))(1 + \Delta t\theta a^{n+1})^{-1}
        \thinspace .
        


The Python code
~~~~~~~~~~~~~~~

Here is a suitable implementation of :eq:`decay:problem:ab:theta:avg:all`
where :math:`a(t)` and :math:`b(t)` are given as
Python functions:


.. code-block:: python

        def solver(I, a, b, T, dt, theta):
            """
            Solve u'=-a(t)*u + b(t), u(0)=I,
            for t in (0,T] with steps of dt.
            a and b are Python functions of t.
            """
            dt = float(dt)            # avoid integer division
            Nt = int(round(T/dt))     # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = ((1 - dt*(1-theta)*a(t[n]))*u[n] + \ 
                          dt*(theta*b(t[n+1]) + (1-theta)*b(t[n])))/\ 
                          (1 + dt*theta*a(t[n+1]))
            return u, t

This function is found in the file `decay_vc.py <http://tinyurl.com/jvzzcfn/decay/decay_vc.py>`_ (``vc`` stands for "variable coefficients").

Coding of variable coefficients
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``solver`` function shown above demands the arguments ``a`` and ``b`` to
be Python functions of time ``t``, say


.. code-block:: python

        def a(t):
            return a_0 if t < tp else k*a_0
        
        def b(t):
            return 1

Here, ``a(t)`` has three parameters ``a0``, ``tp``, and ``k``,
which must be global variables.
A better implementation is to represent ``a`` by a class where the
parameters are attributes and a *special method* ``__call__`` evaluates :math:`a(t)`:


.. code-block:: python

        class A:
            def __init__(self, a0=1, k=2):
                self.a0, self.k = a0, k
        
            def __call__(self, t):
                return self.a0 if t < self.tp else self.k*self.a0
        
        a = A(a0=2, k=1)  # a behaves as a function a(t)



.. index:: lambda functions


For quick tests it is cumbersome to write a complete function or a class.
The *lambda function* construction in Python is then convenient. For example,

.. code-block:: python

        a = lambda t: a_0 if t < tp else k*a_0

is equivalent to the ``def a(t):`` definition above. In general,

.. code-block:: python

        f = lambda arg1, arg2, ...: expressin

is equivalent to

.. code-block:: python

        def f(arg1, arg2, ...):
            return expression

One can use lambda functions directly in calls. Say we want to
solve :math:`u'=-u+1`, :math:`u(0)=2`:

.. code-block:: python

        u, t = solver(2, lambda t: 1, lambda t: 1, T, dt, theta)

A lambda function can appear anywhere where a variable can appear.

.. _decay:verify:trivial:

Verifying a constant solution
-----------------------------

A very useful partial verification method is to construct a test
problem with a very simple solution, usually :math:`u=\hbox{const}`.
Especially the initial debugging of a program code can benefit greatly
from such tests, because 1) all relevant numerical methods will
exactly reproduce a constant solution, 2) many of the intermediate
calculations are easy to control for a constant :math:`u`, and 3) even a
constant :math:`u` can uncover many bugs in an implementation.

The only constant solution for the problem :math:`u'=-au` is :math:`u=0`, but too
many bugs can escape from that trivial solution.  It is much better to
search for a problem where :math:`u=C=\hbox{const}\neq 0`.  Then :math:`u'=-a(t)u
+ b(t)` is more appropriate: with :math:`u=C` we can choose any :math:`a(t)` and
set :math:`b=a(t)C` and :math:`I=C`. An appropriate nose test is


.. code-block:: python

        import nose.tools as nt
        
        def test_constant_solution():
            """
            Test problem where u=u_const is the exact solution, to be
            reproduced (to machine precision) by any relevant method.
            """
            def exact_solution(t):
                return u_const
        
            def a(t):
                return 2.5*(1+t**3)  # can be arbitrary
        
            def b(t):
                return a(t)*u_const
        
            u_const = 2.15
            theta = 0.4; I = u_const; dt = 4
            Nt = 4  # enough with a few steps
            u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
            print u
            u_e = exact_solution(t)
            difference = abs(u_e - u).max()  # max deviation
            nt.assert_almost_equal(difference, 0, places=14)


An interesting question is what type of bugs that will make the
computed :math:`u^n` deviate from the exact solution :math:`C`.
Fortunately, the updating formula and the initial condition must
be absolutely correct for the test to pass! Any attempt to make
a wrong indexing in terms like ``a(t[n])`` or any attempt to
introduce an erroneous factor in the formula creates a solution
that is different from :math:`C`.


.. _decay:MMS:

Verification via manufactured solutions
---------------------------------------


.. index:: method of manufactured solutions


.. index:: MMS (method of manufactured solutions)


Following the idea of the previous section, we can choose any formula
as the exact solution, insert the formula in the ODE problem and fit
the data :math:`a(t)`, :math:`b(t)`, and :math:`I` to make the chosen
formula fulfill the equation. This
powerful technique for generating exact solutions is very useful for
verification purposes and known as the *method of manufactured
solutions*, often abbreviated MMS.

One common choice of solution is a linear function in the independent
variable(s). The rationale behind such a simple variation is that
almost any relevant numerical solution method for differential
equation problems is able to reproduce the linear function exactly to
machine precision (if :math:`u` is about unity in size; precision is lost if
:math:`u` take on large values, see :ref:`decay:fd2:exer:precision`).
The linear solution also makes some stronger demands to the
numerical method and the implementation than the constant solution
used in the section :ref:`decay:verify:trivial`, at least in more
complicated applications. However, the constant solution is often
ideal for initial debugging before proceeding with a linear solution.

We choose a linear solution :math:`u(t) = ct + d`. From the initial condition it
follows that :math:`d=I`.
Inserting this :math:`u` in the ODE results in

.. math::
         c = -a(t)u + b(t) \thinspace . 

Any function :math:`u=ct+I` is then a correct solution if we choose

.. math::
         b(t) = c + a(t)(ct + I) \thinspace . 

With this :math:`b(t)` there are no restrictions on :math:`a(t)` and :math:`c`.

Let prove that such a linear solution obeys the numerical
schemes. To this end, we must check that :math:`u^n = ca(t_n)(ct_n+I)`
fulfills the discrete equations. For these calculations, and
later calculations involving linear solutions inserted in
finite difference schemes, it is convenient to
compute the action of a difference operator on a linear function :math:`t`:


.. math::
   :label: decay:fd2:Dop:tn:fw
        
        \lbrack D_t^+ t\rbrack^n = \frac{t_{n+1}-t_n}{\Delta t}=1,
        
        



.. math::
   :label: decay:fd2:Dop:tn:bw
          
        \lbrack D_t^- t\rbrack^n = \frac{t_{n}-t_{n-1}}{\Delta t}=1,
        
        



.. math::
   :label: decay:fd2:Dop:tn:cn
          
        \lbrack D_t t\rbrack^n = \frac{t_{n+\frac{1}{2}}-t_{n-\frac{1}{2}}}{\Delta t}=\frac{(n+\frac{1}{2})\Delta t - (n-\frac{1}{2})\Delta t}{\Delta t}=1
        
        \thinspace .
        

Clearly, all three finite difference approximations to the derivative are
exact for :math:`u(t)=t` or its mesh function counterpart :math:`u^n = t_n`.

The difference equation for the Forward Euler scheme


.. math::
         [D^+_t u = -au + b]^n, 

with :math:`a^n=a(t_n)`, :math:`b^n=c + a(t_n)(ct_n + I)`, and :math:`u^n=ct_n + I`
then results in


.. math::
         c = -a(t_n)(ct_n+I) + c + a(t_n)(ct_n + I) = c 

which is always fulfilled. Similar calculations can be done for the
Backward Euler and Crank-Nicolson schemes, or the :math:`\theta`-rule for
that matter. In all cases, :math:`u^n=ct_n +I` is an exact solution of
the discrete equations. That is why we should expect that
:math:`u^n - u_{\small\mbox{e}}(t_n) =0` mathematically and :math:`|u^n - u_{\small\mbox{e}}(t_n)|` less
than a small number about the machine precision for :math:`n=0,\ldots,N_t`.

The following function offers an implementation of this verification
test based on a linear exact solution:


.. code-block:: python

        def test_linear_solution():
            """
            Test problem where u=c*t+I is the exact solution, to be
            reproduced (to machine precision) by any relevant method.
            """
            def exact_solution(t):
                return c*t + I
        
            def a(t):
                return t**0.5  # can be arbitrary
        
            def b(t):
                return c + a(t)*exact_solution(t)
        
            theta = 0.4; I = 0.1; dt = 0.1; c = -0.5
            T = 4
            Nt = int(T/dt)  # no of steps
            u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
            u_e = exact_solution(t)
            difference = abs(u_e - u).max()  # max deviation
            print difference
            # No of decimal places for comparison depend on size of c
            nt.assert_almost_equal(difference, 0, places=14)

Any error in the updating formula makes this test fail!

Choosing more complicated formulas as the exact solution, say
:math:`\cos(t)`, will not make the numerical and exact solution
coincide to machine precision, because finite differencing of
:math:`\cos(t)` does not exactly yield the exact derivative :math:`-\sin(t)`.
In such cases, the verification procedure
must be based on measuring the convergence rates as exemplified in
the section :ref:`decay:convergence:rate`. Convergence rates can be
computed as long as one has
an exact solution of a problem that the solver can be tested on, but
this can always be obtained by the method of manufactured solutions.


Extension to systems of ODEs
----------------------------

Many ODE models involves more than one unknown function and more
than one equation. Here is an example of two unknown functions :math:`u(t)`
and :math:`v(t)`:


.. math::
        
        u' = a u + bv,
        



.. math::
          
        v' = cu +  dv,
        

for constants :math:`a,b,c,d`.
Applying the Forward Euler method to each equation results in simple
updating formula


.. math::
        
        u^{n+1} = u^n + \Delta t (a u^n + b v^n),
        



.. math::
          
        v^{n+1} = u^n + \Delta t (cu^n + dv^n)
        \thinspace .
        

On the other hand, the Crank-Nicolson or Backward Euler schemes result in a
:math:`2\times 2` linear system for the new unknowns. The latter schemes gives


.. math::
        
        u^{n+1} = u^n + \Delta t (a u^{n+1} + b v^{n+1}),
        



.. math::
          
        v^{n+1} = v^n + \Delta t (c u^{n+1} + d v^{n+1}){\thinspace .}
        

Collecting :math:`u^{n+1}` as well as :math:`v^{n+1}` on the left-hand side results
in

.. math::
        
        (1 - \Delta t a)u^{n+1} + bv^{n+1} = u^n ,
        



.. math::
          
        c u^{n+1} + (1 - \Delta t d) v^{n+1} = v^n ,
        

which is a system of two coupled, linear, algebraic equations in two
unknowns.


General first-order ODEs
========================

We now turn the attention to general, nonlinear ODEs and systems of
such ODEs.  Our focus is on numerical methods that can be readily
reused for time-discretization PDEs, and diffusion PDEs in particular.
The methods are just briefly listed, and we refer to the rich literature
for more detailed descriptions and analysis - the books
[Ref2]_ [Ref3]_ [Ref4]_ [Ref5]_ are all excellent resources on numerical methods for ODEs.
We also demonstrate the Odespy Python interface to a range
of different software for general first-order ODE systems.

Generic form
------------

ODEs are commonly written in the generic form


.. math::
   :label: decay:ode:general
        
        u' = f(u,t),\quad u(0)=I,
        
        

where :math:`f(u,t)`  is some prescribed function.
As an example, our most
general exponential decay model :eq:`decay:problem:ab` has
:math:`f(u,t)=-a(t)u(t) + b(t)`.

The unknown :math:`u` in :eq:`decay:ode:general` may either be
a scalar function of time :math:`t`, or a vector valued function of :math:`t` in
case of a *system of ODEs* with :math:`m` unknown components:

.. math::
         u(t) = (u^{(0)}(t),u^{(1)}(t),\ldots,u^{(m-1)}(t)) \thinspace . 

In that case, the right-hand side is vector-valued function with :math:`m`
components,

.. math::
        
        f(u, t) = ( & f^{(0)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
                    & f^{(1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
                    & \vdots,\\ 
                    & f^{(m-1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)))
        \thinspace .
        


Actually, any system of ODEs can
be written in the form :eq:`decay:ode:general`, but higher-order
ODEs then need auxiliary unknown functions to enable conversion to
a first-order system.


Next we list some well-known methods for :math:`u'=f(u,t)`, valid both for
a single ODE (scalar :math:`u`) and systems of ODEs (vector :math:`u`).
The choice of methods is inspired by the kind of schemes that are
popular also for partial differential equations.


The :math:`\theta`-rule
-----------------------

The :math:`\theta`-rule scheme applied to :math:`u'=f(u,t)` becomes


.. math::
   :label: decay:fd2:theta
        
        \frac{u^{n+1}-u^n}{\Delta t} = \theta f(u^{n+1},t_{n+1}) +
        (1-\theta)f(u^n, t_n){\thinspace .}
        
        

Bringing the unknown :math:`u^{n+1}` to the left-hand side and the known terms
on the right-hand side gives


.. index:: implicit schemes

.. index:: explicit schemes

.. index:: theta-rule

.. index:: theta-rule



.. math::
        
        u^{n+1} - \Delta t \theta f(u^{n+1},t_{n+1}) =
        u^n + \Delta t(1-\theta)f(u^n, t_n){\thinspace .}
        

For a general :math:`f` (not linear in :math:`u`), this equation is *nonlinear* in
the unknown :math:`u^{+1}` unless :math:`\theta = 0`. For a scalar ODE (:math:`m=1`),
we have to solve a single nonlinear algebraic equation for :math:`u^{n+1}`,
while for a system of ODEs, we get a system of coupled, nonlinear
algebraic equations. Newton's method is a popular solution approach
in both cases. Note that with the Forward Euler scheme (:math:`\theta =0`)
we do not have to deal with nonlinear equations, because in that
case we have an explicit updating formula for :math:`u^{n+1}`. This is known
as an *explicit* scheme. With :math:`\theta\neq 1` we have to solve
systems of algebraic equations, and the scheme is said to be *implicit*.

An implicit 2-step backward scheme
----------------------------------


.. index::
   single: backward scheme, 2-step

.. index:: BDF2 scheme


The implicit backward method with 2 steps applies a
three-level backward difference as approximation to :math:`u'(t)`,

.. math::
         u'(t_{n+1}) \approx \frac{3u^{n+1} - 4u^{n} + u^{n-1}}{2\Delta t},

which is an approximation of order :math:`\Delta t^2` to the first derivative.
The resulting scheme for :math:`u'=f(u,t)` reads

.. math::
   :label: decay:fd2:bw:2step
        
        u^{n+1} = \frac{4}{3}u^n - \frac{1}{3}u^{n-1} +
        \frac{2}{3}\Delta t f(u^{n+1}, t_{n+1})
        \thinspace .
        
        

Higher-order versions of the scheme :eq:`decay:fd2:bw:2step` can
be constructed by including more time levels. These schemes are known
as the Backward Differentiation Formulas (BDF), and the particular
version :eq:`decay:fd2:bw:2step` is often referred to as BDF2.

Note that the scheme :eq:`decay:fd2:bw:2step` is implicit and requires
solution of nonlinear equations when :math:`f` is nonlinear in :math:`u`.  The
standard 1st-order Backward Euler method or the Crank-Nicolson scheme
can be used for the first step.


Leapfrog schemes
----------------


.. index:: Leapfrog scheme


The ordinary Leapfrog scheme
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The derivative of :math:`u` at some point :math:`t_n` can be approximated by
a central difference over two time steps,


.. math::
        
        u'(t_n)\approx \frac{u^{n+1}-u^{n-1}}{2\Delta t} = [D_{2t}u]^n
        

which is an approximation of second order in :math:`\Delta t`. The scheme
can then be written as


.. math::
         [D_{2t}u=f(u,t)]^n, 

in operator notation. Solving for :math:`u^{n+1}` gives


.. math::
   :label: decay:fd2:leapfrog
        
        u^{n+1} = u^{n-1} + \Delta t f(u^n, t_n)
        \thinspace .
        
        

Observe that :eq:`decay:fd2:leapfrog` is an explicit scheme, and that
a nonlinear :math:`f` (in :math:`u`) is trivial to handle since it only involves
the known :math:`u^n` value.
Some other scheme must be used as starter to compute :math:`u^1`, preferably
the Forward Euler scheme since it is also explicit.



.. index::
   single: Leapfrog scheme, filtered


The filtered Leapfrog scheme
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Unfortunately, the Leapfrog scheme :eq:`decay:fd2:leapfrog`
will develop growing oscillations with time (see :ref:`decay:fd2:exer:leapfrog1`). A remedy for such undesired oscillations
is to introduce a *filtering technique*. First, a standard Leapfrog
step is taken, according to :eq:`decay:fd2:leapfrog`, and then
the previous :math:`u^n` value is adjusted according to

.. math::
   :label: decay:fd2:leapfrog:filtered
        
        u^n\ \leftarrow\ u^n + \gamma (u^{n-1} - 2u^n + u^{n+1})
        
        \thinspace .
        

The :math:`\gamma`-terms will effectively damp oscillations in the solution,
especially those with short wavelength (like point-to-point oscillations).
A common choice of :math:`\gamma` is 0.6 (a value used in the
famous NCAR Climate Model).

.. Need to elaborate more on this:

.. The difference in th :math:`\gamma` term in :eq:`decay:fd2:leapfrog:filtered`

.. can be recognized as a finite difference approximation to

.. :math:`\Delta t^2 u''(t_n)`.



The 2nd-order Runge-Kutta scheme
--------------------------------


.. index:: Heun's method


.. index::
   single: Runge-Kutta, 2nd-order scheme


The two-step scheme


.. math::
   :label: decay:fd2:RK2:s1
        
        u^* = u^n + \Delta t f(u^n, t_n),
        
        



.. math::
   :label: decay:fd2:RK2:s2
          
        u^{n+1} = u^n + \Delta t \frac{1}{2} \left( f(u^n, t_n) + f(u^*, t_{n+1})
        \right),
        
        

essentially applies a Crank-Nicolson method :eq:`decay:fd2:RK2:s2`
to the ODE, but replaces
the term :math:`f(u^{n+1}, t_{n+1})` by a prediction
:math:`f(u^{*}, t_{n+1})` based on a Forward Euler step :eq:`decay:fd2:RK2:s1`.
The scheme :eq:`decay:fd2:RK2:s1`-:eq:`decay:fd2:RK2:s2` is
known as Huen's method, but is also a 2nd-order Runge-Kutta method.
The scheme is explicit, and the error is expected to behave as :math:`\Delta t^2`.


A 2nd-order Taylor-series method
--------------------------------


.. index:: Taylor-series methods (for ODEs)


One way to compute :math:`u^{n+1}` given :math:`u^n` is to use a Taylor polynomial.
We may write up a polynomial of 2nd degree:

.. math::
        
        u^{n+1} = u^n + u'(t_n)\Delta t + \frac{1}{2}u''(t_n)\Delta t^2
        \thinspace .
        

From the equation :math:`u'=f(u,t)` it follows that the derivatives of :math:`u`
can be expressed in terms of :math:`f` and its derivatives:

.. math::
        
        u'(t_n) &=f(u^n,t_n),\\ 
        u''(t_n) &=
        \frac{\partial f}{\partial u}(u^n,t_n) u'(t_n) + \frac{\partial f}{\partial t}\\ 
        &=  f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
        \frac{\partial f}{\partial t},
        

resulting in the scheme

.. math::
   :label: decay:fd2:Taylor2
        
        u^{n+1} = u^n + f(u^n,t_n)\Delta t + \frac{1}{2}\left(
        f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
        \frac{\partial f}{\partial t}\right)\Delta t^2
        \thinspace .
        
        

More terms in the series could be included in the Taylor polynomial to
obtain methods of higher order than 2.



The 2nd- and 3rd-order Adams-Bashforth schemes
----------------------------------------------


.. index::
   single: Adams-Bashforth scheme, 2nd-order


The following method is known as the 2nd-order Adams-Bashforth scheme:


.. math::
   :label: decay:fd2:AB2
        
        u^{n+1} = u^n + \frac{1}{2}\Delta t\left( 3f(u^n, t_n) - f(u^{n-1}, t_{n-1})
        \right)
        \thinspace .
        
        

The scheme is explicit and requires another one-step scheme to compute
:math:`u^1` (the Forward Euler scheme or Heun's method, for instance).
As the name implies, the scheme is of order :math:`\Delta t^2`.



.. index::
   single: Adams-Bashforth scheme, 3rd order


Another explicit scheme, involving four time levels, is the
3rd-order Adams-Bashforth scheme


.. math::
   :label: decay:fd2:AB3
        
        u^{n+1} = u^n + \frac{1}{12}\left( 23f(u^n, t_n) - 16 f(u^{n-1},t_{n-1})
        + 5f(u^{n-2}, t_{n-2})\right)
        \thinspace .
        
        

The numerical error is of order :math:`\Delta t^3`, and the scheme needs
some method for computing :math:`u^1` and :math:`u^2`.

More general, higher-order Adams-Bashforth schemes (also called
*explicit Adams methods*) compute :math:`u^{n+1}` as a linear combination
of :math:`f` at :math:`k` previous time steps:


.. math::
         u^{n+1} = u^n + \sum_{j=0}^k \beta_jf(u^{n-j},t_{n-j}),

where :math:`\beta_j` are known coefficients.


.. _decay:fd2:RK4:

4th-order Runge-Kutta scheme
----------------------------


.. index::
   single: Runge-Kutta, 4th-order scheme


.. index:: RK4


The perhaps most widely used method to solve ODEs is the 4th-order
Runge-Kutta method, often called RK4.
Its derivation is a nice illustration of common
numerical approximation strategies, so let us go through the
steps in detail.

The starting point is to integrate the ODE
:math:`u'=f(u,t)` from :math:`t_n` to :math:`t_{n+1}`:


.. math::
         u(t_{n+1}) - u(t_n) = \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt{\thinspace .} 

We want to compute :math:`u(t_{n+1})` and regard :math:`u(t_n)` as known.
The task is to find good approximations for the integral, since the
integrand involves the unknown :math:`u` between :math:`t_n` and :math:`t_{n+1}`.

The integral can be approximated by the famous
`Simpson's rule <http://en.wikipedia.org/wiki/Simpson's_rule>`_:


.. math::
         \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
        \approx \frac{\Delta t}{6}\left( f^n + 4f^{n+1/2} + f^{n+1}\right){\thinspace .}

The problem now is that we do not know :math:`f^{n+1/2}=f(u^{n+1/2},t_{n+1/2})`
and :math:`f^{n+1}=(u^{n+1},t_{n+1})` as we know only :math:`u^n` and hence :math:`f^n`.
The idea is to use various approximations for :math:`f^{n+1/2}` and
:math:`f^{n+1}` based on using well-known schemes for the ODE in the
intervals :math:`[t_n,t_{n+1/2}]` and :math:`[t_n, t_{n+1}]`.
We split the integral approximation into four terms:


.. math::
         \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
        \approx \frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+1/2}
        + 2\tilde{f}^{n+1/2} + \bar{f}^{n+1}\right),

where :math:`\hat{f}^{n+1/2}`, :math:`\tilde{f}^{n+1/2}`, and :math:`\bar{f}^{n+1}`
are approximations to :math:`f^{n+1/2}` and
:math:`f^{n+1}` that can be based on already computed quantities.
For :math:`\hat{f}^{n+1/2}` we can apply
an approximation to :math:`u^{n+1/2}` using the Forward Euler
method with step :math:`\frac{1}{2}\Delta t`:


.. math::
   :label: decay:fd2:RK4:hatf
        
        \hat{f}^{n+1/2} = f(u^n + \frac{1}{2}{\Delta t} f^n, t_{n+1/2})
        
        

Since this gives us a prediction of :math:`f^{n+1/2}`, we can for
:math:`\tilde{f}^{n+1/2}` try a Backward Euler method to approximate :math:`u^{n+1/2}`:


.. math::
   :label: decay:fd2:RK4:tildef
        
        \tilde{f}^{n+1/2} = f(u^n + \frac{1}{2}\Delta t\hat{f}^{n+1/2}, t_{n+1/2}){\thinspace .}
        
        

With :math:`\tilde{f}^{n+1/2}` as a hopefully good approximation to
:math:`f^{n+1/2}`, we can for the final term :math:`\bar{f}^{n+1}` use
a Crank-Nicolson method to approximate :math:`u^{n+1}`:


.. math::
   :label: decay:fd2:RK4:barf
        
        \bar{f}^{n+1} = f(u^n + \Delta t \hat{f}^{n+1/2}, t_{n+1}){\thinspace .}
        
        

We have now used the Forward and Backward Euler methods as well as the
Crank-Nicolson method in the context of Simpson's rule. The hope is
that the combination of these methods yields an overall time-stepping
scheme from :math:`t_n` to :math:`t_n{+1}` that is much more accurate than the
:math:`{{\cal O}(\Delta t)}` and :math:`{{\cal O}(\Delta t^2)}` of the individual steps.
This is indeed true: the overall accuracy is :math:`{{\cal O}(\Delta t^4)}`!

To summarize, the 4th-order Runge-Kutta method becomes


.. math::
        
        u^{n+1} = u^n +
        \frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+1/2}
        + 2\tilde{f}^{n+1/2} + \bar{f}^{n+1}\right),
        

where the quantities on the right-hand side are computed from
:eq:`decay:fd2:RK4:hatf`-:eq:`decay:fd2:RK4:barf`. Note that
the scheme is fully explicit so there is never any need to solve linear or
nonlinear algebraic
equations. However, the stability is conditional and depends on :math:`f`.
There is a whole range of *implicit* Runge-Kutta methods that
are unconditionally stable, but require solution of algebraic
equations involving :math:`f` at each time step.

The simplest way to explore more sophisticated methods for ODEs is to
apply one of the many high-quality software packages that exist, as the
next section explains.

The Odespy software
-------------------

A wide range of the methods and software exist for solving :eq:`decay:ode:general`.
Many of methods are accessible through a unified Python interface offered
by the `Odespy <https://github.com/hplgit/odespy>`_ package.
Odespy features simple Python implementations of the most fundamental
schemes as well as Python interfaces to several famous packages for
solving ODEs: `ODEPACK <https://computation.llnl.gov/casc/odepack/odepack_home.html>`_, `Vode <https://computation.llnl.gov/casc/odepack/odepack_home.html>`_,
`rkc.f <http://www.netlib.org/ode/rkc.f>`_, `rkf45.f <http://www.netlib.org/ode/rkf45.f>`_, `Radau5 <http://www.unige.ch/~hairer/software.html>`_, as well
as the ODE solvers in `SciPy <http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html>`_, `SymPy <http://docs.sympy.org/dev/modules/mpmath/calculus/odes.html>`_, and `odelab <http://olivierverdier.github.com/odelab/>`_.

The usage of Odespy follows this setup for the ODE :math:`u'=-au`,
:math:`u(0)=I`, :math:`t\in (0,T]`, here solved
by the famous 4th-order Runge-Kutta method, using :math:`\Delta t=1`
and :math:`N_t=6` steps:


.. code-block:: text


        def f(u, t):
            return -a*u
        
        import odespy
        import numpy as np
        
        I = 1; a = 0.5; Nt = 6; dt = 1
        solver = odespy.RK4(f)
        solver.set_initial_condition(I)
        t_mesh = np.linspace(0, Nt*dt, Nt+1)
        u, t = solver.solve(t_mesh)


The previously listed methods for ODEs are all accessible in
Odespy:

 * the :math:`\theta`-rule: ``ThetaRule``

 * special cases of the :math:`\theta`-rule: ``ForwardEuler``, ``BackwardEuler``,
   ``CrankNicolson``

 * the 2nd- and 4th-order Runge-Kutta methods: ``RK2`` and ``RK4``

 * The BDF methods and the Adam-Bashforth methods:
   ``Vode``, ``Lsode``, ``Lsoda``, ``lsoda_scipy``

 * The Leapfrog scheme: ``Leapfrog`` and ``LeapfrogFiltered``

Example: Runge-Kutta methods
----------------------------

Since all solvers have the same interface in Odespy, modulo different set of
parameters to the solvers' constructors, one can easily make a list of
solver objects and run a loop for comparing (a lot of) solvers. The
code below, found in complete form in `decay_odespy.py <http://tinyurl.com/jvzzcfn/decay/decay_odespy.py>`_,
compares the famous Runge-Kutta methods of orders 2, 3, and 4
with the exact solution of the decay equation
:math:`u'=-au`.
Since we have quite long time steps, we have included the only
relevant :math:`\theta`-rule for large time steps, the Backward Euler scheme
(:math:`\theta=1`), as well.
Figure :ref:`decay:odespy:fig1` shows the results.


.. code-block:: python

        import numpy as np
        import scitools.std as plt
        import sys
        
        def f(u, t):
            return -a*u
        
        I = 1; a = 2; T = 6
        dt = float(sys.argv[1]) if len(sys.argv) >= 2 else 0.75
        Nt = int(round(T/dt))
        t = np.linspace(0, Nt*dt, Nt+1)
        
        solvers = [odespy.RK2(f),
                   odespy.RK3(f),
                   odespy.RK4(f),
                   odespy.BackwardEuler(f, nonlinear_solver='Newton')]
        
        legends = []
        for solver in solvers:
            solver.set_initial_condition(I)
            u, t = solver.solve(t)
        
            plt.plot(t, u)
            plt.hold('on')
            legends.append(solver.__class__.__name__)
        
        # Compare with exact solution plotted on a very fine mesh
        t_fine = np.linspace(0, T, 10001)
        u_e = I*np.exp(-a*t_fine)
        plt.plot(t_fine, u_e, '-') # avoid markers by specifying line type
        legends.append('exact')
        
        plt.legend(legends)
        plt.title('Time step: %g' % dt)
        plt.show()




.. admonition:: Visualization tip

   We use SciTools for
   plotting here, but importing ``matplotlib.pyplot`` as ``plt`` instead
   also works. However, plain use of Matplotlib as done here results in
   curves with different colors, which may be hard to distinguish on
   black-and-white paper. Using SciTools, curves are
   automatically given colors *and* markers, thus making curves easy
   to distinguish on screen with colors and on black-and-white paper.
   The automatic adding of markers is normally a bad idea for a
   very mesh since all the markers get cluttered, but SciTools limits
   the number of markers in such cases.
   For the exact solution we use a very fine mesh, but in the code
   above we specify the line type as a solid line (``-``), which means
   no markers and just a color to be automatically determined by
   the backend used for plotting (Matplotlib by default, but
   SciTools gives the opportunity to use other backends
   to produce the plot, e.g., Gnuplot or Grace).
   
   Also note the that the legends
   are based on the class names of the solvers, and in Python the name of
   a the class type (as a string) of an object ``obj`` is obtained by
   ``obj.__class__.__name__``.


.. _decay:odespy:fig1:

.. figure:: decay_odespy1_png.png
   :width: 600

   *Behavior of different schemes for the decay equation*



The runs in Figure :ref:`decay:odespy:fig1`
and other experiments reveal that the 2nd-order Runge-Kutta
method (``RK2``) is unstable for :math:`\Delta t>1` and decays slower than the
Backward Euler scheme for large and moderate :math:`\Delta t` (see :ref:`decay:exer:RK2:Taylor:analysis` for an analysis).  However, for
fine :math:`\Delta t = 0.25` the 2nd-order Runge-Kutta method approaches
the exact solution faster than the Backward Euler scheme.  That is,
the latter scheme does a better job for larger :math:`\Delta t`, while the
higher order scheme is superior for smaller :math:`\Delta t`. This is a
typical trend also for most schemes for ordinary and partial
differential equations.

The 3rd-order Runge-Kutta method (``RK3``) has also artifacts in form
of oscillatory behavior for the larger :math:`\Delta t` values, much
like that of the Crank-Nicolson scheme. For finer :math:`\Delta t`,
the 3rd-order Runge-Kutta method converges quickly to the exact
solution.

The 4th-order Runge-Kutta method (``RK4``) is slightly inferior
to the Backward Euler scheme on the coarsest mesh, but is then
clearly superior to all the other schemes. It is definitely the
method of choice for all the tested schemes.


Remark about using the :math:`\theta`-rule in Odespy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Odespy package assumes that the ODE is written as :math:`u'=f(u,t)` with
an :math:`f` that is possibly nonlinear in :math:`u`. The :math:`\theta`-rule for
:math:`u'=f(u,t)` leads to

.. math::
         u^{n+1} = u^{n} + \Delta t\left(\theta f(u^{n+1}, t_{n+1})
        + (1-\theta) f(u^{n}, t_{n})\right),

which is a *nonlinear equation* in :math:`u^{n+1}`. Odespy's implementation
of the :math:`\theta`-rule (``ThetaRule``) and the specialized Backward Euler
(``BackwardEuler``) and Crank-Nicolson (``CrankNicolson``) schemes
must invoke iterative methods for
solving the nonlinear equation in :math:`u^{n+1}`. This is done even when
:math:`f` is linear in :math:`u`, as in the model problem :math:`u'=-au`, where we can
easily solve for :math:`u^{n+1}` by hand.  Therefore, we need to specify
use of Newton's method to the equations.
(Odespy allows other methods than Newton's to be used, for instance
Picard iteration, but that method is not suitable. The reason is that it
applies the Forward Euler scheme to generate a start value for
the iterations. Forward Euler may give very wrong solutions
for large :math:`\Delta t` values. Newton's method, on the other hand,
is insensitive to the start value in *linear problems*.)


.. _decay:fd2:adaptiveRK:

Example: Adaptive Runge-Kutta methods
-------------------------------------


.. index:: adaptive time stepping


Odespy offers solution methods that can adapt the size of :math:`\Delta t`
with time to match a desired accuracy in the solution. Intuitively,
small time steps will be chosen in areas where the solution is changing
rapidly, while larger time steps can be used where the solution
is slowly varying. Some kind of *error estimator* is used to
adjust the next time step at each time level.


.. index:: ode45

.. index:: Dormand-Prince Runge-Kutta 4-5 method


A very popular adaptive method for solving ODEs is the Dormand-Prince
Runge-Kutta method of order 4 and 5. The 5th-order method is used as a
reference solution and the difference between the 4th- and 5th-order
methods is used as an indicator of the error in the numerical
solution.  The Dormand-Prince method is the default choice in MATLAB's
widely used ``ode45`` routine.

We can easily set up Odespy to use the Dormand-Prince method and
see how it selects the optimal time steps. To this end, we request
only one time step from :math:`t=0` to :math:`t=T` and ask the method to
compute the necessary non-uniform time mesh to meet a certain
error tolerance. The code goes like


.. code-block:: python

        import odespy
        import numpy as np
        import decay_mod
        import sys
        #import matplotlib.pyplot as plt
        import scitools.std as plt
        
        def f(u, t):
            return -a*u
        
        def exact_solution(t):
            return I*np.exp(-a*t)
        
        I = 1; a = 2; T = 5
        tol = float(sys.argv[1])
        solver = odespy.DormandPrince(f, atol=tol, rtol=0.1*tol)
        
        Nt = 1  # just one step - let the scheme find its intermediate points
        t_mesh = np.linspace(0, T, Nt+1)
        t_fine = np.linspace(0, T, 10001)
        
        solver.set_initial_condition(I)
        u, t = solver.solve(t_mesh)
        
        # u and t will only consist of [I, u^Nt] and [0,T]
        # solver.u_all and solver.t_all contains all computed points
        plt.plot(solver.t_all, solver.u_all, 'ko')
        plt.hold('on')
        plt.plot(t_fine, exact_solution(t_fine), 'b-')
        plt.legend(['tol=%.0E' % tol, 'exact'])
        plt.savefig('tmp_odespy_adaptive.png')
        plt.show()


Running four cases with tolerances :math:`10^{-1}`, :math:`10^{-3}`, :math:`10^{-5}`,
and :math:`10^{-7}`, gives the results in Figure :ref:`decay:odespy:fig2`.
Intuitively, one would expect denser points in the beginning of
the decay and larger time steps when the solution flattens out.


.. _decay:odespy:fig2:

.. figure:: decay_DormandPrince_adaptivity.png
   :width: 800

   *Choice of adaptive time mesh by the Dormand-Prince method for different tolerances*




Exercises  (3)
==============



.. --- begin exercise ---


.. _decay:fd2:exer:precision:

Exercise 14: Experiment with precision in tests and the size of :math:`u`
-------------------------------------------------------------------------

It is claimed in the section :ref:`decay:MMS` that most numerical methods will
reproduce a linear exact solution to machine precision. Test this
assertion using the nose test function ``test_linear_solution`` in the
`decay_vc.py <http://tinyurl.com/jvzzcfn/decay/decay_vc.py>`_ program.
Vary the parameter ``c`` from very small, via ``c=1`` to many larger values,
and print out the maximum difference between the numerical solution
and the exact solution. What is the relevant value of the ``places``
(or ``delta``) argument to ``nose.tools.assert_almost_equal`` in each
case?
Filename: ``test_precision.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:fd2:exer:bw2:

Exercise 15: Implement the 2-step backward scheme
-------------------------------------------------

Implement the 2-step backward method :eq:`decay:fd2:bw:2step` for the
model :math:`u'(t) = -a(t)u(t) + b(t)`, :math:`u(0)=I`.  Allow the first step to
be computed by either the Backward Euler scheme or the Crank-Nicolson
scheme. Verify the implementation by choosing :math:`a(t)` and :math:`b(t)` such
that the exact solution is linear in :math:`t` (see the section :ref:`decay:MMS`). Show mathematically that a linear solution is indeed a
solution of the discrete equations.

Compute convergence rates (see the section :ref:`decay:convergence:rate`) in
a test case :math:`a=\hbox{const}` and :math:`b=0`, where we easily have an exact
solution, and determine if the choice of a first-order scheme
(Backward Euler) for the first step has any impact on the overall
accuracy of this scheme. The expected error goes like :math:`{{\cal O}(\Delta t^2)}`.
Filename: ``decay_backward2step.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:fd2:exer:AB2:

Exercise 16: Implement the 2nd-order Adams-Bashforth scheme
-----------------------------------------------------------

Implement the 2nd-order Adams-Bashforth method :eq:`decay:fd2:AB2`
for the decay problem :math:`u'=-a(t)u + b(t)`, :math:`u(0)=I`, :math:`t\in (0, T]`.
Use the Forward Euler method for the first step such that the overall
scheme is explicit. Verify the implementation using an exact
solution that is linear in time.
Analyze the scheme by searching for solutions :math:`u^n=A^n` when :math:`a=\hbox{const}`
and :math:`b=0`. Compare this second-order secheme to the Crank-Nicolson scheme.
Filename: ``decay_AdamsBashforth2.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:fd2:exer:AB3:

Exercise 17: Implement the 3rd-order Adams-Bashforth scheme
-----------------------------------------------------------

Implement the 3rd-order Adams-Bashforth method :eq:`decay:fd2:AB3`
for the decay problem :math:`u'=-a(t)u + b(t)`, :math:`u(0)=I`, :math:`t\in (0, T]`.
Since the scheme is explicit, allow it to be started by two steps with
the Forward Euler method.  Investigate experimentally the case where
:math:`b=0` and :math:`a` is a constant: Can we have oscillatory solutions for
large :math:`\Delta t`?
Filename: ``decay_AdamsBashforth3.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:exer:RK2:Taylor:analysis:

Exercise 18: Analyze explicit 2nd-order methods
-----------------------------------------------

Show that the schemes :eq:`decay:fd2:RK2:s2` and
:eq:`decay:fd2:Taylor2` are identical in the case :math:`f(u,t)=-a`, where
:math:`a>0` is a constant. Assume that the numerical solution reads
:math:`u^n=A^n` for some unknown amplification factor :math:`A` to be determined.
Find :math:`A` and derive stability criteria. Can the scheme produce
oscillatory solutions of :math:`u'=-au`? Plot the numerical and exact
amplification factor.
Filename: ``decay_RK2_Taylor2.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:fd2:exer:leapfrog1:

Problem 4: Implement and investigate the Leapfrog scheme
--------------------------------------------------------

A Leapfrog scheme
for the ODE :math:`u'(t) = -a(t)u(t) + b(t)` is defined by


.. math::
         \brack D_{2t}u = -au+b\rbrack^n{\thinspace .}

A separate scheme is needed to compute :math:`u^1`, and the Forward Euler
scheme is a candidate.




.. A possible test case is

.. :math:`u'=-au + b`, :math:`u(0)=0`, where :math:`u_{\small\mbox{e}}(t)=b/a + (I - b/a)e^{-at}` if

.. :math:`a` and :math:`b` are constants.



*a)* Implement the Leapfrog scheme for the model equation.

*b)* Show mathematically that a linear solution in :math:`t` fulfills the
Forward Euler scheme for the first step and the Leapfrog scheme
for the subsequent steps. Use this linear solution to verify
the implementation, and automate the verification through a nose test.

*c)* Create a manufactured solution :math:`u(t)=\sin(t)` for the ODE
:math:`u'=-au+b`.
Compute the convergence rate of the Leapfrog scheme using this
manufactured solution. The expected convergence rate of the
Leapfrog scheme is :math:`{{\cal O}(\Delta t^2)}`. Does the use of a
1st-order method for the first step impact the convergence rate?

*d)* Set up a set of experiments to demonstrate that the Leapfrog scheme
:eq:`decay:fd2:leapfrog` is associated with numerical artifacts
(instabilities). Document the main results from this investigation.

*e)* Analyze and explain the
instabilities of the Leapfrog scheme :eq:`decay:fd2:leapfrog`:

 * Choose :math:`a=\mbox{const}` and :math:`b=0`.

 * Assume that an exact solution of the discrete equations has
   the form :math:`u^n=A^n`, where :math:`A` is an amplification factor to
   be determined.

 * Compute :math:`A` either by and or with the aid of ``sympy``.
   The polynomial for :math:`A` has two roots, :math:`A_1` and :math:`A_2`. We let
   :math:`u^n` be a linear combination :math:`u^n=C_1A_1^n + C_2A_2^n`.

 * Show that one of the roots is the explanation of the instability.

 * Determine :math:`C_1` and :math:`C_2` and compare :math:`A` with the exact
   expression, using a Taylor series approximation.

*f)* Since the original Leapfrog scheme
is unconditionally unstable
as time grows, it demands some stabilization.
This can be done by filtering, where we first find :math:`u^{n+1}`
from the original Leapfrog scheme and then replace :math:`u^{n+1}`
by the average :math:`\gamma (u^{n-1} - 2u^n + u^{n+1}`, where :math:`\gamma`
can be taken as 0.6.
Implement the filtered Leapfrog
scheme and check that it can handle tests where the original
Leapfrog scheme is unstable.

Filenames: ``decay_leapfrog.py``, ``decay_leapfrog.pdf``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:fd2:exer:uni:

Problem 5: Make a unified implementation of many schemes
--------------------------------------------------------

Consider the linear ODE problem :math:`u'(t)=-a(t)u(t) + b(t)`, :math:`u(0)=I`.
Explicit schemes for this problem can be written in the general form

.. math::
   :label: decay:analysis:exer:sumcj
        
        u^{n+1} = \sum_{j=0}^m c_ju^{n-j},
        
        

for some choice of :math:`c_0,\ldots,c_m`.
Find expressions for the :math:`c_j` coefficients in case of the
:math:`\theta`-rule, the three-level backward scheme,
the Leapfrog scheme, the 2nd-order Runge-Kutta method,
and the 3rd-order Adams-Bashforth scheme.

Make a class ``ExpDecay`` that implements the
general updating formula :eq:`decay:analysis:exer:sumcj`.
The formula cannot be applied for :math:`n<m`, and for those :math:`n` values, other
schemes must be used. Assume for simplicity that we just
repeat Crank-Nicolson steps until :eq:`decay:analysis:exer:sumcj` can be used.
Use a subclass
to specify the list :math:`c_0,\ldots,c_m` for a particular method, and
implement subclasses for all the mentioned schemes.
Verify the implementation by testing with a linear solution, which should
be exactly reproduced by all methods.
Filename: ``decay_schemes_oo.py``.

.. --- end exercise ---



.. !split


.. _decay:app:

Applications of exponential decay models
========================================

This section presents many mathematical models that all
end up with ODEs of the type :math:`u'=-au+b`.
The applications are taken from biology,
finance, and physics, and cover population growth or decay, compound
interest and inflation, radioactive decay, cooling of objects,
compaction of geological media, pressure
variations in the atmosphere, and air resistance on falling or rising
bodies.

.. _decay:app:scaling:

Scaling  (1)
------------

Real applications of a model :math:`u'=-au+b` will often involve a lot
of parameters in the expressions for :math:`a` and :math:`b`. It can be quite
a challenge to find relevant values of all parameters. In simple
problems, however, it turns out that it is not always necessary
to estimate all parameters because we can lump them into one or
a few *dimensionless* numbers by using a very attractive technique
called scaling. It simply means to stretch the :math:`u` and :math:`t` axis
is the present problem - and suddenly all parameters in the problem
are lumped one parameter if :math:`b\neq 0` and no parameter when :math:`b=0`!

Scaling means that we introduce a new function :math:`\bar u(\bar t)`,
with


.. math::
         \bar u = \frac{u - u_m}{u_c},\quad \bar t = \frac{t}{t_c},

where :math:`u_m` is a characteristic value of :math:`u`, :math:`u_c` is a characteristic
size of the range of :math:`u` values, and :math:`t_c` is a characteristic
size of the range of :math:`t_c` where :math:`u` varies significantly.
Choosing :math:`u_m`, :math:`u_c`, and :math:`t_c` is not always easy and often an art
in complicated problems. We just state one choice first:


.. math::
         u_c = I,\quad u_m = b/a,\quad t_c = 1/a{\thinspace .}

Inserting :math:`u=u_m + u_c\bar u` and :math:`t=t_c\bar t` in the problem
:math:`u'=-au + b`, assuming :math:`a` and :math:`b` are constants, results after some
algebra in the *scaled problem*


.. math::
         \frac{d\bar u}{d\bar t} = -\bar u,\quad \bar u(0)=1 - \beta,

where :math:`\beta` is a dimensionless number


.. math::
         \beta = \frac{b}{Ia}{\thinspace .}

That is, only the special combination of :math:`b/(Ia)` matters, not what
the individual values of :math:`b`, :math:`a`, and :math:`I` are. Moreover, if :math:`b=0`,
the scaled problem is independent of :math:`a` and :math:`I`! In practice this means
that we can perform one numerical simulation of the scaled problem and
recover the solution of any problem for a given :math:`a` and :math:`I` by
stretching the axis in the plot: :math:`u=I\bar u` and :math:`t =\bar t/a`.
For :math:`b\neq 0`, we simulate the scaled problem for a few :math:`\beta` values
and recover the physical solution :math:`u` by translating and stretching the :math:`u`
axis and stretching the :math:`t` axis.

The scaling breaks down if :math:`I=0`. In that case we may choose :math:`u_m=0`,
:math:`u_c=b/a`, and :math:`t_c=1/b`, resulting in a slightly different scaled problem:


.. math::
         \frac{d\bar u}{d\bar t} = 1 -\bar u,\quad \bar u(0)=0{\thinspace .}

As with :math:`b=0`, the case :math:`I=0` has a scaled problem with no physical
parameters!

It is common to drop the bars after scaling and write the scaled
problem as :math:`u'=-u`, :math:`u(0)=1-\beta`, or :math:`u'=1-u`, :math:`u(0)=0`.
Any implementation of the problem :math:`u'=-au+b`, :math:`u(0)=I`, can be
reused for the scaled problem by setting :math:`a=1`, :math:`b=0`, and :math:`I=1-\beta`
in the code, if :math:`I\neq 0`, or one sets
:math:`a=1`, :math:`b=1`, and :math:`I=0` when the physical :math:`I` is zero.
Falling bodies in fluids, as described in the section :ref:`decay:app:drag`,
involves :math:`u'=-au+b` with seven physical parameters. All these vanish
in the scaled version of the problem if we start the motion from rest!



.. _decay:app:pop:

Evolution of a population
-------------------------


.. index:: population dynamics


Let :math:`N` be the number of individuals in a population occupying some
spatial domain.
Despite :math:`N` being an integer in this problem,
we shall compute with :math:`N` as a real number
and view :math:`N(t)` as a continuous function of time.
The basic model assumption is that in a time interval :math:`\Delta t` the number of
newcomers to the populations (newborns) is proportional to
:math:`N`, with proportionality constant :math:`\bar b`. The amount of
newcomers will increase the population and result in
to

.. math::
         N(t+\Delta t) = N(t) + \bar bN(t)\thinspace . 

It is obvious that a long time interval :math:`\Delta t` will result in
more newcomers and hence a larger :math:`\bar b`. Therefore, we introduce
:math:`b=\bar b/\Delta t`: the number of newcomers per unit time and per
individual. We must then multiply :math:`b` by the length of the time
interval considered and by the population size to get the
total number of new individuals, :math:`b\Delta t N`.

If the number of removals from the population (deaths) is also
proportional to :math:`N`, with proportionality constant :math:`d\Delta t`,
the population evolves according to

.. math::
         N(t+\Delta t) = N(t) + b\Delta t N(t) - d\Delta t N(t)\thinspace . 

Dividing by :math:`\Delta t` and letting :math:`\Delta t \rightarrow 0`,
we get the ODE


.. math::
        
        N' = (b-d)N,\quad N(0)=N_0\thinspace .
        

In a population where the death rate (:math:`d`) is larger than
then newborn rate (:math:`b`), :math:`a>0`, and the population experiences
exponential decay rather than exponential growth.

In some populations there is an immigration of individuals into the
spatial domain. With :math:`I` individuals coming in per time unit,
the equation for the population change becomes


.. math::
         N(t+\Delta t) = N(t) + b\Delta t N(t) - d\Delta t N(t) + \Delta t I\thinspace . 

The corresponding ODE reads

.. math::
        
        N' = (b-d)N + I,\quad N(0)=N_0
        \thinspace .
        


Some simplification arises if we introduce a fractional measure
of the population: :math:`u=N/N_0` and set :math:`r=b-d`. The ODE problem
now becomes

.. math::
   :label: decay:app:pop:ueq
        
        u' = ru + f,\quad u(0)=1,
        
        

where :math:`f=I/N_0` measures the net immigration per time unit as
the fraction of the initial population. Very often, :math:`r` is approximately
constant, but :math:`f` is usually a function of time.


.. index:: logistic model


The growth rate :math:`r` of a population decreases if the environment
has limited resources. Suppose the environment can sustain at
most :math:`N_{\max}` individuals. We may then assume that the growth rate
approaches zero as :math:`N` approaches :math:`N_{\max}`, i.e., as :math:`u` approaches
:math:`M=N_{\max}/N_0`. The simplest possible evolution of :math:`r` is then a
linear function: :math:`r(t)=r_0(1-u(t)/M)`, where :math:`r_0`
is the initial growth rate when the population is small relative to the
maximum size and there is enough resources. Using this :math:`r(t)` in
:eq:`decay:app:pop:ueq` results in the *logistic model* for the
evolution of a population (assuming for the moment that :math:`f=0`):

.. math::
   :label: decay:app:pop:logistic
        
        u' = r_0(1-u/M)u,\quad u(0)=1
        \thinspace .
        
        

Initially, :math:`u` will grow at rate :math:`r_0`, but the growth will decay
as :math:`u` approaches :math:`M`, and then there is no more change in :math:`u`, causing
:math:`u\rightarrow M` as :math:`t\rightarrow\infty`.
Note that the logistic equation :math:`u'=r_0(1-u/M)u` is *nonlinear* because
of the quadratic term :math:`-u^2r_0/M`.

.. _decay:app:interest:

Compound interest and inflation
-------------------------------

Say the annual interest rate is :math:`r` percent and that the bank
adds the interest once a year to your investment.
If :math:`u^n` is the investment in year :math:`n`, the investment in year :math:`u^{n+1}`
grows to


.. math::
         u^{n+1} = u^n + \frac{r}{100}u^n
        \thinspace . 

In reality, the interest rate is added every day. We therefore introduce
a parameter :math:`m` for the number of periods per year when the interest
is added. If :math:`n` counts the periods, we have the fundamental model
for compound interest:

.. math::
   :label: decay:app:interest:eq1
        
        u^{n+1} = u^n + \frac{r}{100 m}u^n
        \thinspace .
        
        

This model is a *difference equation*, but it can be transformed to a
continuous differential equation through a limit process.
The first step is to derive a formula for the growth of the investment
over a time :math:`t`.
Starting with an investment :math:`u^0`, and assuming that :math:`r` is constant in time,
we get

.. math::
        
        u^{n+1} &= \left(1 + \frac{r}{100 m}\right)u^{n}\\ 
        &= \left(1 + \frac{r}{100 m}\right)^2u^{n-1}\\ 
        &\ \ \vdots\\ 
        &= \left(1 +\frac{r}{100 m}\right)^{n+1}u^{0}
        

Introducing time :math:`t`, which here is a real-numbered counter for years,
we have that :math:`n=mt`, so we can write


.. math::
         u^{mt} = \left(1 + \frac{r}{100 m}\right)^{mt} u^0\thinspace . 

The second step is to assume *continuous compounding*, meaning that the
interest is added continuously. This implies :math:`m\rightarrow\infty`, and
in the limit one gets the formula

.. math::
        
        u(t) = u_0e^{rt/100},
        

which is nothing but the solution of the ODE problem

.. math::
   :label: decay:app:interest:eq2
        
        u' = \frac{r}{100}u,\quad u(0)=u_0
        \thinspace .
        
        

This is then taken as the ODE model for compound interest if :math:`r>0`.
However, the reasoning applies equally well to inflation, which is
just the case :math:`r<0`. One may also take the :math:`r` in :eq:`decay:app:interest:eq2`
as the net growth of an investemt, where :math:`r` takes both compound interest
and inflation into account. Note that for real applications we must
use a time-dependent :math:`r` in :eq:`decay:app:interest:eq2`.


Introducing :math:`a=\frac{r}{100}`, continuous inflation of an initial
fortune :math:`I` is then
a process exhibiting exponential decay according to

.. math::
         u' = -au,\quad u(0)=I\thinspace . 



.. _decay:app:nuclear:

Radioactive Decay
-----------------


.. index:: radioactive decay


An atomic nucleus of an unstable atom may lose energy by emitting
ionizing particles and thereby be transformed to a nucleus with a
different number of protons and neutrons.  This process is known as
`radioactive decay <http://en.wikipedia.org/wiki/Radioactive_decay>`_.
Actually, the process is stochastic when viewed for a single atom,
because it is impossible to predict exactly when a particular atom
emits a particle. Nevertheless, with a large number of atoms, :math:`N`, one
may view the process as deterministic and compute the mean behavior of
the decay. Below we reason intuitively about an ODE for the mean
behavior. Thereafter, we show mathematically that a detailed stochastic model
for single atoms leads the same mean behavior.

Deterministic model
~~~~~~~~~~~~~~~~~~~

Suppose at time :math:`t`, the number of the original atom type is :math:`N(t)`.
A basic model assumption is that the transformation of the atoms of the original
type in a small time interval :math:`\Delta t` is proportional to
:math:`N`, so that

.. math::
         N(t+\Delta t) = N(t) - a\Delta t N(t),

where :math:`a>0` is a constant. Introducing :math:`u=N(t)/N(0)`, dividing by
:math:`\Delta t` and letting :math:`\Delta t\rightarrow 0` gives the
following ODE:


.. math::
        
        u' = -au,\quad u(0)=1
        \thinspace .
        

The parameter :math:`a` can for a given nucleus be expressed through the
*half-life* :math:`t_{1/2}`, which is the time taken for the decay to reduce the
initial amount by one half, i.e., :math:`u(t_{1/2}) = 0.5`.
With :math:`u(t)=e^{-at}`, we get :math:`t_{1/2}=a^{-1}\ln 2` or :math:`a=\ln 2/t_{1/2}`.

.. `<http://en.wikipedia.org/wiki/Exponential_decay>`_


Stochastic model
~~~~~~~~~~~~~~~~

We have originally :math:`N_0` atoms. Each atom may have decayed or
survived at a particular time :math:`t`. We want to count how many original
atoms that are left, i.e., how many atoms that have survived.
The survival of a single atom at time :math:`t` is a random event. Since there
are only two outcomes, survival or decay, we have a
`Bernoulli trial <http://en.wikipedia.org/wiki/Bernoulli_trial>`_.
Let :math:`p` be the
probability of survival (implying that the probability of decay
is :math:`1-p`). If each atom survives independently of
the others, and the probability of survival is the same for every
atom, we have :math:`N_0` statistically Bernoulli trials, known as
a *binomial experiment* from probability theory.
The probability :math:`P(N)` that :math:`N` out
of the :math:`N_0` atoms have survived at time :math:`t` is then given by the
famous *binomial distribution*


.. math::
         P(N) = \frac{N_0!}{N! (N_0-N)!}p^N (1-p)^{N_0-N}{\thinspace .} 

The mean (or expected) value :math:`{\hbox{E}\lbrack P \rbrack}` of :math:`P(N)` is known to be :math:`N_0p`.

It remains to estimate :math:`p`. Let the interval :math:`[0,t]` be divided into :math:`m`
small subintervals of length :math:`\Delta t`. We make the assumption that
the probability of decay of a single atom in an interval of length :math:`\Delta t`
is :math:`\tilde p`, and that this probability is proportional to :math:`\Delta t`:
:math:`\tilde p = \lambda\Delta t` (it sounds natural that the probability
of decay increases with :math:`\Delta t`). The corresponding probability of survival
is :math:`1-\lambda\Delta t`. Believing that :math:`\lambda` is independent
of time, we have, for each interval of length :math:`\Delta t`,
a Bernoulli trial: the atom either survives or
decays in that interval. Now, :math:`p` should be the probability that the atom
survives in all the intervals, i.e., that we have :math:`m` successful
Bernoulli trials in a row and therefore


.. math::
         p = (1-\lambda\Delta t)^m{\thinspace .}

The expected number of atoms of the original type at time :math:`t` is


.. math::
        
        {\hbox{E}\lbrack P \rbrack} = N_0p = N_0(1-\lambda\Delta t)^m,\quad m=t/\Delta t{\thinspace .}
        


To see the relation between the two types of Bernoulli trials and the
ODE above, we go to the limit :math:`\Delta t\rightarrow t`, :math:`m\rightarrow\infty`.
One can show that


.. math::
         p = \lim_{m\rightarrow\infty} (1-\lambda\Delta t)^m
        = \lim_{m\rightarrow\infty} \left(1-\lambda\frac{t}{m}\right)^m = e^{-\lambda t}
        

This is the famous exponential waiting time (or arrival time) distribution for a
Poisson process in probability theory (obtained here, as often done, as
the limit of a binomial experiment). The probability of decay,
:math:`1-e^{-\lambda t}`, follows an `exponential distribution <http://en.wikipedia.org/wiki/Exponential_distribution>`_.
The limit means that :math:`m` is very
large, hence :math:`\Delta t` is very small, and :math:`\tilde p=\lambda\Delta t`
is very small since the intensity of the events, :math:`\lambda`, is assumed
finite. This situation corresponds to a very small probability
that an atom will decay in a very short time interval, which is a
reasonable model.
The same model occurs in lots of different applications, e.g.,
when waiting for a taxi, or when finding defects along a rope.

Relation between stochastic and deterministic models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With :math:`p=e^{-\lambda t}` we get the expected number of original atoms
at :math:`t` as :math:`N_0p=N_0e^{-\lambda t}`, which is exactly the solution of
the ODE model :math:`N'=-\lambda N`. This gives also an interpretation
of :math:`a` via :math:`\lambda` or vice versa. Our important finding here
is that the ODE model
captures the mean behavior of the underlying stochastic model. This
is, however, not always the common relation between microscopic stochastic
models and macroscopic "averaged" models.

Also of interest is to see that a Forward Euler discretization of
:math:`N'=-\lambda N`, :math:`N(0)=N_0`, gives :math:`N^m = N_0(1-\lambda\Delta t)^m`
at time :math:`t_m=m\Delta t`, which is exactly the
expected value of the stochastic experiment with :math:`N_0` atoms
and :math:`m` small intervals of length :math:`\Delta t`, where each atom can
decay with probability :math:`\lambda\Delta t` in an interval.

A fundamental question is how accurate the ODE model is. The underlying
stochastic model fluctuates around its expected value. A measure
of the fluctuations is the standard deviation of the binomial experiment with
:math:`N_0` atoms, which can be shown to be :math:`{\hbox{Std}\lbrack P \rbrack}=\sqrt{N_0p(1-p)}`. Compared
to the size of the expectation, we get
the normalized standard deviation


.. math::
         \frac{\sqrt{{\hbox{Var}\lbrack P \rbrack}}}{{\hbox{E}\lbrack P \rbrack}} = N_0^{-1/2}\sqrt{p^{-1}-1}
        = N_0^{-1/2}\sqrt{(1-e^{-\lambda t})^{-1}-1}\approx
        (N_0\lambda t)^{-1/2},
        

showing that the normalized fluctuations are very small if :math:`N_0` is
very large, which is usually the case.



.. _decay:app:Newton:cooling:

Newton's law of cooling
-----------------------

.. `<http://web.bham.ac.uk/winterhs/Newton.htm>`_

.. I. Newton, Scala Graduum Caloris, Philosophical Transactions of the Royal Society of London, 1701

.. explanation: `<http://www.madsci.org/posts/archives/2000-11/973522810.Ph.r.html>`_


When a body at some temperature is placed in a cooling environment,
experience shows that the temperature falls rapidly in the beginning,
and then the changes in temperature levels off until the body's
temperature equals that of the surroundings. Newton carried out some
experiments on cooling hot iron and found that the temperature
evolved as a "geometric progression at times in arithmetic progression",
meaning that the temperature decayed exponentially.
Later, this result was formulated as a differential equation:
the rate of change of the temperature in a body is proportional to
the temperature difference between the body and its surroundings.
This statement is known as *Newton's law of cooling*, which
can be mathematically expressed as

.. math::
   :label: decay:Newton:cooling
        
        {dT\over dt} = -k(T-T_s),
        
        

where :math:`T` is the temperature of the body, :math:`T_s` is the temperature
of the surroundings, :math:`t` is time, and :math:`k` is a positive constant.
Equation :eq:`decay:Newton:cooling` is primarily viewed as an
empirical law, valid when heat is efficiently convected away
from the surface of the body by a flowing fluid such as air
at constant temperature :math:`T_s`.
The *heat transfer coefficient* :math:`k` reflects the transfer of
heat from the body to
the surroundings and must be determined from physical experiments.

We must obviously have an initial condition :math:`T(0)=T_0` in addition
to the cooling law :eq:`decay:Newton:cooling`.


.. _decay:app:atm:

Decay of atmospheric pressure with altitude
-------------------------------------------

.. The Barometric Formula

.. `<http://en.wikipedia.org/wiki/Barometric_formula>`_


Vertical equilibrium of air in the atmosphere is governed by
the equation


.. math::
   :label: decay:app:atm:dpdz
        
        \frac{dp}{dz} = -\varrho g
        \thinspace .
        
        

Here, :math:`p(z)` is the air pressure, :math:`\varrho` is the density of
air, and :math:`g=9.807\hbox{ m/s}^2` is a standard value of
the acceleration of gravity.
(Equation :eq:`decay:app:atm:dpdz` follows directly from the general
Navier-Stokes equations for fluid motion, with
the assumption that the air does not move.)

The pressure is related to density and temperature through the ideal gas law


.. math::
   :label: decay:app:atm:rho
        
        \varrho = \frac{Mp}{R^*T}, 
        

where :math:`M` is the molar mass of the Earth's air (0.029 kg/mol),
:math:`R^*` is the universal
gas constant (:math:`8.314` Nm/(mol K)), and :math:`T` is the temperature.
All variables :math:`p`, :math:`\varrho`, and :math:`T` vary with the height :math:`z`.
Inserting :eq:`decay:app:atm:rho` in :eq:`decay:app:atm:dpdz` results
in an ODE with a variable coefficient:


.. math::
   :label: decay:app:atm:ode
        
        \frac{dp}{dz} = -\frac{Mg}{R^*T(z)} p
        
        \thinspace  .
        


Multiple atmospheric layers
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The atmosphere can be approximately modeled by seven layers.
In each layer, :eq:`decay:app:atm:ode` is applied with
a linear temperature of the form


.. math::
         T(z) = \bar T_i + L_i(z-h_i),

where :math:`z=h_i` denotes the bottom of layer number :math:`i`,
having temperature :math:`\bar T_i`,
and :math:`L_i` is a constant in layer number :math:`i`. The table below
lists :math:`h_i` (m), :math:`\bar T_i` (K), and :math:`L_i` (K/m) for the layers
:math:`i=0,\ldots,6`.

================  ================  ================  ================  
   :math:`i`        :math:`h_i`     :math:`\bar T_i`    :math:`L_i`     
================  ================  ================  ================  
0                                0               288           -0.0065  
1                           11,000               216               0.0  
2                           20,000               216             0.001  
3                           32,000               228            0.0028  
4                           47,000               270               0.0  
5                           51,000               270           -0.0028  
6                           71,000               214            -0.002  
================  ================  ================  ================  

For implementation it might be convenient to write :eq:`decay:app:atm:ode`
on the form

.. math::
        
        \frac{dp}{dz} = -\frac{Mg}{R^*(\bar T(z) + L(z)(z-h(z)))} p,
        

where :math:`\bar T(z)`, :math:`L(z)`, and :math:`h(z)` are piecewise constant
functions with values given in the table.
The value of the pressure at the sea level :math:`z=0`, :math:`p_0=p(0)`, is :math:`101325` Pa.

Simplification: :math:`L=0`
~~~~~~~~~~~~~~~~~~~~~~~~~~~

One commonly used simplification is to assume that the temperature is
constant within each layer. This means that :math:`L=0`.

Simplification: one-layer model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Another commonly used approximation is to work with one layer instead of
seven. This `one-layer model <http://en.wikipedia.org/wiki/Density_of_air>`_
is based on :math:`T(z)=T_0 - Lz`, with
sea level standard temperature :math:`T_0=288` K and
temperature lapse rate :math:`L=0.0065` K/m.

.. _decay:app:sediment:

Compaction of sediments
-----------------------

Sediments, originally made from materials like sand and mud, get
compacted through geological time by the weight of new material that
is deposited on the sea bottom. The porosity :math:`\phi` of the sediments
tells how much void (fluid) space there is between the sand and
mud grains. The porosity reduces with depth because the weight of
the sediments above and causes the void space to shrink and thereby
increase the compaction.

A typical assumption is that the change in :math:`\phi` at some depth :math:`z`
is negatively proportional to :math:`\phi`. This assumption leads to
the differential equation problem


.. math::
   :label: decay:app:sediment:phi:eq
        
        \frac{d\phi}{dz} = -c\phi,\quad \phi(0)=\phi_0,
        
        

where the :math:`z` axis points downwards, :math:`z=0` is the surface with known
porosity, and :math:`c>0` is a constant.

The upper part of the Earth's crust consists of many geological layers
stacked on top of each other, as indicated in Figure
:ref:`decay:app:sediment:fig:layers`.  The model
:eq:`decay:app:sediment:phi:eq` can be applied for each layer. In
layer number :math:`i`, we have the unknown porosity function :math:`\phi_i(z)`
fulfilling :math:`\phi_i'(z)=-c_iz`, since the constant :math:`c` in the model
:eq:`decay:app:sediment:phi:eq` depends on the type of sediment in
the layer. From the figure we see that new layers of sediments are
deposited on top of older ones as time progresses. The compaction,
as measured by :math:`\phi`, is
rapid in the beginning and then decreases (exponentially) with depth
in each layer.


.. _decay:app:sediment:fig:layers:

.. figure:: Compaction_of_Sediment.png
   :width: 600

   *Illustration of the compaction of geological layers (with different colors) through time*


When we drill a well at present time through the right-most column of
sediments in Figure :ref:`decay:app:sediment:fig:layers`, we can measure
the thickness of the sediment in (say) the bottom layer. Let :math:`L_1` be
this thickness.  Assuming that the volume of sediment remains constant
through time, we have that the initial volume, :math:`\int_0^{L_{1,0}}
\phi_1 dz`, must equal the volume seen today,
:math:`\int_{\ell-L_1}^{\ell}\phi_1 dz`, where :math:`\ell` is the depth of the
bottom of the sediment in the present day configuration.  After having
solved for :math:`\phi_1` as a function of :math:`z`, we can then find the
original thickness :math:`L_{1,0}` of the sediment from the equation


.. math::
         \int_0^{L_{1,0}} \phi_1 dz = \int_{\ell-L_1}^{\ell}\phi_1 dz \thinspace .

In hydrocarbon exploration it is important to know :math:`L_{1,0}` and the
compaction history of the various layers of sediments.

.. _decay:app:drag:

Vertical motion of a body in a viscous fluid
--------------------------------------------


A body moving vertically through a fluid (liquid or gas) is subject to
three different types of forces: the gravity force, `the drag force <http://en.wikipedia.org/wiki/Drag_(physics)>`_,
and the buoyancy force.

Overview of forces
~~~~~~~~~~~~~~~~~~

The gravity force is :math:`F_g= -mg`, where :math:`m` is the mass of the body and
:math:`g` is the acceleration of gravity.
The uplift or buoyancy force ("Archimedes force") is :math:`F_b = \varrho gV`,
where :math:`\varrho` is the density of the fluid and
:math:`V` is the volume of the body.
Forces and other quantities are taken as positive in the upward
direction.

The drag force is of two types, depending on the Reynolds number

.. math::
        
        \hbox{Re} = \frac{\varrho d|v|}{\mu},
        

where :math:`d` is the diameter of the body in
the direction perpendicular to the flow, :math:`v` is the velocity of the
body, and :math:`\mu` is the dynamic viscosity of the fluid.
When :math:`\hbox{Re} < 1`, the drag force is fairly well modeled by
the so-called Stokes' drag,
which for a spherical body of diameter :math:`d` reads

.. math::
        
        F_d^{(S)} = - 3\pi d\mu v
        \thinspace .
        

For large Re, typically :math:`\hbox{Re} > 10^3`, the drag force is quadratic
in the velocity:

.. math::
        
        F_d^{(q)} = -{1\over2}C_D\varrho A|v|v,
        

where :math:`C_D` is a dimensionless drag coefficient depending on the body's shape,
and :math:`A` is the cross-sectional area as
produced by a cut plane, perpendicular to the motion, through the thickest
part of the body. The superscripts :math:`\,{}^q` and :math:`\,{}^S` in
:math:`F_d^{(S)}` and :math:`F_d^{(q)}` indicates Stokes drag and quadratic drag,
respectively.

Equation of motion
~~~~~~~~~~~~~~~~~~

All the mentioned forces act in the vertical direction.
Newton's second law of motion applied to the body says that the sum of
these forces must equal the mass of the body times its acceleration
:math:`a` in the vertical direction.


.. math::
         ma = F_g + F_d^{(S)} + F_b ,

if we choose to work with the Stokes drag.
Inserting the expressions for the forces yields


.. math::
          ma = -mg - 3\pi d\mu v + \varrho gV
        \thinspace .
        

The unknowns here are :math:`v` and :math:`a`, i.e., we have two unknowns but only
one equation. From kinematics in physics we know that
the acceleration is the time derivative of the velocity: :math:`a = dv/dt`.
This is our second equation.
We can easily eliminate :math:`a` and get a single differential equation for :math:`v`:


.. math::
         m{dv\over dt} = -mg - 3\pi d\mu v + \varrho gV
        \thinspace .
        

A small rewrite of this equation is handy: We express :math:`m` as :math:`\varrho_bV`,
where :math:`\varrho_b` is the density of the body, and we divide by
the mass to get


.. math::
   :label: decay:app:fallingbody:model:S
        
        v'(t) = - \frac{3\pi d\mu}{\varrho_b V} v + g\left(\frac{\varrho}{\varrho_b} -1\right)
        
        \thinspace .
        

We may introduce the constants

.. math::
        
        a = \frac{3\pi d\mu}{\varrho_b V},\quad
        b = g\left(\frac{\varrho}{\varrho_b} -1\right),
        

so that the structure of the differential equation becomes evident:


.. math::
   :label: decay:app:fallingbody:gmodel:S
        
        v'(t) = -av(t) + b
        
        \thinspace .
        

The corresponding initial condition is :math:`v(0)=v_0` for some prescribed
starting velocity :math:`v_0`.

This derivation can be repeated with the quadratic drag force
:math:`F_d^{(q)}`, giving the result


.. math::
   :label: decay:app:fallingbody:model:q
        
        v'(t) =
        -{1\over2}C_D{\varrho A\over\varrho_b V}|v|v +
        g\left({\varrho\over\varrho_b} - 1\right)
        \thinspace .
        
        

Defining


.. math::
        
        a = {1\over2}C_D{\varrho A\over\varrho_b V},
        

and :math:`b` as above, we can write :eq:`decay:app:fallingbody:model:q` as

.. math::
   :label: decay:app:fallingbody:gmodel:q
        
        v'(t) = -a|v|v + b
        \thinspace .
        
        



.. index:: terminal velocity


Terminal velocity
~~~~~~~~~~~~~~~~~

An interesting aspect of :eq:`decay:app:fallingbody:gmodel:S` and
:eq:`decay:app:fallingbody:gmodel:q` is whether we can approach
a constant, so-called *terminal velocity* :math:`v_T`, as :math:`t\rightarrow\infty`. The
existence of :math:`v_T` assumes that
:math:`v'(t)\rightarrow 0` as :math:`t\rightarrow\infty` and therefore


.. math::
        0 = -av_T + b

and

.. math::
         0 = -a|v_T|v_T + b
        \thinspace .
        

The former equation implies :math:`v_T = b/a`, while the latter has solutions
:math:`v_T =-\sqrt{|b|/a}` for a falling body (:math:`v_T<0`) and
:math:`v_T = \sqrt{b/a}` for a rising body (:math:`v_T>0`).

A Crank-Nicolson scheme
~~~~~~~~~~~~~~~~~~~~~~~

Both governing equations, the Stokes' drag model
:eq:`decay:app:fallingbody:gmodel:S` and the quadratic drag model
:eq:`decay:app:fallingbody:gmodel:q`, can be readily solved
by the Forward Euler scheme. The Crank-Nicolson method gives
a nonlinear equation in :math:`v` when applied to
:eq:`decay:app:fallingbody:gmodel:q`:


.. math::
        
        \frac{v^{n+1}-v^n}{\Delta t}
        = -a\frac{1}{2}(|v^{n+1}|v^{n+1} + |v^n|v^n) + b
        \thinspace .
        

However, instead of approximating the term :math:`-|v|v` by an arithmetic
average, we can use a *geometric average*:


.. index:: geometric average


.. index::
   single: averaging; geometric



.. math::
        
        (|v|v)^{n+\frac{1}{2}} \approx |v^n|v^{n+1}
        \thinspace .
        

The error is of second order in :math:`\Delta t`, just as for the arithmetic
average. With this approximation trick,


.. math::
        
        \frac{v^{n+1}-v^n}{\Delta t} = - a|v^{n}|v^{n+1} + b
        

becomes a *linear* equation in :math:`v^{n+1}`, and we can
therefore easily solve for :math:`v^{n+1}`:

.. math::
   :label: decay:app:fallingbody:gmodel:q:CN
        
        v^{n+1} = \frac{v_n + \Delta t b^{n+\frac{1}{2}}}{1 + \Delta t a^{n+\frac{1}{2}}|v^{n}|}{\thinspace .}
        
        


Physical data
~~~~~~~~~~~~~

Suitable values of :math:`\mu` are :math:`1.8\cdot 10^{-5}\hbox{ Pa}\, \hbox{s}` for air
and :math:`8.9\cdot 10^{-4}\hbox{ Pa}\, \hbox{s}` for water.
Densities can be taken as :math:`1.2\hbox{ kg/m}^3` for air and as
:math:`1.0\cdot 10^3\hbox{ kg/m}^3` for water. For considerable vertical
displacement in the atmosphere one should take into account that
the density of air varies with height, see the section :ref:`decay:app:atm`.
One possible density variation arises from the one-layer model
in the section :ref:`decay:app:atm`.

Any density variation makes :math:`b` time dependent and we need
:math:`b^{n+\frac{1}{2}}` in :eq:`decay:app:fallingbody:gmodel:q:CN`.
To compute :math:`b^{n+\frac{1}{2}}` we must also compute the vertical
position :math:`z(t)` of the body. Since :math:`v=dz/dt`, we can use a centered
difference approximation:


.. math::
         \frac{z^{n+\frac{1}{2}} - z^{n-\frac{1}{2}}}{\Delta t} = v^n
        \quad\Rightarrow\quad z^{n+\frac{1}{2}} = z^{n-\frac{1}{2}}+\Delta t\, v^n{\thinspace .}

This :math:`z^{n+\frac{1}{2}}` is used in the expression for :math:`b`
to compute :math:`\varrho(z^{n+\frac{1}{2}})` and then :math:`b^{n+\frac{1}{2}}`.

The `drag coefficient <http://en.wikipedia.org/wiki/Drag_coefficient>`_ :math:`C_D` depends heavily
on the shape of the body.  Some values are: 0.45 for a sphere, 0.42
for a semi-sphere, 1.05 for a cube, 0.82 for a long cylinder (with the
length along the vertical direction), 0.75 for a rocket,
1.0-1.3 for a man in upright position, 1.3 for a flat plate perpendicular
to the flow, and
0.04 for a streamlined (droplet-like) body.

Verification
~~~~~~~~~~~~

To verify the program, one may assume a heavy body in air such that the :math:`F_b`
force can be neglected, and further assume a small velocity such that the
air resistance :math:`F_d` can also be neglected. Simply setting :math:`\varrho =0`
removes both these terms from the equation. The motion then leads to
the velocity
:math:`v(t)=v_0 - gt`, which is linear in :math:`t` and therefore should be
reproduced to machine precision (say tolerance :math:`10^{-15}`) by any
implementation based on the Crank-Nicolson or Forward Euler schemes.

Another verification, but not as powerful as the one above,
can be based on computing the terminal velocity and comparing with
the exact expressions.
The advantage of this verification is that we can also
the test situation :math:`\varrho\neq 0`.

Obviously, the method of manufactured solutions can be applied to
test the implementation of all terms in the governing equation.


.. index:: scaling


Scaling  (2)
~~~~~~~~~~~~

Applying scaling, as described in the section :ref:`decay:app:scaling`,
will for the linear case reduce the need to estimate values for
seven parameters to choose values of *one* dimensionless parameter


.. math::
         \beta = \frac{\varrho_bgV\left(\frac{\varrho}{\varrho_b} -1\right)}{3\pi d\mu I},

if :math:`I\neq 0`, and if the motion starts from rest (:math:`I=0`), the scaled
problem :math:`\bar u'=1-\bar u`, :math:`\bar u(0)=0`, has no need for
estimating physical parameters.
This means that there is one universal solution to the problem,
the scaled solution :math:`\bar u(t) = 1 - e^{-\bar t}`, and all
physical cases correspond to stretching the :math:`\bar t` axis and the :math:`\bar u`
axis, i.e.,


.. math::
         u = \frac{\varrho_bgV\left(\frac{\varrho}{\varrho_b} -1\right)}{3\pi d\mu}\bar u(t/(g(\varrho/\varrho_b -1))){\thinspace .}





.. _decay:app:diffusion:Fourier:

Decay ODEs from solving a PDE by Fourier expansions
---------------------------------------------------

.. Maybe move to diffusion part? Makes sense there too, or refer...or

.. repeat, or make one exer with two k's and then generalize in diffusion


Suppose we have a partial differential equation

.. math::
         \frac{\partial u}{\partial t} = \alpha\frac{\partial^2u}{\partial x^2}
        + f(x,t),
        

with boundary conditions :math:`u(0,t)=u(L,t)=0` and initial condition
:math:`u(x,0)=I(x)`. One may express the solution as

.. math::
         u(x,t) = \sum_{k=1}^m A_k(t)e^{ikx\pi/L},

for appropriate unknown functions :math:`A_k`, :math:`k=1,\ldots,m`.
We use the complex exponential :math:`e^{ikx\pi/L}` for easy algebra, but
the physical :math:`u` is taken as the real part of any complex expression.
Note that the expansion in terms of :math:`e^{ikx\pi/L}` is compatible with
the boundary conditions: all functions :math:`e^{ikx\pi/L}` vanish for
:math:`x=0` and :math:`x=L`. Suppose we can express :math:`I(x)` as


.. math::
         I(x) = \sum_{k=1}^m I_ke^{ikx\pi/L}
        \thinspace .
        

Such an expansion can be computed by well-known Fourier expansion techniques,
but the details are not important here.
Also, suppose we can express the given :math:`f(x,t)` as

.. math::
         f(x,t) = \sum_{k=1}^m b_k(t)e^{ikx\pi/L}
        \thinspace .
        

Inserting the expansions for :math:`u`
and :math:`f` in the differential equations demands that all terms corresponding
to a given :math:`k` must be equal. The calculations results in the follow
system of ODEs:


.. math::
        
        A_k'(t) = -\alpha\frac{k^2\pi^2}{L^2} + b_k(t),\quad k=1,\ldots,m
        \thinspace .
        

From the initial condition

.. math::
         u(x,0)=\sum_k A_k(0)e^{ikx\pi/L}=I(x)=\sum_k I_k e^{(ikx\pi/L)},

it follows that :math:`A_k(0)=I_k`, :math:`k=1,\ldots,m`. We then have :math:`m`
equations of the form :math:`A_k'=-a A_k +b`, :math:`A_k(0)=I_k`, for
appropriate definitions of :math:`a` and :math:`b`. These ODE problems
independent each other such that we can solve one problem
at a time. The outline technique is a quite common approach for solving
partial differential equations.

*Remark.* Since :math:`a_k` depends on :math:`k` and the stability of
the Forward Euler scheme demands :math:`a_k\Delta t \leq 1`, we get that
:math:`\Delta t \leq \alpha^{-1}L^2\pi^{-2} k^{-2}`. Usually, quite large
:math:`k` values are needed to accurately represent the given functions
:math:`I` and :math:`f` and then :math:`\Delta t` needs to be very small for these large
values of :math:`k`.
Therefore, the Crank-Nicolson and Backward Euler schemes, which
allow larger :math:`\Delta t` without any growth in the solutions, are
more popular choices when creating time-stepping algorithms for
partial differential equations of the type considered in this
example.


Exercises  (4)
==============




.. --- begin exercise ---


.. _decay:app:exer:cooling:osc:

Exercise 19: Simulate an oscillating cooling process
----------------------------------------------------

The surrounding temperature :math:`T_s` in Newton's law of cooling
:eq:`decay:Newton:cooling` may vary in time. Assume that the
variations are periodic with period :math:`P` and amplitude :math:`a` around
a constant mean temperature :math:`T_m`:

.. math::
        
        T_s(t) = T_m + a\sin\left(\frac{2\pi}{P}t\right)
        \thinspace .
        

Simulate a process with the following data: :math:`k=20 \hbox{ min}^{-1}`,
:math:`T(0)=5` C, :math:`T_m=25` C, :math:`a=2.5` C, and :math:`P=1` h. Also experiment with
:math:`P=10` min and :math:`P=3` h. Plot :math:`T` and :math:`T_s` in the same plot.
Filename: ``osc_cooling.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:radio:C14:

Exercise 20: Radioactive decay of Carbon-14
-------------------------------------------

The `Carbon-14 <http://en.wikipedia.org/wiki/Carbon-14>`_ isotope,
whose radioactive decay is used extensively in dating organic material
that is tens of thousands of years old, has a half-life of :math:`5,730`
years.  Determine the age of an organic material that contains 8.4 percent
of its initial amount of Carbon-14.  Use a time unit of 1 year in the
computations.  The uncertainty in the half time of Carbon-14 is :math:`\pm
40` years.  What is the corresponding uncertainty in the estimate of
the age?

.. --- begin hint in exercise ---


*Hint.* Use simulations with :math:`5,730\pm 40` y as input
and find the corresponding interval for the result.

.. --- end hint in exercise ---

Filename: ``carbon14.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:stoch:nuclear:

Exercise 21: Simulate stochastic radioactive decay
--------------------------------------------------

The purpose of this exercise is to implement the stochastic model
described in the section :ref:`decay:app:nuclear` and show that its
mean behavior approximates the solution of the corresponding
ODE model.

The simulation goes on for a time interval :math:`[0,T]` divided into
:math:`N_t` intervals of length :math:`\Delta t`. We start with :math:`N_0`
atoms. In some time interval, we have :math:`N` atoms that have survived.
Simulate :math:`N` Bernoulli trials with probability :math:`\lambda\Delta t`
in this interval by drawing :math:`N` random numbers, each being 0 (survival)
or 1 (decay), where the probability of getting 1 is :math:`\lambda\Delta t`.
We are interested in the number of decays, :math:`d`, and the number of
survived atoms in the next interval is then :math:`N-d`.
The Bernoulli trials
are simulated by drawing :math:`N` uniformly distributed real numbers on
:math:`[0,1]` and saying that 1 corresponds to a value less than :math:`\lambda\Delta t`:


.. code-block:: python

        # Given lambda_, dt, N
        import numpy as np
        uniform = np.random.uniform(N)
        Bernoulli_trials = np.asarray(uniform < lambda_*dt, dtype=np.int)
        d = Bernoulli_trials.size

Observe that ``uniform < lambda_*dt`` is a boolean array whose true
and false values become 1 and 0, respectively, when converted to an
integer array.

Repeat the simulation over :math:`[0,T]` a large number of times, compute the average
value of :math:`N` in each interval, and compare with the solution of
the corresponding ODE model.
Filename: ``stochastic_decay.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:radio:twosubst:

Exercise 22: Radioactive decay of two substances
------------------------------------------------

Consider two radioactive substances A and B. The nuclei in substance A
decay to form nuclei of type B with a half-life :math:`A_{1/2}`, while
substance B decay to form type A nuclei with a half-life :math:`B_{1/2}`.
Letting :math:`u_A` and :math:`u_B` be the fractions of the initial amount of
material in substance A and B, respectively, the following system of
ODEs governs the evolution of :math:`u_A(t)` and :math:`u_B(t)`:

.. math::
        
        \frac{1}{\ln 2} u_A' = u_B/B_{1/2} - u_A/A_{1/2},
        



.. math::
          
        \frac{1}{\ln 2} u_B' = u_A/A_{1/2} - u_B/B_{1/2},
        

with :math:`u_A(0)=u_B(0)=1`.

Make a simulation program that solves for :math:`u_A(t)` and :math:`u_B(t)`.
Verify the implementation by computing analytically
the limiting values of
:math:`u_A` and :math:`u_B` as :math:`t\rightarrow \infty` (assume :math:`u_A',u_B'\rightarrow 0`)
and comparing these with those obtained numerically.

Run the program for the case of :math:`A_{1/2}=10` minutes and :math:`B_{1/2}=50` minutes.
Use a time unit of 1 minute. Plot :math:`u_A` and :math:`u_B` versus time in the same
plot.
Filename: ``radioactive_decay_2subst.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:atm1:

Exercise 23: Simulate the pressure drop in the atmosphere
---------------------------------------------------------

We consider the models for atmospheric pressure in
the section :ref:`decay:app:atm`.
Make a program with three functions,

 * one computing the pressure :math:`p(z)` using a seven-layer model
   and varying :math:`L`,

 * one computing :math:`p(z)` using a seven-layer model,
   but with constant temperature in each layer, and

 * one computing :math:`p(z)` based on the
   one-layer model.

How can these implementations be verified? Should ease of verification
impact how you code the functions?
Compare the three models in a plot.
Filename: ``atmospheric_pressure.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:drag:prog:

Exercise 24: Make a program for vertical motion in a fluid
----------------------------------------------------------

Implement the Stokes' drag model :eq:`decay:app:fallingbody:model:S`
and the quadratic drag model :eq:`decay:app:fallingbody:model:q` from
the section :ref:`decay:app:drag` in a function, using the Crank-Nicolson
scheme as explained. At each time level, compute the Reynolds number
Re and choose the Stokes' drag model if :math:`\hbox{Re} < 1` and the
quadratic drag model otherwise.  Include nose tests (in the file) that
runs the two suggested verification tests in the section :ref:`decay:app:drag`.

Apply the function to a case involving a ball rising in water.  The
buoyancy force is here the driving force, but the drag will be
significant and balance the other forces after some time.  A soccer
ball has :math:`a=11` cm and mass 0.43 kg.  Start the motion from rest, set
the density of water to :math:`1000\hbox{ kg/m}^3`, and use a drag
coefficient for a ball: 0.45. Plot the velocity of the rising ball.
Filename: ``vertical_motion.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:class:

Exercise 25: Use classes to make a simulation program
-----------------------------------------------------

Implement the software from :ref:`decay:app:exer:drag:prog`
in terms of problem, solver, and visualization classes in a module as
explained in the section :ref:`decay:prog:se:class`. Incorporate
nose tests in the module.
Filename: ``problem_simulate_class.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:parachute:

Project 1: Simulate parachuting
-------------------------------

The aim of this project is to develop a general solver for the
vertical motion of a body with quadratic air drag, verify the solver,
apply the solver to a skydiver in free fall, and finally apply the
solver to a complete parachute jump.

All the pieces of software implemented in this project
should be realized as Python functions and collected
in one module.


*a)* Set up the differential equation problem that governs the velocity
of the motion.
The parachute jumper is subject to the gravity force and a quadratic
drag force.
Add an extra source term be used for program verification.
Identify the input data to the problem.

*b)* Make a function ``solver`` for computing the velocity of the motion.
The function takes the input data in the problem as arguments and
returns the velocity and the mesh points.

.. --- begin hint in exercise ---


*Hint.* Use the Crank-Nicolson scheme with a geometric average in time to
linearize the equation of motion with quadratic drag.

.. --- end hint in exercise ---


*c)* Check if a linear solution fulfills the discrete equation and can be
used for verification in the sense that the numerical solution should
produce this solution without errors (to machine precision).

Construct verification tests for the ``solver`` function in simplified
physical cases where you have information about the exact solution
Also use the method of manufactured solutions to create a verification test.
Convergence rates must be computed in these cases.

Implement all tests as nose tests.

*d)* Make a function ``forces(v, t, plot=None)``
for computing the drag force, the gravity
force, and the buoyancy force as a function of time. Create
a plot with these three functions if ``plot`` is not ``None`` but instead
a string specifying the name of the plot file. Show the plot on
the screen too.

*e)* Compute the velocity of
a skydiver in free fall before the parachute opens.

.. --- begin hint in exercise ---


*Hint.* Meade and Struthers [Ref6]_ provide some data relevant
to `skydiving <http://en.wikipedia.org/wiki/Parachuting>`_.
The mass of the human body and equipment
can be set to :math:`100` kg.
A skydiver in spread-eagle formation has a cross-section of 0.5 :math:`\hbox{m}^2`
in the horizontal plane.
The density of air decreases varies altitude, but can be taken
as constant, 1 :math:`\hbox{kg/m}^3`, for altitudes relevant to
skydiving (0-4000 m).
The drag coefficient for a man in upright position can be set to 1.2.
Start with a zero velocity.
A free fall typically has a terminating velocity of 45 m/s. (This value
can be used to tune other parameters.)
.. --- end hint in exercise ---


*f)* The next task is to simulate
a parachute jumper during free fall and after the parachute opens.
At the time the parachute opens, the drag coefficient and the cross-sectional
area change dramatically.
Generalize the solver function to a new function ``solver_parachute`` where
the drag coefficient and the cross-sectional area have different
values before and after time :math:`t_p` when the parachute is released.

Use the program to simulate a jump from :math:`z=3000` m to the ground :math:`z=0`.

.. --- begin hint in exercise ---


*Hint.* Following Meade and Struthers [Ref6]_, one can set the
cross-section area perpendicular to the motion to 44 :math:`\hbox{m}^2`
when the parachute is open. The drag coefficient for an open
parachute can be taken as 1.8, but tuned using the known value
of the typical terminating velocity reached before landing:
5.3 m/s. The parachute is released after 10 s.

.. --- end hint in exercise ---


Filename: ``skydiving.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:drag:atm1:

Exercise 26: Formulate vertical motion in the atmosphere
--------------------------------------------------------

Vertical motion of a body in the atmosphere needs to take into
account a varying air density if the range of altitudes is
many kilometers. In this case, :math:`\varrho` varies with the altitude :math:`z`.
The equation of motion for the body is given in
the section :ref:`decay:app:drag`. Let us assume quadratic drag force
(otherwise the body has to be very, very small).
A differential equation problem for the air density, based on
the information for the one-layer atmospheric model in
the section :ref:`decay:app:atm`, can be set up as


.. math::
        
        p'(z) = -\frac{Mg}{R^*(T_0+Lz)} p,
        



.. math::
          
        \varrho = p \frac{M}{R^*T}
        \thinspace .
        

To evaluate :math:`p(z)` we need the altitude :math:`z`. From the principle that the
velocity is the derivative of the position we have that


.. math::
        
        z'(t) = v(t),
        

where :math:`v` is the velocity of the body.

Explain in detail how the governing equations can be discretized
by the Forward Euler and the Crank-Nicolson methods.
Filename: ``falling_in_variable_density.pdf``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:drag:atm2:

Exercise 27: Simulate vertical motion in the atmosphere
-------------------------------------------------------

Implement the Forward Euler or the Crank-Nicolson scheme
derived in :ref:`decay:app:exer:drag:atm1`.
Demonstrate the effect of air density variation on a falling
human, e.g., the famous fall of `Felix Baumgartner <http://en.wikipedia.org/wiki/Felix_Baumgartner>`_. The drag coefficient can be set to 1.2.

*Remark.* In the Crank-Nicolson scheme one must solve a :math:`3\times 3` system of
equations at each time level, since :math:`p`, :math:`\varrho`, and :math:`v` are
coupled, while each equation can be stepped forward at a time with the
Forward Euler scheme.
Filename: ``falling_in_variable_density.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:signum:

Exercise 28: Compute :math:`y=|x|` by solving an ODE
----------------------------------------------------

Consider the ODE problem

.. math::
        
        y'(x) = \left\lbrace\begin{array}{ll}
        -1, & x < 0,\\ 
        1, & x \geq 0
        \end{array}\right.\quad x\in (-1, 1],
        \quad y(1-)=1,
        

which has the solution :math:`y(x)=|x|`.
Using a mesh :math:`x_0=-1`, :math:`x_1=0`, and :math:`x_2=1`, calculate by hand
:math:`y_1` and :math:`y_2` from the Forward Euler, Backward Euler, Crank-Nicolson,
and Leapfrog methods. Use all of the former three methods for computing
the :math:`y_1` value to be used in the Leapfrog calculation of :math:`y_2`.
Thereafter, visualize how these schemes perform for a uniformly partitioned
mesh with :math:`N=10` and :math:`N=11` points.
Filename: ``signum.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:interest:

Exercise 29: Simulate growth of a fortune with random interest rate
-------------------------------------------------------------------

The goal of this exercise is to compute the value of a fortune subject
to inflation and a random interest rate.
Suppose that the inflation is constant at :math:`i` percent per year and that the
annual interest rate, :math:`p`, changes randomly at each time step,
starting at some value :math:`p_0` at :math:`t=0`.
The random change is from a value :math:`p^n` at :math:`t=t_n` to
:math:`p_n +\Delta p` with probability 0.25 and :math:`p_n -\Delta p` with probability 0.25.
No change occurs with probability 0.5. There is also no change if
:math:`p^{n+1}` exceeds 15 or becomes below 1.
Use a time step of one month, :math:`p_0=i`, initial fortune scaled to 1,
and simulate 1000 scenarios of
length 20 years. Compute the mean evolution of one unit of money and the
corresponding
standard deviation. Plot the mean curve along with the mean plus one
standard deviation and the mean minus one standard deviation. This will
illustrate the uncertainty in the mean curve.

.. --- begin hint in exercise ---


*Hint 1.* The following code snippet computes :math:`p^{n+1}`:

.. code-block:: python

        import random
        
        def new_interest_rate(p_n, dp=0.5):
            r = random.random()  # uniformly distr. random number in [0,1)
            if 0 <= r < 0.25:
                p_np1 = p_n + dp
            elif 0.25 <= r < 0.5:
                p_np1 = p_n - dp
            else:
                p_np1 = p_n
            return (p_np1 if 1 <= p_np1 <= 15 else p_n)

.. --- end hint in exercise ---


.. --- begin hint in exercise ---


*Hint 2.* If :math:`u_i(t)` is the value of the fortune in experiment number :math:`i`,
:math:`i=0,\ldots,N-1`,
the mean evolution of the fortune is

.. math::
         \bar u(t)= \frac{1}{N}\sum_{i=0}^{N-1} u_i(t),
        

and the standard deviation is

.. math::
         s(t) = \sqrt{\frac{1}{N-1}\left(- (\bar u(t))^2 +
                        \sum_{i=0}^{N-1} (u_i(t))^2\right)}
        \thinspace .
        

Suppose :math:`u_i(t)` is stored in an array ``u``.
The mean and the standard deviation of the fortune
is most efficiently computed by
using two accumulation arrays, ``sum_u`` and ``sum_u2``, and
performing ``sum_u += u`` and ``sum_u2 += u**2`` after every experiment.
This technique avoids storing all the :math:`u_i(t)` time series for
computing the statistics.

.. --- end hint in exercise ---

Filename: ``random_interest.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:pop:at:

Exercise 30: Simulate a population in a changing environment
------------------------------------------------------------

We shall study a population modeled by :eq:`decay:app:pop:ueq` where
the environment, represented by :math:`r` and :math:`f`, undergoes changes with time.


*a)* Assume that there is a sudden drop (increase) in the birth (death)
rate at time :math:`t=t_r`,
because of limited nutrition or food supply:

.. math::
         a(t) =\left\lbrace\begin{array}{ll}
        r_0, & t< t_r,\\ 
        r_0 - A, & t\geq t_r,\end{array}\right.
        

This drop in population growth is compensated by a sudden net immigration
at time :math:`t_f>t_r`:

.. math::
         f(t) =\left\lbrace\begin{array}{ll}
        0, & t< t_f,\\ 
        f_0, & t\geq t_a,\end{array}\right.
        

Start with :math:`r_0` and make :math:`A>r_0`. Experiment with
these and other parameters to
illustrate the interplay of growth and decay in such a problem.
Filename: ``population_drop.py``.

*b)* Now we assume that the environmental conditions changes periodically with
time so that we may take

.. math::
         r(t) = r_0 + A\sin\left(\frac{2\pi}{P}t\right)
        \thinspace .
        

That is, the combined birth and death rate oscillates around :math:`r_0` with
a maximum change of :math:`\pm A` repeating over a period of length :math:`P` in time.
Set :math:`f=0` and experiment with the other parameters to illustrate typical
features of the solution.
Filename: ``population_osc.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:pop:logistic1:

Exercise 31: Simulate logistic growth
-------------------------------------

Solve the logistic ODE
:eq:`decay:app:pop:logistic` using a Crank-Nicolson scheme where
:math:`(u^{n+1/2})^2` is approximated by a *geometric mean*:

.. math::
         (u^{n+1/2})^2 \approx u^{n+1}u^n
        \thinspace .
        

This trick makes the discrete equation linear in :math:`u^{n+1}`.
Filename: ``logistic_CN.py``.

.. --- end exercise ---





.. --- begin exercise ---


.. _decay:app:exer:interest:derive:

Exercise 32: Rederive the equation for continuous compound interest
-------------------------------------------------------------------

The ODE model :eq:`decay:app:interest:eq2` was derived under the assumption
that :math:`r` was constant. Perform an alternative derivation without
this assumption: 1) start with :eq:`decay:app:interest:eq1`;
2) introduce a time step :math:`\Delta t` instead of :math:`m`: :math:`\Delta t = 1/m` if
:math:`t` is measured in years; 3) divide by :math:`\Delta t` and take the
limit :math:`\Delta t\rightarrow 0`. Simulate a case where the inflation is
at a constant level :math:`I` percent per year and the interest rate oscillates:
:math:`r=-I/2 + r_0\sin(2\pi t)`.
Compare solutions for :math:`r_0=I, 3I/2, 2I`.
Filename: ``interest_modeling.py``.

.. --- end exercise ---



.. !split

.. [Ref1]
   **H. P. Langtangen**. *A Primer on Scientific Programming With Python*,
   Springer,
   2012.

.. [Ref2]
   **L. Petzold and U. M. Ascher**. *Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations*,
   SIAM,
   1998.

.. [Ref3]
   **D. Griffiths, F. David and D. J. Higham**. *Numerical Methods for Ordinary Differential Equations: Initial Value Problems*,
   Springer,
   2010.

.. [Ref4]
   **E. Hairer, S. P. N\orsett and G. Wanner**. *Solving Ordinary Differential Equations I. Nonstiff Problems*,
   Springer,
   1993.

.. [Ref5]
   **G. Hairer and E. Wanner**. *Solving Ordinary Differential Equations II*,
   Springer,
   2010.

.. [Ref6]
   **D. B. Meade and A. A. Struthers**. Differential Equations in the New Millenium: the Parachute Problem,
   *International Journal of Engineering Education*,
   15(6),
   pp. 417-424,
   1999,
   `http://www.matematicas.unam.mx/gfgf/ode/ode_files/parachute.pdf <http://www.matematicas.unam.mx/gfgf/ode/ode_files/parachute.pdf>`_.


