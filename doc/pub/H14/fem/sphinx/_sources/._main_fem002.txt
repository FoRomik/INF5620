.. !split

.. _fem:approx:vec:

Approximation of vectors
========================

We shall start with introducing two fundamental methods for
determining the coefficients :math:`c_i` in :ref:`(2.1) <Eq:fem:u>` and illustrate
the methods on approximation of vectors, because vectors in vector
spaces give a more intuitive understanding than starting directly
with approximation of functions in function spaces.
The extension from vectors to functions will be trivial as soon as
the fundamental ideas are understood.

The first method of approximation is called the *least squares method*
and consists in finding :math:`c_i` such that the difference :math:`u-f`, measured
in some norm, is minimized. That is, we aim at finding the best
approximation :math:`u` to :math:`f` (in some norm). The second method is not
as intuitive: we find :math:`u` such that the error :math:`u-f` is orthogonal to
the space where we seek :math:`u`. This is known as *projection*, or
we may also call it a *Galerkin method*.
When approximating vectors and functions, the two methods are
equivalent, but this is no longer the case when applying the
principles to differential equations.

.. _fem:approx:vec:plane:

Approximation of planar vectors
-------------------------------

.. index::
   single: approximation; of vectors in the plane

Suppose we have given a vector :math:`\boldsymbol{f} = (3,5)` in the :math:`xy` plane
and that we want to approximate this vector by a vector aligned
in the direction of the vector :math:`(a,b)`. Figure :ref:`fem:approx:vec:plane:fig`
depicts the situation.

.. _fem:approx:vec:plane:fig:

.. figure:: vecapprox_plane.png
   :width: 400

   *Approximation of a two-dimensional vector by a one-dimensional vector*

We introduce the vector space :math:`V`
spanned by the vector :math:`\boldsymbol{\psi}_0=(a,b)`:

.. math::
        
        V = \mbox{span}\,\{ \boldsymbol{\psi}_0\}{\thinspace .}  

We say that :math:`\boldsymbol{\psi}_0` is a basis vector in the space :math:`V`.
Our aim is to find the vector :math:`\boldsymbol{u} = c_0\boldsymbol{\psi}_0\in V` which best approximates
the given vector :math:`\boldsymbol{f} = (3,5)`. A reasonable criterion for a best
approximation could be to minimize the length of the difference between
the approximate :math:`\boldsymbol{u}` and the given :math:`\boldsymbol{f}`. The difference, or error
:math:`\boldsymbol{e} = \boldsymbol{f} -\boldsymbol{u}`, has its length given by the *norm*

.. math::
         ||\boldsymbol{e}|| = (\boldsymbol{e},\boldsymbol{e})^{\frac{1}{2}},

where :math:`(\boldsymbol{e},\boldsymbol{e})` is the *inner product* of :math:`\boldsymbol{e}` and itself. The inner
product, also called *scalar product* or *dot product*, of two vectors
:math:`\boldsymbol{u}=(u_0,u_1)` and :math:`\boldsymbol{v} =(v_0,v_1)` is defined as

.. math::
        
        (\boldsymbol{u}, \boldsymbol{v}) = u_0v_0 + u_1v_1{\thinspace .}  

**Remark 1.**
We should point out that we use the notation
:math:`(\cdot,\cdot)` for two different things: :math:`(a,b)` for scalar
quantities :math:`a` and :math:`b` means the vector starting in the origin and
ending in the point :math:`(a,b)`, while :math:`(\boldsymbol{u},\boldsymbol{v})` with vectors :math:`\boldsymbol{u}` and
:math:`\boldsymbol{v}` means the inner product of these vectors.  Since vectors are here
written in boldface font there should be no confusion.  We may add
that the norm associated with this inner product is the usual
Eucledian length of a vector.

**Remark 2.**
It might be wise to refresh some basic linear algebra by consulting a
textbook.  :ref:`fem:approx:exer:linalg1` and
:ref:`fem:approx:exer:linalg2` suggest specific tasks to regain
familiarity with fundamental operations on inner product vector
spaces.

.. index::
   single: least squreas method; vectors

The least squares method  (1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We now want to find :math:`c_0` such that it minimizes :math:`||\boldsymbol{e}||`. The algebra
is simplified if we minimize the square of the norm, :math:`||\boldsymbol{e}||^2 = (\boldsymbol{e}, \boldsymbol{e})`,
instead of the norm itself.
Define the function

.. math::
        
        E(c_0) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{f} - c_0\boldsymbol{\psi}_0)
        {\thinspace .}
        

We can rewrite the expressions of the right-hand side in a more
convenient form for further work:

.. _Eq:fem:vec:E:

.. math::
   :label: fem:vec:E
        
        E(c_0) = (\boldsymbol{f},\boldsymbol{f}) - 2c_0(\boldsymbol{f},\boldsymbol{\psi}_0) + c_0^2(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0){\thinspace .}
        
        

The rewrite results from using the following fundamental rules for inner
product spaces:

.. _Eq:fem:vec:rule:scalarmult:

.. math::
   :label: fem:vec:rule:scalarmult
        
        (\alpha\boldsymbol{u},\boldsymbol{v})=\alpha(\boldsymbol{u},\boldsymbol{v}),\quad \alpha\in\mathbb{R},
        
        

.. _Eq:fem:vec:rule:sum:

.. math::
   :label: fem:vec:rule:sum
        
        (\boldsymbol{u} +\boldsymbol{v},\boldsymbol{w}) = (\boldsymbol{u},\boldsymbol{w}) + (\boldsymbol{v}, \boldsymbol{w}),
        
        

.. _Eq:fem:vec:rule:symmetry:

.. math::
   :label: fem:vec:rule:symmetry
        
        (\boldsymbol{u}, \boldsymbol{v}) = (\boldsymbol{v}, \boldsymbol{u}){\thinspace .}  
        

Minimizing :math:`E(c_0)` implies finding :math:`c_0` such that

.. math::
         \frac{\partial E}{\partial c_0} = 0{\thinspace .}  

Differentiating :eq:`fem:vec:E` with respect to :math:`c_0` gives

.. _Eq:fem:vec:dEdc0:v1:

.. math::
   :label: fem:vec:dEdc0:v1
        
        \frac{\partial E}{\partial c_0} = -2(\boldsymbol{f},\boldsymbol{\psi}_0) + 2c_0 (\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)
        {\thinspace .}
        
        

Setting the above expression equal to zero and solving for :math:`c_0` gives

.. _Eq:fem:vec:c0:

.. math::
   :label: fem:vec:c0
        
        c_0 = \frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)},
        
        

which in the present case with :math:`\boldsymbol{\psi}_0=(a,b)` results in

.. math::
        
        c_0 = \frac{3a + 5b}{a^2 + b^2}{\thinspace .}  

For later, it is worth mentioning that setting
the key equation :eq:`fem:vec:dEdc0:v1` to zero can be rewritten
as

.. math::
        
        (\boldsymbol{f}-c0\boldsymbol{\psi}_0,\boldsymbol{\psi}_0) = 0,
        

or

.. _Eq:fem:vec:dEdc0:Galerkin:

.. math::
   :label: fem:vec:dEdc0:Galerkin
        
        (\boldsymbol{e}, \boldsymbol{\psi}_0) = 0
        {\thinspace .}
        
        

.. index::
   single: Galerkin method; vectors

.. index::
   single: projection; vectors

The projection method
~~~~~~~~~~~~~~~~~~~~~

We shall now show that minimizing :math:`||\boldsymbol{e}||^2` implies that :math:`\boldsymbol{e}` is
orthogonal to *any* vector :math:`\boldsymbol{v}` in the space :math:`V`. This result is
visually quite clear from Figure :ref:`fem:approx:vec:plane:fig` (think of
other vectors along the line :math:`(a,b)`: all of them will lead to
a larger distance between the approximation and :math:`\boldsymbol{f}`).
To see this result mathematically, we
express any :math:`\boldsymbol{v}\in V` as :math:`\boldsymbol{v}=s\boldsymbol{\psi}_0` for any scalar parameter :math:`s`,
recall that two vectors are orthogonal when their inner product vanishes,
and calculate the inner product

.. math::
        
        (\boldsymbol{e}, s\boldsymbol{\psi}_0) &= (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\ 
        &= (\boldsymbol{f},s\boldsymbol{\psi}_0) - (c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\ 
        &= s(\boldsymbol{f},\boldsymbol{\psi}_0) - sc_0(\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)\\ 
        &= s(\boldsymbol{f},\boldsymbol{\psi}_0) - s\frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)}(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)\\ 
        &= s\left( (\boldsymbol{f},\boldsymbol{\psi}_0) - (\boldsymbol{f},\boldsymbol{\psi}_0)\right)\\ 
        &=0{\thinspace .}
        

Therefore, instead of minimizing the square of the norm, we could
demand that :math:`\boldsymbol{e}` is orthogonal to any vector in :math:`V`.
This method is known as *projection*, because it is the same as
projecting the vector onto the subspace.
(The approach can also be referred to as a Galerkin method as
explained at the end of the section :ref:`fem:approx:vec:Np1dim`.)

Mathematically the projection method is stated
by the equation

.. _Eq:fem:vec:Galerkin1:

.. math::
   :label: fem:vec:Galerkin1
        
        (\boldsymbol{e}, \boldsymbol{v}) = 0,\quad\forall\boldsymbol{v}\in V{\thinspace .}
        
        

An arbitrary :math:`\boldsymbol{v}\in V` can be expressed as
:math:`s\boldsymbol{\psi}_0`, :math:`s\in\mathbb{R}`, and therefore
:eq:`fem:vec:Galerkin1` implies

.. math::
         (\boldsymbol{e},s\boldsymbol{\psi}_0) = s(\boldsymbol{e}, \boldsymbol{\psi}_0) = 0,

which means that the error must be orthogonal to the basis vector in
the space :math:`V`:

.. math::
        
        (\boldsymbol{e}, \boldsymbol{\psi}_0)=0\quad\hbox{or}\quad
        (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)=0
        {\thinspace .}
        

The latter equation gives :eq:`fem:vec:c0` and it
also arose from least squares computations in
:eq:`fem:vec:dEdc0:Galerkin`.

.. _fem:approx:vec:Np1dim:

Approximation of general vectors
--------------------------------

.. index::
   single: approximation; of general vectors

Let us generalize the vector approximation from the previous section
to vectors in spaces with arbitrary dimension. Given some vector :math:`\boldsymbol{f}`,
we want to find the best approximation to this vector in
the space

.. math::
        
        V = \hbox{span}\,\{\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N\}
        {\thinspace .}
        

We assume that the *basis vectors* :math:`\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N` are
linearly independent so that none of them are redundant and
the space has dimension :math:`N+1`.
Any vector :math:`\boldsymbol{u}\in V` can be written as a linear combination
of the basis vectors,

.. math::
         \boldsymbol{u} = \sum_{j=0}^N c_j\boldsymbol{\psi}_j,

where :math:`c_j\in\mathbb{R}` are scalar coefficients to be determined.

The least squares method  (2)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now we want to find :math:`c_0,\ldots,c_N`, such that :math:`\boldsymbol{u}` is the best
approximation to :math:`\boldsymbol{f}` in the sense that the distance (error)
:math:`\boldsymbol{e} = \boldsymbol{f} - \boldsymbol{u}` is minimized. Again, we define
the squared distance as a function of the free parameters
:math:`c_0,\ldots,c_N`,

.. math::
        
        E(c_0,\ldots,c_N) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j,\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j)
        \nonumber
        

.. _Eq:fem:vec:genE:

.. math::
   :label: fem:vec:genE
          
        = (\boldsymbol{f},\boldsymbol{f}) - 2\sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) +
        \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q){\thinspace .}
        
        

Minimizing this :math:`E` with respect to the independent variables
:math:`c_0,\ldots,c_N` is obtained by requiring

.. math::
        
        \frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N
        {\thinspace .}
        

The second term in :eq:`fem:vec:genE` is differentiated as follows:

.. math::
        
        \frac{\partial}{\partial c_i}
        \sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) = (\boldsymbol{f},\boldsymbol{\psi}_i),
        

since the expression to be differentiated is a sum and only one term,
:math:`c_i(\boldsymbol{f},\boldsymbol{\psi}_i)`,
contains :math:`c_i` and this term is linear in :math:`c_i`.
To understand this differentiation in detail, write out the sum specifically for,
e.g, :math:`N=3` and :math:`i=1`.

The last term in :eq:`fem:vec:genE`
is more tedious to differentiate. We start with

.. math::
        
        \frac{\partial}{\partial c_i}
        c_pc_q =
        \left\lbrace\begin{array}{ll}
        0,  \hbox{ if } p\neq i\hbox{ and } q\neq i,
        

.. math::
          
        c_q,  \hbox{ if } p=i\hbox{ and } q\neq i,
        

.. math::
          
        c_p,  \hbox{ if } p\neq i\hbox{ and } q=i,
        

.. math::
          
        2c_i,  \hbox{ if } p=q= i,
        

.. math::
          
        \end{array}\right.
        

Then

.. math::
         \frac{\partial}{\partial c_i}
        \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
        = \sum_{p=0, p\neq i}^N c_p(\boldsymbol{\psi}_p,\boldsymbol{\psi}_i)
        + \sum_{q=0, q\neq i}^N c_q(\boldsymbol{\psi}_q,\boldsymbol{\psi}_i)
        +2c_i(\boldsymbol{\psi}_i,\boldsymbol{\psi}_i){\thinspace .}  

The last term can be included in the other two sums, resulting in

.. math::
        
        \frac{\partial}{\partial c_i}
        \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
        = 2\sum_{j=0}^N c_i(\boldsymbol{\psi}_j,\boldsymbol{\psi}_i){\thinspace .}  

It then follows that setting

.. math::
        
        \frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N,

leads to a linear system
for :math:`c_0,\ldots,c_N`:

.. _Eq:fem:approx:vec:Np1dim:eqsys:

.. math::
   :label: fem:approx:vec:Np1dim:eqsys
        
        \sum_{j=0}^N A_{i,j} c_j = b_i, \quad i=0,\ldots,N,
        
        

where

.. math::
        
        A_{i,j} = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),
        

.. math::
          
        b_i = (\boldsymbol{\psi}_i, \boldsymbol{f}){\thinspace .}  

We have changed the order of the two vectors in the inner
product according to :eq:`fem:vec:rule:symmetry`:

.. math::
         A_{i,j} = (\boldsymbol{\psi}_j,\boldsymbol{\psi}_i) = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),

simply because the sequence :math:`i`-$j$ looks more aesthetic.

The Galerkin or projection method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In analogy with the "one-dimensional" example in
the section :ref:`fem:approx:vec:plane`, it holds also here in the general
case that minimizing the distance
(error) :math:`\boldsymbol{e}` is equivalent to demanding that :math:`\boldsymbol{e}` is orthogonal to
all :math:`\boldsymbol{v}\in V`:

.. index::
   single: Galerkin method; vectors

.. index::
   single: projection; vectors

.. _Eq:fem:approx:vec:Np1dim:Galerkin:

.. math::
   :label: fem:approx:vec:Np1dim:Galerkin
        
        (\boldsymbol{e},\boldsymbol{v})=0,\quad \forall\boldsymbol{v}\in V{\thinspace .}
        
        

Since any :math:`\boldsymbol{v}\in V` can be written as :math:`\boldsymbol{v} =\sum_{i=0}^N c_i\boldsymbol{\psi}_i`,
the statement :eq:`fem:approx:vec:Np1dim:Galerkin` is equivalent to
saying that

.. math::
         (\boldsymbol{e}, \sum_{i=0}^N c_i\boldsymbol{\psi}_i) = 0,

for any choice of coefficients :math:`c_0,\ldots,c_N`.
The latter equation can be rewritten as

.. math::
         \sum_{i=0}^N c_i (\boldsymbol{e},\boldsymbol{\psi}_i) =0{\thinspace .}  

If this is to hold for arbitrary values of :math:`c_0,\ldots,c_N`
we must require that each term in the sum vanishes,

.. _Eq:fem:approx:vec:Np1dim:Galerkin0:

.. math::
   :label: fem:approx:vec:Np1dim:Galerkin0
        
        (\boldsymbol{e},\boldsymbol{\psi}_i)=0,\quad i=0,\ldots,N{\thinspace .}
        
        

These :math:`N+1` equations result in the same linear system as
:eq:`fem:approx:vec:Np1dim:eqsys`:

.. math::
         (\boldsymbol{f} - \sum_{j=0}^N c_j\boldsymbol{\psi}_j, \boldsymbol{\psi}_i) = (\boldsymbol{f}, \boldsymbol{\psi}_i) - \sum_{j\in{\mathcal{I}_s}}
        (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = 0,

and hence

.. math::
         \sum_{j=0}^N (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = (\boldsymbol{f}, \boldsymbol{\psi}_i),\quad i=0,\ldots, N
        {\thinspace .}
        

So, instead of differentiating the
:math:`E(c_0,\ldots,c_N)` function, we could simply use
:eq:`fem:approx:vec:Np1dim:Galerkin` as the principle for
determining :math:`c_0,\ldots,c_N`, resulting in the :math:`N+1`
equations :eq:`fem:approx:vec:Np1dim:Galerkin0`.

The names *least squares method* or *least squares approximation*
are natural since the calculations consists of
minimizing :math:`||\boldsymbol{e}||^2`, and :math:`||\boldsymbol{e}||^2` is a sum of squares
of differences between the components in :math:`\boldsymbol{f}` and :math:`\boldsymbol{u}`.
We find :math:`\boldsymbol{u}` such that this sum of squares is minimized.

The principle :eq:`fem:approx:vec:Np1dim:Galerkin`,
or the equivalent form :eq:`fem:approx:vec:Np1dim:Galerkin0`,
is known as *projection*. Almost the same mathematical idea
was used by the Russian mathematician `Boris Galerkin <http://en.wikipedia.org/wiki/Boris_Galerkin>`__ to solve
differential equations, resulting in what is widely known as
*Galerkin's method*.

