!split
======= Why finite elements? =======

 * Can with ease solve PDEs in domains with *complex geometry*
 * Can with ease provide higher-order approximations
 * Has (in simpler stationary problems) a rigorus mathematical
   analysis framework (not much considered here)

# The theoretical framework is not powerful enough to uncover the
# serious limitations of the method in time-dependent problems
# and the necessary adjustments)

!split
===== Domain for flow around a dolphin =====

FIGURE: [fig-fem/dolfin_mesh, width=400, frac=0.8]

!split
===== The flow =====

FIGURE: [fig-fem/dolfin_flow.gif, width=400, frac=0.8]

!split
===== Basic ingredients of the finite element method =====

 * Transform the PDE problem to a *variational form*
 * Define function approximation over *finite elements*
 * Use a machinery to derive *linear systems*
 * Solve linear systems

!split
===== Our learning strategy =====

 * Start with approximation of functions, not PDEs
 * Introduce finite element *approximations*
 * See later how this is applied to PDEs

Reason: the finite element method has many concepts and a jungle of details.
This strategy minimizes the mixing of ideas, concepts, and technical details.

!split
======= Approximation in vector spaces =======

FIGURE: [fig-fem/vecapprox_plane, width=300]

!split
===== Approximation set-up =====

General idea of finding an approximation $u(x)$ to some given $f(x)$:

!bt
\begin{equation}
 u(x) = \sum_{i=0}^N c_i\baspsi_i(x)
label{fem:u}
\end{equation}
!et

where

 * $\baspsi_i(x)$ are prescribed functions
 * $c_i$, $i=0,\ldots,N$ are unknown coefficients to be determined

!split
===== How to determine the coefficients? =====

We shall address three approaches:

 * The least squares method
 * The projection (or Galerkin) method
 * The interpolation (or collocation) method

!bblock Underlying motivation for our notation
Our mathematical framework for doing this is phrased in a way such
that it becomes easy to understand and use the "FEniCS": "http://fenicsproject.org" software package for finite element computing.
!eblock

!split
===== Approximation of planar vectors; problem =====
label{fem:approx:vec:plane}

Given a vector $\f = (3,5)$, find an approximation
to $\f$ directed along a given line.

FIGURE: [fig-fem/vecapprox_plane, width=300]

!split
===== Approximation of planar vectors; vector space terminology =====

!bt
\begin{equation}
V = \mbox{span}\,\{ \psib_0\}
\end{equation}
!et

 * $\psib_0$ is a basis vector in the space $V$
 * Seek $\u = c_0\psib_0\in V$
 * Determine $c_0$ such that $\u$ is the "best" approximation to $\f$
 * Visually, "best" is obvious

Define

 * the error $\e = \f - \u$
 * the (Eucledian) scalar product of two vectors: $(\u,\v)$
 * the norm of $\e$: $||\e|| = \sqrt{(\e, \e)}$

!split
===== The least squares method; principle =====

 * Idea: find $c_0$ such that $||\e||$ is minimized
 * Actually, we always minimize $E=||\e||^2$

!bt
\begin{equation*}
\frac{\partial E}{\partial c_0} = 0
\end{equation*}
!et

!split
===== The least squares method; calculations =====

!bt
\begin{equation}
E(c_0) = (\e,\e) = (\f,\f) - 2c_0(\f,\psib_0) + c_0^2(\psib_0,\psib_0)
\end{equation}
!et

!bt
\begin{equation}
\frac{\partial E}{\partial c_0} = -2(\f,\psib_0) + 2c_0 (\psib_0,\psib_0) = 0
label{fem:vec:dEdc0:v1}
\end{equation}
!et

!bt
\begin{equation}
c_0 = \frac{(\f,\psib_0)}{(\psib_0,\psib_0)}
label{fem:vec:c0}
\end{equation}
!et

!bt
\begin{equation}
c_0 = \frac{3a + 5b}{a^2 + b^2}
\end{equation}
!et

Observation for later: the vanishing derivative (ref{fem:vec:dEdc0:v1})
can be alternatively written as

!bt
\begin{equation}
(\e, \psib_0) = 0
label{fem:vec:dEdc0:Galerkin}
\end{equation}
!et

!split
===== The projection (or Galerkin) method =====

 * Backgrund: minimizing $||\e||^2$ implies that $\e$ is
   orthogonal to *any* vector $\v$ in the space $V$
   (visually clear, but can easily be computed too)
 * Alternative idea: demand $(\e, \v) = 0,\quad\forall\v\in V$
 * Equivalent statement: $(\e, \psib_0)=0$ (see notes for why)
 * Insert $\e = \f - c_0\psib_0$ and solve for $c_0$
 * Same equation for $c_0$ and hence same solution as in the least squares
   method

!split
===== Approximation of general vectors =====
label{fem:approx:vec:Np1dim}
idx{approximation!of general vectors}

Given a vector $\f$, find an approximation $\u\in V$:

!bt
\begin{equation*}
V = \hbox{span}\,\{\psib_0,\ldots,\psib_N\}
\end{equation*}
!et

 * We have a set of linearly independent basis vectors
   $\psib_0,\ldots,\psib_N$
 * Any $\u\in V$ can then be written as $\u = \sum_{j=0}^Nc_j\psib_j$

!split
===== The least squares method =====

Idea: find $c_0,\ldots,c_N$ such that $E= ||\e||^2$ is minimized, $\e=\f-\u$.

!bt
\begin{align*}
E(c_0,\ldots,c_N) &= (\e,\e) = (\f -\sum_jc_j\psib_j,\f -\sum_jc_j\psib_j)
\nonumber\\
&= (\f,\f) - 2\sum_{j=0}^Nc_j(\f,\psib_j) +
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\psib_p,\psib_q)
\end{align*}
!et

!bt
\begin{equation*}
\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N
\end{equation*}
!et

After some work we end up with a *linear system*

!bt
\begin{align}
\sum_{j=0}^N A_{i,j}c_j &= b_i,\quad i=0,\ldots,N\\
A_{i,j} &= (\psib_i,\psib_j)\\
b_i &= (\psib_i, \f)
\end{align}
!et

!split
===== The projection (or Galerkin) method =====

Can be shown that minimizing $||\e||$ implies that
$\e$ is orthogonal to all $\v\in V$:

idx{Galerkin method} idx{projection}

!bt
\[
(\e,\v)=0,\quad \forall\v\in V
\]
!et
which implies that $\e$ most be orthogonal to each basis vector:

!bt
\begin{equation}
(\e,\psib_i)=0,\quad i=0,\ldots,N
label{fem:approx:vec:Np1dim:Galerkin0}
\end{equation}
!et

This orthogonality condition is the principle of the projection
(or Galerkin) method. Leads to
the same linear system as in the least squares method.

!split
======= Approximation of functions =======
label{fem:approx:global}
idx{approximation!of functions}

Let $V$ be a *function space* spanned by a set of *basis functions*
$\baspsi_0,\ldots,\baspsi_N$,

!bt
\begin{equation*}
V = \hbox{span}\,\{\baspsi_0,\ldots,\baspsi_N\}
\end{equation*}
!et

Find $u\in V$ as a linear
combination of the basis functions:

!bt
\begin{equation}
u = \sum_{j\in\If} c_j\baspsi_j,\quad\If = \{0,1,\ldots,N\}
label{fem:approx:ufem}
\end{equation}
!et

!split
===== The least squares method =====
label{fem:approx:LS}

 * Extend the ideas from the vector case: minimize the (square) norm
   of the error.
 * What norm? $(f,g) = \int_\Omega f(x)g(x)\, dx$

!bt
\begin{equation}
E = (e,e) = (f-u,f-u) = (f(x)-\sum_{j\in\If} c_j\baspsi_j(x), f(x)-\sum_{j\in\If} c_j\baspsi_j(x))
label{fem:approx:LS:E}
\end{equation}
!et

!bt
\begin{equation}
E(c_0,\ldots,c_N) = (f,f) -2\sum_{j\in\If} c_j(f,\baspsi_i)
+ \sum_{p\in\If}\sum_{q\in\If} c_pc_q(\baspsi_p,\baspsi_q)
\end{equation}
!et

!bt
\begin{equation*}
\frac{\partial E}{\partial c_i} = 0,\quad i=\in\If
\end{equation*}
!et

After computations *identical to the vector case*, we get a linear system

!bt
\begin{align}
\sum_{j\in\If}^N A_{i,j}c_j &= b_i,\quad i\in\If
label{fem:approx:vec:Np1dim:eqsys}\\
A_{i,j} &= (\baspsi_i,\baspsi_j)
label{fem:approx:Aij}\\
b_i &= (f,\baspsi_i)
label{fem:approx:bi}
\end{align}
!et

!split
===== The projection (or Galerkin) method =====

As before, minimizing $(e,e)$ is equivalent to the projection (or Galerkin)
method

!bt
\begin{equation}
(e,v)=0,\quad\forall v\in V
label{fem:approx:Galerkin}
\end{equation}
!et
which means, as before,

!bt
\begin{equation}
(e,\baspsi_i)=0,\quad i\in\If
label{fem:approx:Galerkin0}
\end{equation}
!et

With the same algebra as in the multi-dimensional vector case,
we get the same linear system as arose from the least squares method.

!split
===== Example: linear approximation; problem =====
label{fem:approx:global:linear}

!bblock Problem
Approximate a parabola $f(x) = 10(x-1)^2 - 1$ by a straight line.
!eblock

!bt
\begin{equation*} V = \hbox{span}\,\{1, x\}  \end{equation*}
!et
That is, $\baspsi_0(x)=1$, $\baspsi_1(x)=x$, and $N=1$.
We seek

!bt
\begin{equation*}
u=c_0\baspsi_0(x) + c_1\baspsi_1(x) = c_0 + c_1x
\end{equation*}
!et

!split
===== Example: linear approximation; solution =====

!bt
\begin{align}
A_{0,0} &= (\baspsi_0,\baspsi_0) = \int_1^21\cdot 1\, dx = 1\\
A_{0,1} &= (\baspsi_0,\baspsi_1) = \int_1^2 1\cdot x\, dx = 3/2\\
A_{1,0} &= A_{0,1} = 3/2\\
A_{1,1} &= (\baspsi_1,\baspsi_1) = \int_1^2 x\cdot x\,dx = 7/3
\end{align}
!et

!bt
\begin{align}
b_1 &= (f,\baspsi_0) = \int_1^2 (10(x-1)^2 - 1)\cdot 1 \, dx = 7/3\\
b_2 &= (f,\baspsi_1) = \int_1^2 (10(x-1)^2 - 1)\cdot x\, dx = 13/3
\end{align}
!et
Solution of 2x2 linear system:

!bt
\begin{equation}
c_0 = -38/3,\quad c_1 = 10,\quad u(x) = 10x - \frac{38}{3}
\end{equation}
!et

!split
===== Example: linear approximation; plot =====

FIGURE: [fig-fem/parabola_ls_linear, width=400]

!split
===== Implementation of the least squares method; ideas =====
label{fem:approx:global:LS:code}

Consider symbolic computation of the linear system,
where

 * $f(x)$ is given as a `sympy` expression `f` (involving
   the symbol `x`),
 * `psi` is a list of $\sequencei{\baspsi}$,
 * `Omega` is a 2-tuple/list holding the domain $\Omega$

Carry out the integrations, solve the linear system, and
return $u(x)=\sum_jc_j\baspsi_j(x)$

!split
===== Implementation of the least squares method; symbolic code =====

!bc pycod
import sympy as sp

def least_squares(f, psi, Omega):
    N = len(psi) - 1
    A = sp.zeros((N+1, N+1))
    b = sp.zeros((N+1, 1))
    x = sp.Symbol('x')
    for i in range(N+1):
        for j in range(i, N+1):
            A[i,j] = sp.integrate(psi[i]*psi[j],
                                  (x, Omega[0], Omega[1]))
            A[j,i] = A[i,j]
        b[i,0] = sp.integrate(psi[i]*f, (x, Omega[0], Omega[1]))
    c = A.LUsolve(b)
    u = 0
    for i in range(len(psi)):
        u += c[i,0]*psi[i]
    return u, c
!ec
Observe: symmetric coefficient matrix so we can halve the integrations.

!split
===== Implementation of the least squares method; numerical code =====

 * Symbolic integration may be impossible and/or very slow
 * Turn to pure numerical computations in those cases
 * Supply Python functions `f(x)`, `psi(x,i)`, and a mesh `x`

!bc pycod
def least_squares_numerical(f, psi, N, x,
                            integration_method='scipy',
                            orthogonal_basis=False):
    import scipy.integrate
    A = np.zeros((N+1, N+1))
    b = np.zeros(N+1)
    Omega = [x[0], x[-1]]
    dx = x[1] - x[0]

    for i in range(N+1):
        j_limit = i+1 if orthogonal_basis else N+1
        for j in range(i, j_limit):
            print '(%d,%d)' % (i, j)
            if integration_method == 'scipy':
                A_ij = scipy.integrate.quad(
                    lambda x: psi(x,i)*psi(x,j),
                    Omega[0], Omega[1], epsabs=1E-9, epsrel=1E-9)[0]
            elif ...
            A[i,j] = A[j,i] = A_ij

        if integration_method == 'scipy':
            b_i = scipy.integrate.quad(
                lambda x: f(x)*psi(x,i), Omega[0], Omega[1],
                epsabs=1E-9, epsrel=1E-9)[0]
        elif ...
        b[i] = b_i

    c = b/np.diag(A) if orthogonal_basis else np.linalg.solve(A, b)
    u = sum(c[i]*psi(x, i) for i in range(N+1))
    return u, c
!ec

!split
===== Implementation of the least squares method; plotting =====

Compare $f$ and $u$ visually:

!bc pycod
def comparison_plot(f, u, Omega, filename='tmp.pdf'):
    x = sp.Symbol('x')
    # Turn f and u to ordinary Python functions
    f = sp.lambdify([x], f, modules="numpy")
    u = sp.lambdify([x], u, modules="numpy")
    resolution = 401  # no of points in plot
    xcoor  = linspace(Omega[0], Omega[1], resolution)
    exact  = f(xcoor)
    approx = u(xcoor)
    plot(xcoor, approx)
    hold('on')
    plot(xcoor, exact)
    legend(['approximation', 'exact'])
    savefig(filename)
!ec

All code in module "`approx1D.py`": "${src_fem}/approx1D.py"

!split
===== Implementation of the least squares method; application =====

!bc pycod
>>> from approx1D import *
>>> x = sp.Symbol('x')
>>> f = 10*(x-1)**2-1
>>> u, c = least_squares(f=f, psi=[1, x], Omega=[1, 2])
>>> comparison_plot(f, u, Omega=[1, 2])
!ec

FIGURE: [fig-fem/parabola_ls_linear, width=400]

!split
===== Perfect approximation; parabola approximating parabola =====
label{fem:approx:global:exact}

 * What if we add $\baspsi_2=x^2$ to the space $V$?
 * That is, approximating a parabola by any parabola?
 * (Hopefully we get the exact parabola!)

!bc pycod
>>> from approx1D import *
>>> x = sp.Symbol('x')
>>> f = 10*(x-1)**2-1
>>> u, c = least_squares(f=f, psi=[1, x, x**2], Omega=[1, 2])
>>> print u
10*x**2 - 20*x + 9
>>> print sp.expand(f)
10*x**2 - 20*x + 9
!ec


!split
===== Perfect approximation; the general result =====

 * What if we use $\psi_i(x)=x^i$ for $i=0,\ldots,N=40$?
 * The output from `least_squares` is $c_i=0$ for $i>2$

!bblock General result
If $f\in V$, least squares and projection/Galerkin give $u=f$.
!eblock

!split
===== Perfect approximation; proof of the general result =====

If $f\in V$, $f=\sum_{j\in\If}d_j\baspsi_j$, for
some  $\sequencei{d}$. Then

!bt
\begin{equation*}
b_i = (f,\baspsi_i) = \sum_{j\in\If}d_j(\baspsi_j, \baspsi_i)
= \sum_{j\in\If} d_jA_{i,j}
\end{equation*}
!et
The linear system $\sum_j A_{i,j}c_j = b_i$, $i\in\If$, is then

!bt
\begin{equation*}
\sum_{j\in\If}c_jA_{i,j} = \sum_{j\in\If}d_jA_{i,j},\quad i\in\If
\end{equation*}
!et
which implies that $c_i=d_i$ for $i\in\If$ and $u$ is identical to $f$.

!split
===== Finite-precision/numerical computations =====
label{fem:approx:global:illconditioning}

The previous computations were symbolic. What if we solve the
linear system numerically with standard arrays?

|-----------------------------------------|
| exact | `sympy` | `numpy32` | `numpy64` |
|---r--------r---------r-----------r------|
|9      | 9.62    | 5.57      |  8.98     |
|-20    | -23.39  | -7.65     | -19.93    |
|10     | 17.74   | -4.50     |  9.96     |
|0      | -9.19   | 4.13      | -0.26     |
|0      | 5.25    | 2.99      |  0.72     |
|0      | 0.18    | -1.21     | -0.93     |
|0      | -2.48   | -0.41     |  0.73     |
|0      | 1.81    | -0.013    | -0.36     |
|0      | -0.66   | 0.08      |  0.11     |
|0      | 0.12    | 0.04      | -0.02     |
|0      | -0.001  | -0.02     |  0.002    |
|-----------------------------------------|

  * Column 2: `sympy.mpmath.fp.matrix` and `sympy.mpmath.fp.lu_solve`
  * Column 3: `numpy` arrays with `numpy.float32` entries
  * Column 4: `numpy` arrays with `numpy.float64` entries

!split
===== Ill-conditioning (1) =====

Observations:

 * Significant round-off errors in the numerical computations (!)
 * But if we plot the approximations they look good (!)

Problem: The basis functions $x^i$ become almost linearly dependent for
large $N$.

FIGURE: [fig-fem/ill_conditioning, width=400 frac=0.7]

!split
===== Ill-conditioning (2) =====

  * Almost linearly dependent basis functions give almost singular matrices
  * Such matrices are said to be *ill conditioned*, and Gaussian elimination
    is severely affected by round-off errors
  * The basis $1, x, x^2, x^3, x^4, \ldots$ is a bad basis
  * Polynomials are fine as basis, but the more orthogonal they are,
    $(\baspsi_i,\baspsi_j)\approx 0$, the better

!split
===== Fourier series approximation; problem and code =====
label{fem:approx:global:Fourier}
idx{approximation!by sines}

Consider

!bt
\begin{equation*}
V = \hbox{span}\,\{ \sin \pi x, \sin 2\pi x,\ldots,\sin (N+1)\pi x\}
\end{equation*}
!et

!bc pycod
N = 3
from sympy import sin, pi
psi = [sin(pi*(i+1)*x) for i in range(N+1)]
f = 10*(x-1)**2 - 1
Omega = [0, 1]
u, c = least_squares(f, psi, Omega)
comparison_plot(f, u, Omega)
!ec

!split
===== Fourier series approximation; plot =====

$N=3$ vs $N=11$:

FIGURE: [fig-fem/parabola_ls_sines4_12, width=800, frac=1.2]

!split
===== Fourier series approximation; improvements =====

 * Considerably improvement by $N=11$
 * But always discrepancy of $f(0)-u(0)=9$ at $x=0$, because all the
   $\baspsi_i(0)=0$ and hence $u(0)=0$
 * Possible remedy: add a term that leads to correct boundary values

!bt
\begin{equation}
u(x) = f(0)(1-x) + xf(1) + \sum_{j\in\If} c_j\baspsi_j(x)
\end{equation}
!et
The extra term ensures $u(0)=f(0)$ and $u(1)=f(1)$ and
is a strikingly good help to get a good
approximation!

!split
===== Fourier series approximation; final results =====

$N=3$ vs $N=11$:

FIGURE: [fig-fem/parabola_ls_sines4_12_wfterm, width=800, frac=1.2]


!split
===== Orthogonal basis functions =====

This choice of sine functions as basis functions is popular because

 * the basis functions are orthogonal: $(\baspsi_i,\baspsi_j)=0$
 * implying that $A_{i,j}$ is a diagonal matrix
 * implying that we can solve for $c_i = 2\int_0^1 f(x)\sin ((i+1)\pi x) dx$

In general for an orthogonal basis, $A_{i,j}$ is diagonal and we can
easily solve for $c_i$:

!bt
\[
c_i = \frac{b_i}{A_{i,i}} = \frac{(f,\baspsi_i)}{(\baspsi_i,\baspsi_i)}
\]
!et

!split
===== The collocation or interpolation method; ideas and math =====
label{fem:approx:global:interp}

idx{collocation method (approximation)}
idx{approximation!collocation}

Here is another idea for approximating $f(x)$ by $u(x)=\sum_jc_j\baspsi_j$:

 * Force $u(\xno{i}) = f(\xno{i})$ at some selected *collocation* points
   $\sequencei{x}$
 * Then $u$ interpolates $f$
 * The method is known as *interpolation* or *collocation*

!bt
\begin{equation}
u(\xno{i}) = \sum_{j\in\If} c_j \baspsi_j(\xno{i}) = f(\xno{i})
\quad i\in\If,N
\end{equation}
!et

This is a linear system with no need for integration:

!bt
\begin{align}
\sum_{j\in\If} A_{i,j}c_j &= b_i,\quad i\in\If\\
A_{i,j} &= \baspsi_j(\xno{i})\\
b_i &= f(\xno{i})
\end{align}
!et

No symmetric matrix: $\baspsi_j(\xno{i})\neq \baspsi_i(\xno{j})$ in general

!split
===== The collocation or interpolation method; implementation =====

`points` holds the interpolation/collocation points

!bc pycod
def interpolation(f, psi, points):
    N = len(psi) - 1
    A = sp.zeros((N+1, N+1))
    b = sp.zeros((N+1, 1))
    x = sp.Symbol('x')
    # Turn psi and f into Python functions
    psi = [sp.lambdify([x], psi[i]) for i in range(N+1)]
    f = sp.lambdify([x], f)
    for i in range(N+1):
        for j in range(N+1):
            A[i,j] = psi[j](points[i])
        b[i,0] = f(points[i])
    c = A.LUsolve(b)
    u = 0
    for i in range(len(psi)):
        u += c[i,0]*psi[i](x)
    return u
!ec


!split
===== The collocation or interpolation method; approximating a parabola by linear functions =====

 * Potential difficulty: how to choose $\xno{i}$?
 * The results are sensitive to the points!

$(4/3,5/3)$ vs $(1,2)$:

FIGURE: [fig-fem/parabola_inter, width=700, frac=1.2]

!split
===== Lagrange polynomials; motivation and ideas =====
label{fem:approx:global:Lagrange}
idx{Lagrange (interpolating) polynomial}

Motivation:

 * The interpolation/collocation method avoids integration
 * With a diagonal matrix $A_{i,j} = \baspsi_j(\xno{i})$ we
   can solve the linear system by hand

The *Lagrange interpolating polynomials* $\baspsi_j$ have the property that

!bt
\[ \baspsi_i(\xno{j}) =\delta_{ij},\quad \delta_{ij} =
\left\lbrace\begin{array}{ll}
1, & i=j\\
0, & i\neq j
\end{array}\right.
\]
!et

Hence, $c_i = f(x_i)$ and

!bt
\begin{equation}
u(x) = \sum_{j\in\If} f(\xno{i})\baspsi_i(x)
\end{equation}
!et

 * Lagrange polynomials and interpolation/collocation look convenient
 * Lagrange polynomials are very much used in the finite element method

!split
===== Lagrange polynomials; formula and code =====

!bt
\begin{equation}
\baspsi_i(x) =
\prod_{j=0,j\neq i}^N
\frac{x-\xno{j}}{\xno{i}-\xno{j}}
= \frac{x-x_0}{\xno{i}-x_0}\cdots\frac{x-\xno{i-1}}{\xno{i}-\xno{i-1}}\frac{x-\xno{i+1}}{\xno{i}-\xno{i+1}}
\cdots\frac{x-x_N}{\xno{i}-x_N}
label{fem:approx:global:Lagrange:poly}
\end{equation}
!et

!bc pycod
def Lagrange_polynomial(x, i, points):
    p = 1
    for k in range(len(points)):
        if k != i:
            p *= (x - points[k])/(points[i] - points[k])
    return p
!ec

!split
===== Lagrange polynomials; successful example =====

FIGURE: [fig-fem/Lagrange_ls_interp_sin_4, width=800, frac=1.2]

!split
===== Lagrange polynomials; a less successful example =====

FIGURE: [fig-fem/Lagrange_interp_abs_8_15, width=800, frac=1.2]

!split
===== Lagrange polynomials; oscillatory behavior =====

12 points, degree 11, plot of two of the Lagrange polynomials - note that
they are zero at all points except one.

FIGURE: [fig-fem/Lagrange_basis_12, width=500]

Problem: strong oscillations near the boundaries for larger $N$ values.

!split
===== Lagrange polynomials; remedy for strong oscillations =====

The oscillations can be reduced by a more clever choice of
interpolation points, called the *Chebyshev nodes*:

!bt
\begin{equation}
\xno{i} = \half (a+b) + \half(b-a)\cos\left( \frac{2i+1}{2(N+1)}pi\right),\quad i=0\ldots,N
\end{equation}
!et
on an interval $[a,b]$.

!split
===== Lagrange polynomials; recalculation with Chebyshev nodes =====

FIGURE: [fig-fem/Lagrange_interp_abs_Cheb_8_15, width=800, frac=1.2]

!split
===== Lagrange polynomials; less oscillations with Chebyshev nodes =====

12 points, degree 11, plot of two of the Lagrange polynomials - note that
they are zero at all points except one.

FIGURE: [fig-fem/Lagrange_basis_Cheb_12, width=500]

!split
======= Finite element basis functions =======
label{fem:approx:fe}

!split
===== The basis functions have so far been global: $\baspsi_i(x) \neq 0$ almost everywhere =====

FIGURE: [fig-fem/u_example_sin, width=500]

!split
===== In the finite element method we use basis functions with local support =====

 * *Local support*: $\baspsi_i(x) \neq 0$ for $x$ in a
   small subdomain of $\Omega$
 * Typically hat-shaped
 * $u(x)$ based on these $\baspsi_i$ is a piecewise polynomial
   defined over many (small) subdomains
 * We introduce $\basphi_i$ as the name of these finite element hat
   functions (and for now choose $\baspsi_i=\basphi_i$)

FIGURE: [fig-fem/fe_mesh1D_phi_2_3, width=350 frac=0.5]

!split
===== The linear combination of hat functions is a piecewise linear function =====

FIGURE: [fig-fem/u_example_fe2, width=500]

!split
===== Elements and nodes =====
label{fem:approx:fe:def:elements:nodes}

Split $\Omega$ into non-overlapping subdomains called *elements*:

!bt
\begin{equation}
\Omega = \Omega^{(0)}\cup \cdots \cup \Omega^{(N_e)}
\end{equation}
!et

On each element, introduce points called *nodes*: $\xno{0},\ldots,\xno{N_n}$

 * The finite element basis functions are named $\basphi_i(x)$
 * $\basphi_i=1$ at node $i$ and 0 at all other nodes
 * $\basphi_i$ is a Lagrange polynomial on each element
 * For nodes at the boundary between two elements, $\basphi_i$ is made
   up of a Lagrange polynomial over each element

!split
===== Example on elements with two nodes (P1 elements) =====

FIGURE: [fig-fem/fe_mesh1D, width=500 frac=0.8]

Data structure: `nodes` holds coordinates or nodes, `elements` holds the
node numbers in each element

!bc pycod
nodes = [0, 1.2, 2.4, 3.6, 4.8, 5]
elements = [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]
!ec

!split
===== Illustration of two basis functions on the mesh =====

FIGURE: [fig-fem/fe_mesh1D_phi_2_3, width=500 frac=0.8]


!split
===== Example on elements with three nodes (P2 elements) =====

FIGURE: [fig-fem/fe_mesh1D_P2, width=500 frac=0.8]

!bc pycod
nodes = [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]
elements = [[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]]
!ec

!split
===== Some corresponding basis functions (P2 elements) =====

FIGURE: [fig-fem/phi/mpl_fe_basis_p2_4e, width=600]

!split
===== Examples on elements with four nodes per element (P3 elements) =====

FIGURE: [fig-fem/fe_mesh1D_d4_stretched, width=500 frac=0.8]

!bc pycod
d = 3  # d+1 nodes per element
num_elements = 4
num_nodes = num_elements*d + 1
nodes = [i*0.5 for i in range(num_nodes)]
elements = [[i*d+j for j in range(d+1)] for i in range(num_elements)]
!ec

!split
===== Some corresponding basis functions (P3 elements) =====

FIGURE: [fig-fem/phi/mpl_fe_basis_p3_4e, width=600]

!split
===== The numbering does not need to be regular from left to right =====

FIGURE: [fig-fem/fe_mesh1D_random_numbering, width=500 frac=0.8]

!bc pycod
nodes = [1.5, 5.5, 4.2, 0.3, 2.2, 3.1]
elements = [[2, 1], [4, 5], [0, 4], [3, 0], [5, 2]]
!ec


!split
===== Interpretation of the coefficients $c_i$ =====

Important property: $c_i$
is the value of $u$ at node $i$, $\xno{i}$:

!bt
\begin{equation}
u(\xno{i}) = \sum_{j\in\If} c_j\basphi_j(\xno{i}) =
c_i\basphi_i(\xno{i}) = c_i
label{fem:approx:fe:phi:prop1}
\end{equation}
!et

because $\basphi_j(\xno{i}) =0$ if $i\neq j$

!split
===== Properties of the basis functions =====

 * $\basphi_i(x) \neq 0$ only on those elements that contain global node $i$
 * $\basphi_i(x)\basphi_j(x) \neq 0$ if and only if $i$ and $j$ are global node
   numbers in the same element

Since $A_{i,j}=\int\basphi_i\basphi_j\dx$,
*most of the elements in the coefficient matrix will be zero*

FIGURE: [fig-fem/fe_mesh1D_phi_i_im1, width=350 frac=0.5]

!split
===== How to construct quadratic $\basphi_i$ (P2 elements) =====

FIGURE: [fig-fem/phi/mpl_fe_basis_p2_4e, width=350 frac=0.6]

 o Associate Lagrange polynomials with the nodes in an element
 o When the polynomial is 1 on the element boundary, combine it
   with the polynomial in the neighboring element

!split
===== Example on linear $\basphi_i$ (P1 elements) =====

FIGURE: [fig-fem/phi/mpl_fe_basis_p1_4e, width=350 frac=0.6]

!bt
\begin{equation}
\basphi_i(x) = \left\lbrace\begin{array}{ll}
0, & x < \xno{i-1}\\
(x - \xno{i-1})/h
& \xno{i-1} \leq x < \xno{i}\\
1 -
(x - x_{i})/h,
& \xno{i} \leq x < \xno{i+1}\\
0, & x\geq \xno{i+1}
\end{array}
\right.
label{fem:approx:fe:phi:1:formula2}
\end{equation}
!et

!split
===== Example on cubic $\basphi_i$ (P3 elements) =====

FIGURE: [fig-fem/phi/mpl_fe_basis_p3_4e, width=350, frac=0.6]


!split
======= Calculating the linear system for $c_i$ =======
label{fem:approx:global:linearsystem}

!split
===== Computing a specific matrix entry (1) =====

FIGURE: [fig-fem/fe_mesh1D_phi_2_3, width=300 frac=0.5]

$A_{2,3}=\int_\Omega\basphi_2\basphi_3 dx$: $\basphi_2\basphi_3\neq 0$
only over element 2. There,

!bt
\[ \basphi_3(x) = (x-x_2)/h,\quad \basphi_2(x) = 1- (x-x_2)/h\]
!et

!bt
\[
A_{2,3} = \int_\Omega \basphi_2\basphi_{3}\dx =
\int_{\xno{2}}^{\xno{3}}
\left(1 - \frac{x - \xno{2}}{h}\right) \frac{x - x_{2}}{h}
 \dx = \frac{h}{6}
\]
!et

!split
===== Computing a specific matrix entry (2) =====

FIGURE: [fig-fem/fe_mesh1D_phi_2_3, width=500 frac=0.8]

!bt
\[ A_{2,2} =
\int_{\xno{1}}^{\xno{2}}
\left(\frac{x - \xno{1}}{h}\right)^2\dx +
\int_{\xno{2}}^{\xno{3}}
\left(1 - \frac{x - \xno{2}}{h}\right)^2\dx
= \frac{h}{3}
\]
!et

!split
===== Calculating a general row in the matrix; figure =====

FIGURE: [fig-fem/fe_mesh1D_phi_i_im1, width=500 frac=0.8]

!bt
\[ A_{i,i-1} = \int_\Omega \basphi_i\basphi_{i-1}\dx = \hbox{?}\]
!et

!split
===== Calculating a general row in the matrix; details =====

!bt
\begin{align*}
A_{i,i-1} &= \int_\Omega \basphi_i\basphi_{i-1}\dx\\
&=
\underbrace{\int_{\xno{i-2}}^{\xno{i-1}} \basphi_i\basphi_{i-1}\dx}_{\basphi_i=0} +
\int_{\xno{i-1}}^{\xno{i}} \basphi_i\basphi_{i-1}\dx +
\underbrace{\int_{\xno{i}}^{\xno{i+1}} \basphi_i\basphi_{i-1}\dx}_{\basphi_{i-1}=0}\\
&= \int_{\xno{i-1}}^{\xno{i}}
\underbrace{\left(\frac{x - x_{i}}{h}\right)}_{\basphi_i(x)}
\underbrace{\left(1 - \frac{x - \xno{i-1}}{h}\right)}_{\basphi_{i-1}(x)} \dx =
\frac{h}{6}
\end{align*}
!et

 * $A_{i,i+1}=A_{i,i-1}$ due to symmetry
 * $A_{i,i}=h/3$ (same calculation as for $A_{2,2}$)
 * $A_{0,0}=A_{N,N}=h/3$ (only one element)

!split
===== Calculation of the right-hand side =====

FIGURE: [fig-fem/fe_mesh1D_phi_i_f, width=500 frac=0.8]

!bt
\begin{equation}
b_i = \int_\Omega\basphi_i(x)f(x)\dx
= \int_{\xno{i-1}}^{\xno{i}} \frac{x - \xno{i-1}}{h} f(x)\dx
+ \int_{x_{i}}^{\xno{i+1}} \left(1 - \frac{x - x_{i}}{h}\right) f(x)
\dx
label{fem:approx:fe:bi:formula1}
\end{equation}
!et

Need a specific $f(x)$ to do more...

!split
===== Specific example with two elements; linear system and solution =====

 * $f(x)=x(1-x)$ on $\Omega=[0,1]$
 * Two equal-sized elements $[0,0.5]$ and $[0.5,1]$

!bt
\begin{equation*}
A = \frac{h}{6}\left(\begin{array}{ccc}
2 & 1 & 0\\
1 & 4 & 1\\
0 & 1 & 2
\end{array}\right),\quad
b = \frac{h^2}{12}\left(\begin{array}{c}
2 - 3h\\
12 - 14h\\
10 -17h
\end{array}\right)
\end{equation*}
!et

!bt
\begin{equation*} c_0 = \frac{h^2}{6},\quad c_1 = h - \frac{5}{6}h^2,\quad
c_2 = 2h - \frac{23}{6}h^2
\end{equation*}
!et

!split
===== Specific example with two elements; plot =====

!bt
\begin{equation*} u(x)=c_0\basphi_0(x) + c_1\basphi_1(x) + c_2\basphi_2(x)\end{equation*}
!et

FIGURE: [fig-fem/fe_p1_x2_2e, width=400 frac=0.7]

!split
===== Specific example: what about four elements? =====

FIGURE: [fig-fem/fe_p1_x2_2e_4e, width=800, frac=1.0]

!split
======= Assembly of elementwise computations =======
label{fem:approx:fe:elementwise}

!split
===== Split the integrals into elementwise integrals =====

!bt
\begin{equation}
A_{i,j} = \int_\Omega\basphi_i\basphi_jdx =
\sum_{e} \int_{\Omega^{(e)}} \basphi_i\basphi_jdx,\quad
A^{(e)}_{i,j}=\int_{\Omega^{(e)}} \basphi_i\basphi_jdx
label{fem:approx:fe:elementwise:Asplit}
\end{equation}
!et

Important:

 * $A^{(e)}_{i,j}\neq 0$ if and only if $i$ and $j$ are nodes in element
   $e$ (otherwise no overlap between the basis functions)
 * all the nonzero elements in $A^{(e)}_{i,j}$ are collected in an
   *element matrix*

!split
===== The element matrix =====

!bt
\[
\tilde A^{(e)} = \{ \tilde A^{(e)}_{r,s}\},\quad
\tilde A^{(e)}_{r,s} =
\int_{\Omega^{(e)}}\basphi_{q(e,r)}\basphi_{q(e,s)}dx,
\quad r,s\in\Ifd=\{0,\ldots,d\}
\]
!et

 * $r,s$ run over *local node numbers* in an element; $i,j$ run
   over *global node numbers*
 * $i=q(e,r)$: mapping of local node number $r$ in element
   $e$ to the global node number $i$ (math equivalent to `i=elements[e][r]`)
 * Add $\tilde A^{(e)}_{r,s}$ into the global $A_{i,j}$
   (*assembly*)

!bt
\begin{equation}
 A_{q(e,r),q(e,s)} := A_{q(e,r),q(e,s)} + \tilde A^{(e)}_{r,s},\quad
r,s\in\Ifd
\end{equation}
!et

!split
===== Illustration of the matrix assembly: regularly numbered P1 elements =====

FIGURE: [mov-fem/fe_assembly_regular_2x2/fe_assembly_regular_2x2, width=400 frac=0.7]

# #if FORMAT == "html"
"Animation": "mov-fem/fe_assembly.html"
# #else
"Animation": "${doc_notes}/mov-fem/fe_assembly.html"
# #endif

!split
===== Illustration of the matrix assembly: regularly numbered P3 elements =====

FIGURE: [mov-fem/fe_assembly_regular_4x4/fe_assembly_regular_4x4, width=400 frac=0.7]

# #if FORMAT == "html"
"Animation": "mov-fem/fe_assembly.html"
# #else
"Animation": "${doc_notes}/mov-fem/fe_assembly.html"
# #endif

!split
===== Illustration of the matrix assembly: irregularly numbered P1 elements =====

FIGURE: [mov-fem/fe_assembly_irregular/fe_assembly_irregular, width=400 frac=0.7]

# #if FORMAT == "html"
"Animation": "mov-fem/fe_assembly.html"
# #else
"Animation": "${doc_notes}/mov-fem/fe_assembly.html"
# #endif

!split
===== Assembly of the right-hand side =====

!bt
\begin{equation}
b_i = \int_\Omega f(x)\basphi_i(x)dx =
\sum_{e} \int_{\Omega^{(e)}} f(x)\basphi_i(x)dx,\quad
b^{(e)}_{i}=\int_{\Omega^{(e)}} f(x)\basphi_i(x)dx
\end{equation}
!et

Important:

  * $b_i^{(e)}\neq 0$ if and only if global node $i$ is a node in element $e$
    (otherwise $\basphi_i=0$)
  * The $d+1$ nonzero $b_i^{(e)}$ can be collected in an *element vector*
    $\tilde b_r^{(e)}=\{ \tilde b_r^{(e)}\}$, $r\in\Ifd$

Assembly:

!bt
\begin{equation}
b_{q(e,r)} := b_{q(e,r)} + \tilde b^{(e)}_{r},\quad
r,s\in\Ifd
\end{equation}
!et

!split
======= Mapping to a reference element =======
label{fem:approx:fe:mapping}

Instead of computing

!bt
\begin{equation*} \tilde A^{(e)}_{r,s} = \int_{\Omega^{(e)}}\basphi_{q(e,r)}(x)\basphi_{q(e,s)}(x)dx
= \int_{x_L}^{x_R}\basphi_{q(e,r)}(x)\basphi_{q(e,s)}(x)dx
\end{equation*}
!et
we now map $[x_L, x_R]$ to
a standardized reference element domain $[-1,1]$ with local coordinate $X$

!split
===== Affine mapping =====

!bt
\begin{equation}
x = \half (x_L + x_R) + \half (x_R - x_L)X
label{fem:approx:fe:affine:mapping}
\end{equation}
!et
or rewritten as
!bt
\begin{equation}
x = x_m + {\half}hX, \qquad x_m=(x_L+x_R)/2
label{fem:approx:fe:affine:mapping2}
\end{equation}
!et

!split
===== Integral transformation =====

Reference element integration: just change integration variable
from $x$ to $X$. Introduce local basis function

!bt
\begin{equation}
\refphi_r(X) = \basphi_{q(e,r)}(x(X))
\end{equation}
!et

!bt
\begin{equation}
\tilde A^{(e)}_{r,s} = \int_{\Omega^{(e)}}\basphi_{q(e,r)}(x)\basphi_{q(e,s)}(x)dx
= \int\limits_{-1}^1 \refphi_r(X)\refphi_s(X)\underbrace{\frac{dx}{dX}}_{\det J = h/2}dX
= \int\limits_{-1}^1 \refphi_r(X)\refphi_s(X)\det J\,dX
\end{equation}
!et

!bt
\begin{equation}
\tilde b^{(e)}_{r} = \int_{\Omega^{(e)}}f(x)\basphi_{q(e,r)}(x)dx
= \int\limits_{-1}^1 f(x(X))\refphi_r(X)\det J\,dX
label{fem:approx:fe:mapping:be}
\end{equation}
!et

!split
===== Advantages of the reference element =====

  * Always the same domain for integration: $[-1,1]$
  * We only need formulas for $\refphi_r(X)$ over one element
    (no piecewise polynomial definition)
  * $\refphi_r(X)$ is the same for all elements: no dependence on
    element length and location, which is "factored out"
    in the mapping and $\det J$

!split
===== Standardized basis functions for P1 elements =====

!bt
\begin{align}
\refphi_0(X) &= \half (1 - X)
label{fem:approx:fe:mapping:P1:phi0}\\
\refphi_1(X) &= \half (1 + X)
label{fem:approx:fe:mapping:P1:phi1}
\end{align}
!et

!split
===== Standardized basis functions for P2 elements =====

P2 elements:

!bt
\begin{align}
\refphi_0(X) &= \half (X-1)X\\
\refphi_1(X) &= 1 - X^2\\
\refphi_2(X) &= \half (X+1)X
\end{align}
!et

Easy to generalize to arbitrary order!

!split
===== Integration over a reference element; element matrix =====
label{fem:approx:fe:intg:ref}

P1 elements and $f(x)=x(1-x)$.

!bt
\begin{align}
\tilde A^{(e)}_{0,0}
&= \int_{-1}^1 \refphi_0(X)\refphi_0(X)\frac{h}{2} dX\nonumber\\
&=\int_{-1}^1 \half(1-X)\half(1-X) \frac{h}{2} dX =
\frac{h}{8}\int_{-1}^1 (1-X)^2 dX = \frac{h}{3}
label{fem:approx:fe:intg:ref:Ae00}\\
\tilde A^{(e)}_{1,0}
&= \int_{-1}^1 \refphi_1(X)\refphi_0(X)\frac{h}{2} dX\nonumber\\
&=\int_{-1}^1 \half(1+X)\half(1-X) \frac{h}{2} dX =
\frac{h}{8}\int_{-1}^1 (1-X^2) dX = \frac{h}{6}\\
\tilde A^{(e)}_{0,1} &= \tilde A^{(e)}_{1,0}
label{fem:approx:fe:intg:ref:Ae10}\\
\tilde A^{(e)}_{1,1}
&= \int_{-1}^1 \refphi_1(X)\refphi_1(X)\frac{h}{2} dX\nonumber\\
&=\int_{-1}^1 \half(1+X)\half(1+X) \frac{h}{2} dX =
\frac{h}{8}\int_{-1}^1 (1+X)^2 dX = \frac{h}{3}
label{fem:approx:fe:intg:ref:Ae11}
\end{align}
!et

!split
===== Integration over a reference element; element vector =====

!bt
\begin{align}
\tilde b^{(e)}_{0}
&= \int_{-1}^1 f(x(X))\refphi_0(X)\frac{h}{2} dX\nonumber\\
&= \int_{-1}^1 (x_m + \half hX)(1-(x_m + \half hX))
\half(1-X)\frac{h}{2} dX \nonumber\\
&= - \frac{1}{24} h^{3} + \frac{1}{6} h^{2} x_{m} - \frac{1}{12} h^{2} - \half h x_{m}^{2} + \half h x_{m}
label{fem:approx:fe:intg:ref:be0}\\
\tilde b^{(e)}_{1}
&= \int_{-1}^1 f(x(X))\refphi_1(X)\frac{h}{2} dX\nonumber\\
&= \int_{-1}^1 (x_m + \half hX)(1-(x_m + \half hX))
\half(1+X)\frac{h}{2} dX \nonumber\\
&= - \frac{1}{24} h^{3} - \frac{1}{6} h^{2} x_{m} + \frac{1}{12} h^{2} -
\half h x_{m}^{2} + \half h x_{m}
\end{align}
!et

$x_m$: element midpoint.

!split
===== Tedious calculations! Let's use symbolic software =====

!bc pycod
>>> import sympy as sp
>>> x, x_m, h, X = sp.symbols('x x_m h X')
>>> sp.integrate(h/8*(1-X)**2, (X, -1, 1))
h/3
>>> sp.integrate(h/8*(1+X)*(1-X), (X, -1, 1))
h/6
>>> x = x_m + h/2*X
>>> b_0 = sp.integrate(h/4*x*(1-x)*(1-X), (X, -1, 1))
>>> print b_0
-h**3/24 + h**2*x_m/6 - h**2/12 - h*x_m**2/2 + h*x_m/2
!ec

Can printe out in LaTeX too (convenient for copying into reports):

!bc pycod
>>> print sp.latex(b_0, mode='plain')
- \frac{1}{24} h^{3} + \frac{1}{6} h^{2} x_{m}
- \frac{1}{12} h^{2} - \half h x_{m}^{2}
+ \half h x_{m}
!ec

!split
======= Implementation =======

 * Coming functions appear in "`fe_approx1D.py`": "${src_fem}/fe_approx1D.py"
 * Functions can operate in symbolic or numeric mode
 * The code documents all steps in finite element calculations!

!split
===== Compute finite element basis functions in the reference element =====

Let $\refphi_r(X)$ be a Lagrange polynomial of degree `d`:

!bc pycod
import sympy as sp
import numpy as np

def phi_r(r, X, d):
    if isinstance(X, sp.Symbol):
        h = sp.Rational(1, d)  # node spacing
        nodes = [2*i*h - 1 for i in range(d+1)]
    else:
        # assume X is numeric: use floats for nodes
        nodes = np.linspace(-1, 1, d+1)
    return Lagrange_polynomial(X, r, nodes)

def Lagrange_polynomial(x, i, points):
    p = 1
    for k in range(len(points)):
        if k != i:
            p *= (x - points[k])/(points[i] - points[k])
    return p

def basis(d=1):
    """Return the complete basis."""
    X = sp.Symbol('X')
    phi = [phi_r(r, X, d) for r in range(d+1)]
    return phi
!ec

!split
===== Compute the element matrix =====

!bc pycod
def element_matrix(phi, Omega_e, symbolic=True):
    n = len(phi)
    A_e = sp.zeros((n, n))
    X = sp.Symbol('X')
    if symbolic:
        h = sp.Symbol('h')
    else:
        h = Omega_e[1] - Omega_e[0]
    detJ = h/2  # dx/dX
    for r in range(n):
        for s in range(r, n):
            A_e[r,s] = sp.integrate(phi[r]*phi[s]*detJ, (X, -1, 1))
            A_e[s,r] = A_e[r,s]
    return A_e
!ec

!split
===== Example on symbolic vs numeric element matrix =====

!bc pycod
>>> from fe_approx1D import *
>>> phi = basis(d=1)
>>> phi
[1/2 - X/2, 1/2 + X/2]
>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=True)
[h/3, h/6]
[h/6, h/3]
>>> element_matrix(phi, Omega_e=[0.1, 0.2], symbolic=False)
[0.0333333333333333, 0.0166666666666667]
[0.0166666666666667, 0.0333333333333333]
!ec

!split
===== Compute the element vector =====

!bc pycod
def element_vector(f, phi, Omega_e, symbolic=True):
    n = len(phi)
    b_e = sp.zeros((n, 1))
    # Make f a function of X
    X = sp.Symbol('X')
    if symbolic:
        h = sp.Symbol('h')
    else:
        h = Omega_e[1] - Omega_e[0]
    x = (Omega_e[0] + Omega_e[1])/2 + h/2*X  # mapping
    f = f.subs('x', x)  # substitute mapping formula for x
    detJ = h/2  # dx/dX
    for r in range(n):
        b_e[r] = sp.integrate(f*phi[r]*detJ, (X, -1, 1))
    return b_e
!ec
Note `f.subs('x', x)`: replace `x` by $x(X)$ such that `f` contains `X`

!split
===== Fallback on numerical integration if symbolic integration fails =====

  * Element matrix: only polynomials and `sympy` always succeeds
  * Element vector: $\int f\refphi \dx$ can fail
    (`sympy` then returns an `Integral` object instead of a number)

!bc pycod
def element_vector(f, phi, Omega_e, symbolic=True):
        ...
        I = sp.integrate(f*phi[r]*detJ, (X, -1, 1))  # try...
        if isinstance(I, sp.Integral):
            h = Omega_e[1] - Omega_e[0]  # Ensure h is numerical
            detJ = h/2
            integrand = sp.lambdify([X], f*phi[r]*detJ)
            I = sp.mpmath.quad(integrand, [-1, 1])
        b_e[r] = I
        ...
!ec

!split
===== Linear system assembly and solution =====

!bc pycod
def assemble(nodes, elements, phi, f, symbolic=True):
    N_n, N_e = len(nodes), len(elements)
    zeros = sp.zeros if symbolic else np.zeros
    A = zeros((N_n, N_n))
    b = zeros((N_n, 1))
    for e in range(N_e):
        Omega_e = [nodes[elements[e][0]], nodes[elements[e][-1]]]

        A_e = element_matrix(phi, Omega_e, symbolic)
        b_e = element_vector(f, phi, Omega_e, symbolic)

        for r in range(len(elements[e])):
            for s in range(len(elements[e])):
                A[elements[e][r],elements[e][s]] += A_e[r,s]
            b[elements[e][r]] += b_e[r]
    return A, b
!ec

!split
===== Linear system solution =====

!bc pycod
if symbolic:
    c = A.LUsolve(b)           # sympy arrays, symbolic Gaussian elim.
else:
    c = np.linalg.solve(A, b)  # numpy arrays, numerical solve
!ec

Note: the symbolic computation of `A` and `b` and the symbolic
solution can be very tedious.

!split
===== Example on computing symbolic approximations =====

!bc pycod
>>> h, x = sp.symbols('h x')
>>> nodes = [0, h, 2*h]
>>> elements = [[0, 1], [1, 2]]
>>> phi = basis(d=1)
>>> f = x*(1-x)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
>>> A
[h/3,   h/6,   0]
[h/6, 2*h/3, h/6]
[  0,   h/6, h/3]
>>> b
[     h**2/6 - h**3/12]
[      h**2 - 7*h**3/6]
[5*h**2/6 - 17*h**3/12]
>>> c = A.LUsolve(b)
>>> c
[                           h**2/6]
[12*(7*h**2/12 - 35*h**3/72)/(7*h)]
[  7*(4*h**2/7 - 23*h**3/21)/(2*h)]
!ec

!split
===== Example on computing numerical approximations =====

!bc pycod
>>> nodes = [0, 0.5, 1]
>>> elements = [[0, 1], [1, 2]]
>>> phi = basis(d=1)
>>> x = sp.Symbol('x')
>>> f = x*(1-x)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=False)
>>> A
[ 0.166666666666667, 0.0833333333333333,                  0]
[0.0833333333333333,  0.333333333333333, 0.0833333333333333]
[                 0, 0.0833333333333333,  0.166666666666667]
>>> b
[          0.03125]
[0.104166666666667]
[          0.03125]
>>> c = A.LUsolve(b)
>>> c
[0.0416666666666666]
[ 0.291666666666667]
[0.0416666666666666]
!ec

!split
===== The structure of the coefficient matrix =====
label{fem:approx:fe:A:structure}

!bc pycod
>>> d=1; N_e=8; Omega=[0,1]  # 8 linear elements on [0,1]
>>> phi = basis(d)
>>> f = x*(1-x)
>>> nodes, elements = mesh_symbolic(N_e, d, Omega)
>>> A, b = assemble(nodes, elements, phi, f, symbolic=True)
>>> A
[h/3,   h/6,     0,     0,     0,     0,     0,     0,   0]
[h/6, 2*h/3,   h/6,     0,     0,     0,     0,     0,   0]
[  0,   h/6, 2*h/3,   h/6,     0,     0,     0,     0,   0]
[  0,     0,   h/6, 2*h/3,   h/6,     0,     0,     0,   0]
[  0,     0,     0,   h/6, 2*h/3,   h/6,     0,     0,   0]
[  0,     0,     0,     0,   h/6, 2*h/3,   h/6,     0,   0]
[  0,     0,     0,     0,     0,   h/6, 2*h/3,   h/6,   0]
[  0,     0,     0,     0,     0,     0,   h/6, 2*h/3, h/6]
[  0,     0,     0,     0,     0,     0,     0,   h/6, h/3]
!ec

Note: do this by hand to understand what is going on!

!split
===== General result: the coefficient matrix is sparse =====

 * Sparse = most of the entries are zeros
 * Below: P1 elements

!bt
\begin{equation}
A = \frac{h}{6}
\left(
\begin{array}{cccccccccc}
2 & 1 & 0
&\cdots & \cdots & \cdots & \cdots & \cdots & 0 \\
1 & 4 & 1 & \ddots &   & &  & &  \vdots \\
0 & 1 & 4 & 1 &
\ddots & &  &  & \vdots \\
\vdots & \ddots &  & \ddots & \ddots & 0 &  & & \vdots \\
\vdots &  & \ddots & \ddots & \ddots & \ddots & \ddots & & \vdots \\
\vdots & &  & 0 & 1 & 4 & 1 & \ddots & \vdots \\
\vdots & & &  & \ddots & \ddots & \ddots &\ddots  & 0 \\
\vdots & & & &  &\ddots  & 1  & 4  & 1 \\
0 &\cdots & \cdots &\cdots & \cdots & \cdots  & 0 & 1 & 2
\end{array}
\right)
\end{equation}
!et

!split
===== Exemplifying the sparsity for P2 elements =====

!bt
\begin{equation}
A = \frac{h}{30}
\left(
\begin{array}{ccccccccc}
4 & 2 & - 1 & 0
  & 0 & 0 & 0 & 0 & 0\\
  2 & 16 & 2
  & 0 & 0 & 0 & 0 & 0 & 0\\- 1 & 2 &
  8 & 2 & - 1 & 0 & 0 & 0 & 0\\
  0 & 0 & 2 & 16 & 2 & 0 & 0 & 0 & 0\\
  0 & 0 & - 1 & 2 & 8 & 2 & - 1 & 0 & 0\\
  0 & 0 & 0 & 0 & 2 & 16 & 2 & 0 & 0\\
  0 & 0 & 0 & 0 & - 1 & 2 & 8 & 2 & - 1
  \\0 & 0 & 0 & 0 & 0 & 0 &
  2 & 16 & 2\\0 & 0 & 0 & 0 & 0
  & 0 & - 1 & 2 & 4
\end{array}
\right)
\end{equation}
!et

!split
===== Matrix sparsity pattern for regular/random numbering of P1 elements =====

 * Left: number nodes and elements from left to right
 * Right: number nodes and elements arbitrarily

FIGURE: [fig-fem/sparsity_pattern_1D_30, width=800]

!split
===== Matrix sparsity pattern for regular/random numbering of P3 elements =====

 * Left: number nodes and elements from left to right
 * Right: number nodes and elements arbitrarily

FIGURE: [fig-fem/sparsity_pattern_1DP3_30, width=800]

!split
===== Sparse matrix storage and solution =====
label{fem:approx:fe:impl:sparse}

idx{sparse matrices}

The minimum storage requirements for the coefficient matrix $A_{i,j}$:

 * P1 elements: only 3 nonzero entires per row
 * P2 elements: only 5 nonzero entires per row
 * P3 elements: only 7 nonzero entires per row
 * It is important to utilize sparse storage and sparse solvers
 * In Python: `scipy.sparse` package

!split
===== Approximate $f\sim x^9$ by various elements; code =====

Compute a mesh with $N_e$ elements, basis functions of
degree $d$, and approximate a given symbolic expression
$f(x)$ by a finite element expansion $u(x) = \sum_jc_j\basphi_j(x)$:

!bc pycod
import sympy as sp
from fe_approx1D import approximate
x = sp.Symbol('x')

approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=4)
approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=2)
approximate(f=x*(1-x)**8, symbolic=False, d=1, N_e=8)
approximate(f=x*(1-x)**8, symbolic=False, d=2, N_e=4)
!ec

!split
===== Approximate $f\sim x^9$ by various elements; plot =====

FIGURE: [fig-fem/fe_p1_p2_x9_248e, width=800, frac=1.2]

!split
======= Comparison of finite element and finite difference approximation =======
label{fem:approx:fe:fd}

 * Finite difference approximation of a function $f(x)$: simply
   choose $u_i = f(x_i)$ (interpolation)
 * Galerkin/projection and least squares method:
   must derive and solve a linear system
 * What is *really* the difference in $u$?

!split
===== Interpolation/collocation with finite elements =====

Let $\{\xno{i}\}_{i\in\If}$ be the nodes in the mesh.
Collocation means

!bt
\begin{equation}
u(\xno{i})=f(\xno{i}),\quad i\in\If,
\end{equation}
!et
which translates to

!bt
\[ \sum_{j\in\If} c_j \basphi_j(\xno{i}) = f(\xno{i}),\]
!et
but $\basphi_j(\xno{i})=0$ if $i\neq j$ so the sum collapses to one
term $c_i\basphi_i(\xno{i}) = c_i$, and we have the result

!bt
\begin{equation}
c_i = f(\xno{i})
\end{equation}
!et

Same result as the standard finite difference approach, but finite elements
define $u$ also *between* the $\xno{i}$ points

!split
===== Galerkin/project and least squares vs collocation/interpolation or finite differences =====

 * Scope: work with P1 elements
 * Use projection/Galerkin or least squares (equivalent)
 * Interpret the resulting linear system as finite difference equations

The P1 finite element machinery results in a linear system where
equation no $i$ is

!bt
\begin{equation}
\frac{h}{6}(u_{i-1} + 4u_i + u_{i+1}) = (f,\basphi_i)
label{fem:deq:1D:approx:deq:massmat:diffeq2}
\end{equation}
!et

Note:

  * We have used $u_i$ for $c_i$ to make notation similar to
    finite differences
  * The finite difference counterpart is just $u_i=f_i$

!split
===== Expressing the left-hand side in finite difference operator notation =====

Rewrite the left-hand side of finite element equation no $i$:

!bt
\begin{equation}
h(u_i + \frac{1}{6}(u_{i-1} - 2u_i + u_{i+1})) = [h(u + \frac{h^2}{6}D_x D_x u)]_i
\end{equation}
!et
This is the standard finite difference approximation of

!bt
\[ h(u + \frac{h^2}{6}u'')\]
!et

!split
===== Treating the right-hand side; Trapezoidal rule =====

!bt
\[ (f,\basphi_i) = \int_{\xno{i-1}}^{\xno{i}} f(x)\frac{1}{h} (x - \xno{i-1}) dx
+ \int_{\xno{i}}^{\xno{i+1}} f(x)\frac{1}{h}(1 - (x - x_{i})) dx
\]
!et
Cannot do much unless we specialize $f$ or use *numerical integration*.

Trapezoidal rule using the nodes:

!bt
\[ (f,\basphi_i) = \int_\Omega f\basphi_i dx\approx h\half(
f(\xno{0})\basphi_i(\xno{0}) + f(\xno{N})\basphi_i(\xno{N}))
+ h\sum_{j=1}^{N-1} f(\xno{j})\basphi_i(\xno{j})
\]
!et
$\basphi_i(\xno{j})=\delta_{ij}$, so this formula collapses to one term:

!bt
\begin{equation}
(f,\basphi_i) \approx hf(\xno{i}),\quad i=1,\ldots,N-1\thinspace.
\end{equation}
!et

Same result as in collocation (interpolation)
and the finite difference method!

!split
===== Treating the right-hand side; Simpson's rule =====

!bt
\[ \int_\Omega g(x)dx \approx \frac{h}{6}\left( g(\xno{0}) +
2\sum_{j=1}^{N-1} g(\xno{j})
+ 4\sum_{j=0}^{N-1} g(\xno{j+\half}) + f(\xno{2N})\right),
\]
!et
Our case: $g=f\basphi_i$. The sums collapse because $\basphi_i=0$ at most of
the points.

!bt
\begin{equation}
(f,\basphi_i) \approx \frac{h}{3}(f_{i-\half} + f_i + f_{i+\half})
\end{equation}
!et

Conclusions:

  * While the finite difference method just samples $f$ at $x_i$,
    the finite element method applies an average (smoothing) of $f$ around $x_i$
  * On the left-hand side we have a term $\sim hu''$, and $u''$
    also contribute to smoothing
  * There is some inherent smoothing in the finite element
    method

!split
===== Finite element approximation vs finite differences =====

With Trapezoidal integration of $(f,\basphi_i)$, the finite element
metod essentially solve

!bt
\begin{equation}
u + \frac{h^2}{6} u'' = f,\quad u'(0)=u'(L)=0,
\end{equation}
!et
by the finite difference method

!bt
\begin{equation}
[u + \frac{h^2}{6} D_x D_x u = f]_i
\end{equation}
!et

With Simpson integration of $(f,\basphi_i)$ we essentially solve

!bt
\begin{equation}
[u + \frac{h^2}{6} D_x D_x u = \bar f]_i,
\end{equation}
!et
where
!bt
\[ \bar f_i = \frac{1}{3}(f_{i-1/2} + f_i + f_{i+1/2}) \]
!et

Note: as $h\rightarrow 0$, $hu''\rightarrow 0$ and $\bar f_i\rightarrow f_i$.

!split
===== Making finite elements behave as finite differences =====

 * Can we adjust the finite element method so that we do not
   get the extra $hu''$ smoothing term and averaging of $f$?
 * This is sometimes important in time-dependent problems to incorporate good
   properties of finite differences into finite elements

idx{mass matrix} idx{mass lumping} idx{lumped mass matrix}

Result:

 * Compute all integrals by the Trapezoidal method and P1 elements
 * Specifically, the coefficient matrix becomes diagonal ("lumped") -
   no linear system (!)
 * Loss of accuracy? The Trapezoidal rule has error $\Oof{h^2}$,
   the same as the approximation error in P1 elements

!split
======= Limitations of the nodes and element concepts =======
label{fem:approx:fe:limitations}

So far,

 * *Nodes*: points for defining $\basphi_i$ and computing $u$ values
 * *Elements*: subdomain (containing a few nodes)
 * This is a common notion of nodes and elements

One problem:

 * Our algorithms need nodes at the element boundaries
 * This is often not desirable, so we need to throw the `nodes` and
   `elements` arrays away and find a more generalized element concept

!split
======= A generalized element concept =======
label{fem:approx:fe:element}

 * We introduce *cell* for the subdomain that we up to now called element
 * A cell has *vertices* (interval end points)
 * *Nodes* are, almost as before,
   points where we want to compute unknown functions
 * *Degrees of freedom* is what the $c_j$ represent (usually function values
   at nodes)

!split
===== The concept of a finite element =====

idx{finite element, definition} idx{dof map}

  o a *reference cell* in a local reference coordinate system
  o a set of *basis functions* $\refphi_r$ defined on the cell
  o a set of *degrees of freedom* (e.g., function values)
    that uniquely determine
    the basis functions such that $\refphi_r=1$ for degree of freedom
    number $r$ and $\refphi_r=0$ for all other degrees of freedom
  o a mapping between local and global degree of freedom numbers (*dof map*)
  o a geometric *mapping* of the reference cell onto to cell in the physical
    domain: $[-1,1]\ \Rightarrow\ [x_L,x_R]$

!split
===== Implementation; basic data structures =====
label{fem:approx:fe:element:impl}

idx{`cells` list} idx{`vertices` list} idx{`dof_map` list}

  * Cell vertex coordinates: `vertices` (equals `nodes` for P1 elements)
  * Element vertices: `cell[e][r]` holds global vertex number of
    local vertex no `r` in element `e` (same as `elements` for P1 elements)
  * `dof_map[e,r]` maps local dof `r` in element `e` to global dof
    number (same as `elements` for P$d$ elements)

The assembly process now applies `dof_map`:

!bc pycod
A[dof_map[e][r], dof_map[e][s]] += A_e[r,s]
b[dof_map[e][r]] += b_e[r]
!ec

!split
===== Implementation; example with P2 elements =====

FIGURE: [fig-fem/fe_mesh1D_P2, width=500 frac=0.8]

!bc pycod
vertices = [0, 0.4, 1]
cells = [[0, 1], [1, 2]]
dof_map = [[0, 1, 2], [2, 3, 4]]
!ec

!split
===== Implementation; example with P0 elements =====

Example: Same mesh, but $u$ is piecewise constant in each cell (P0 element).
Same `vertices` and `cells`, but

!bc pycod
dof_map = [[0], [1]]
!ec
May think of one node in the middle of each element.

!bblock
We will hereafter work with `cells`, `vertices`, and `dof_map`.
!eblock

!split
===== Example on doing the algorithmic steps =====

!bc pycod
# Use modified fe_approx1D module
from fe_approx1D_numint import *

x = sp.Symbol('x')
f = x*(1 - x)

N_e = 10
# Create mesh with P3 (cubic) elements
vertices, cells, dof_map = mesh_uniform(N_e, d=3, Omega=[0,1])

# Create basis functions on the mesh
phi = [basis(len(dof_map[e])-1) for e in range(N_e)]

# Create linear system and solve it
A, b = assemble(vertices, cells, dof_map, phi, f)
c = np.linalg.solve(A, b)

# Make very fine mesh and sample u(x) on this mesh for plotting
x_u, u = u_glob(c, vertices, cells, dof_map,
                resolution_per_element=51)
plot(x_u, u)
!ec


!split
===== Approximating a parabola by P0 elements =====

FIGURE: [fig-fem/fe_p0_x2_4e_8e, width=800]

The `approximate` function automates the steps in the previous slide:

!bc pycod
from fe_approx1D_numint import *
x=sp.Symbol("x")
for N_e in 4, 8:
    approximate(x*(1-x), d=0, N_e=N_e, Omega=[0,1])
!ec

!split
===== Computing the error of the approximation; principles =====
label{fem:approx:fe:element:impl:error}

!bt
\[ L^2 \hbox{ error: }\quad ||e||_{L^2} =
\left(\int_{\Omega} e^2 dx\right)^{1/2}\]
!et

Accurate approximation of the integral:

 * Sample $u(x)$ at many points in each element (call `u_glob`, returns `x` and `u`)
 * Use the Trapezoidal rule based on the samples
 * It is important to integrate $u$ accurately *over the elements*
 * (In a finite difference method we would just sample the mesh point values)

!split
===== Computing the error of the approximation; details =====

!bwarning Note
We need a version of the Trapezoidal rule valid
for non-uniformly spaced points:

!bt
\[ \int_\Omega g(x) dx \approx \sum_{j=0}^{n-1} \half(g(x_j) +
g(x_{j+1}))(x_{j+1}-x_j)\]
!et
!ewarning

!bc pycod
# Given c, compute x and u values on a very fine mesh
x, u = u_glob(c, vertices, cells, dof_map,
              resolution_per_element=101)
# Compute the error on the very fine mesh
e = f(x) - u
e2 = e**2
# Vectorized Trapezoidal rule
E = np.sqrt(0.5*np.sum((e2[:-1] + e2[1:])*(x[1:] - x[:-1]))
!ec

!split
===== How does the error depend on $h$ and $d$? =====

Theory and experiments show that the least squares or projection/Galerkin
method in combination with P$d$ elements of equal length $h$ has an error

!bt
\begin{equation}
||e||_{L^2} = Ch^{d+1}
label{fem:approx:fe:error:theorem}
\end{equation}
!et
where $C$ depends on $f$, but not on $h$ or $d$.


!split
===== Cubic Hermite polynomials; definition =====

 * Can we construct $\basphi_i(x)$ with continuous derivatives? Yes!

Consider a reference cell $[-1,1]$. We introduce two nodes, $X=-1$ and $X=1$.
The degrees of freedom are

  * 0: value of function at $X=-1$
  * 1: value of first derivative at $X=-1$
  * 2: value of function at $X=1$
  * 3: value of first derivative at $X=1$

!bblock
Derivatives as unknowns ensure the same $\basphi_i'(x)$ value at nodes
and thereby continuous derivatives.
!eblock

!split
===== Cubic Hermite polynomials; derivation =====

4 constraints on $\refphi_r$ (1 for dof $r$, 0 for all others):

  * $\refphi_0(\Xno{0}) = 1$, $\refphi_0(\Xno{1}) = 0$,
    $\refphi_0'(\Xno{0}) = 0$, $\refphi_0'(\Xno{1}) = 0$
  * $\refphi_1'(\Xno{0}) = 1$, $\refphi_1'(\Xno{1}) = 0$,
    $\refphi_1(\Xno{0}) = 0$, $\refphi_1(\Xno{1}) = 0$
  * $\refphi_2(\Xno{1}) = 1$, $\refphi_2(\Xno{0}) = 0$,
    $\refphi_2'(\Xno{0}) = 0$, $\refphi_2'(\Xno{1}) = 0$
  * $\refphi_3'(\Xno{1}) = 1$, $\refphi_3'(\Xno{0}) = 0$,
    $\refphi_3(\Xno{0}) = 0$, $\refphi_3(\Xno{1}) = 0$

This gives 4 linear, coupled
equations *for each* $\refphi_r$ to determine the 4
coefficients in the cubic polynomial

!split
===== Cubic Hermite polynomials; result =====


!bt
\begin{align}
\refphi_0(X) &= 1 - \frac{3}{4}(X+1)^2 + \frac{1}{4}(X+1)^3\\
\refphi_1(X) &= -(X+1)(1 - \half(X+1))^2\\
\refphi_2(X) &= \frac{3}{4}(X+1)^2 - \half(X+1)^3\\
\refphi_3(X) &= -\half(X+1)(\half(X+1)^2 - (X+1))\\
\end{align}
!et

!split
======= Numerical integration =======

 * $\int_\Omega f\basphi_idx$ must in general be computed by numerical integration
 * Numerical integration is often used for the matrix too

Common form of a numerical integration rule:

!bt
\begin{equation}
\int_{-1}^{1} g(X)dX \approx \sum_{j=0}^M w_jg(\bar X_j),
\end{equation}
!et
where

 * $\bar X_j$ are *integration points*
 * $w_j$ are *integration weights*

Different rules correspond to different choices of points and weights

!split
===== The Midpoint rule =====

Simplest possibility: the Midpoint rule,

!bt
\begin{equation}
\int_{-1}^{1} g(X)dX \approx 2g(0),\quad \bar X_0=0,\ w_0=2,
\end{equation}
!et

Exact for linear integrands

!split
===== Newton-Cotes rules =====
label{fem:approx:fe:numint1}

idx{numerical integration!Midpoint rule}
idx{numerical integration!Trapezoidal rule}
idx{numerical integration!Simpson's rule}
idx{Midpoint rule} idx{Trapezoidal rule} idx{Simpson's rule}

 * Idea: use a fixed, uniformly distributed set of points in $[-1,1]$
 * The points often coincides with nodes
 * Very useful for making $\basphi_i\basphi_j=0$ and get diagonal
   ("mass") matrices ("lumping")


The Trapezoidal rule:

!bt
\begin{equation}
\int_{-1}^{1} g(X)dX \approx g(-1) + g(1),\quad \bar X_0=-1,\ \bar X_1=1,\ w_0=w_1=1,
 label{fem:approx:fe:numint1:trapez}
\end{equation}
!et

Simpson's rule:

!bt
\begin{equation}
\int_{-1}^{1} g(X)dX \approx \frac{1}{3}\left(g(-1) + 4g(0)
+ g(1)\right),
\end{equation}
!et
where

!bt
\begin{equation}
\bar X_0=-1,\ \bar X_1=0,\ \bar X_2=1,\ w_0=w_2=\frac{1}{3},\ w_1=\frac{4}{3}
\end{equation}
!et

!split
===== Gauss-Legendre rules with optimized points =====

idx{Gauss-Legendre quadrature}

 * Optimize the location of points to get higher accuracy
 * Gauss-Legendre rules (quadrature) adjust points and weights to
   integrate polynomials exactly

!bt
\begin{align}
M=1&:\quad \bar X_0=-\frac{1}{\sqrt{3}},\
\bar X_1=\frac{1}{\sqrt{3}},\ w_0=w_1=1\\
M=2&:\quad \bar X_0=-\sqrt{\frac{3}{{5}}},\ \bar X_0=0,\
\bar X_2= \sqrt{\frac{3}{{5}}},\ w_0=w_2=\frac{5}{9},\ w_1=\frac{8}{9}
\end{align}
!et

 * $M=1$: integrates 3rd degree polynomials exactly
 * $M=2$: integrates 5th degree polynomials exactly
 * In general, $M$-point rule integrates a polynomial
   of degree $2M+1$ exactly.

See "`numint.py`": "${src_fem}/numint.py" for a large collection of Gauss-Legendre rules.


!split
======= Approximation of functions in 2D =======
label{fem:approx:2D}

!bnotice Extensibility of 1D ideas.
All the concepts and algorithms developed for approximation of 1D functions
$f(x)$ can readily be extended to 2D functions $f(x,y)$ and 3D functions
$f(x,y,z)$. Key formulas stay the same.
!enotice

Inner product in 2D:

!bt
\begin{equation}
(f,g) = \int_\Omega f(x,y)g(x,y) dx dy
\end{equation}
!et

Least squares and project/Galerkin lead to a linear system

!bt
\begin{align*}
\sum_{j\in\If} A_{i,j}c_j &= b_i,\quad i\in\If\\
A_{i,j} &= (\baspsi_i,\baspsi_j)\\
b_i &= (f,\baspsi_i)
\end{align*}
!et

Challenge: How to construct 2D basis functions $\baspsi_i(x,y)$?

!split
===== 2D basis functions as tensor products of 1D functions =====
label{fem:approx:2D:global}

Use a 1D basis for $x$ variation and a similar for $y$ variation:

!bt
\begin{align}
V_x &= \mbox{span}\{ \hat\baspsi_0(x),\ldots,\hat\baspsi_{N_x}(x)\}
label{fem:approx:2D:Vx}\\
V_y &= \mbox{span}\{ \hat\baspsi_0(y),\ldots,\hat\baspsi_{N_y}(y)\}
label{fem:approx:2D:Vy}
\end{align}
!et

The 2D vector space can be defined as a *tensor product* $V = V_x\otimes V_y$
with basis functions

!bt
\[
\baspsi_{p,q}(x,y) = \hat\baspsi_p(x)\hat\baspsi_q(y)
\quad p\in\Ix,q\in\Iy\tp
\]
!et

!split
===== Tensor products =====

Given two vectors $a=(a_0,\ldots,a_M)$ and $b=(b_0,\ldots,b_N)$
their *outer tensor product*, also called the *dyadic product*,
is $p=a\otimes b$, defined through

!bt
\[ p_{i,j}=a_ib_j,\quad i=0,\ldots,M,\ j=0,\ldots,N\tp\]
!et
Note: $p$ has two indices (as a matrix or two-dimensional array)

Example: 2D basis as tensor product of 1D spaces,

!bt
\[ \baspsi_{p,q}(x,y) = \hat\baspsi_p(x)\hat\baspsi_q(y),
\quad p\in\Ix,q\in\Iy\]
!et

!split
===== Double or single index? =====

The 2D basis can employ a double index and double sum:

!bt
\[ u = \sum_{p\in\Ix}\sum_{q\in\Iy} c_{p,q}\baspsi_{p,q}(x,y)
\]
!et

Or just a single index:

!bt
\[ u = \sum_{j\in\If} c_j\baspsi_j(x,y)\]
!et
with

!bt
\[
\baspsi_i(x,y) = \hat\baspsi_p(x)\hat\baspsi_q(y),
\quad i=p N_y + q\hbox{ or } i=q N_x + p
\]
!et

!split
===== Example on 2D (bilinear) basis functions; formulas =====

In 1D we use the basis

!bt
\[ \{ 1, x \} \]
!et

2D tensor product (all combinations):

!bt
\[ \baspsi_{0,0}=1,\quad \baspsi_{1,0}=x, \quad \baspsi_{0,1}=y,
\quad \baspsi_{1,1}=xy
\]
!et
or with a single index:

!bt
\[ \baspsi_0=1,\quad \baspsi_1=x, \quad \baspsi_2=y,\quad\baspsi_3 =xy
\]
!et

See notes for details of a hand-calculation.

!split
===== Example on 2D (bilinear) basis functions; plot =====

Quadratic $f(x,y) = (1+x^2)(1+2y^2)$ (left), bilinear $u$ (right):

FIGURE: [fig-fem/approx2D_bilinear, width=800]

!split
===== Implementation; principal changes to the 1D code =====
label{fem:approx:2D:global:code}

Very small modification of `approx1D.py`:

 * `Omega = [[0, L_x], [0, L_y]]`
 * Symbolic integration in 2D
 * Construction of 2D (tensor product) basis functions

!split
===== Implementation; 2D integration =====

!bc pycod
import sympy as sp

integrand = psi[i]*psi[j]
I = sp.integrate(integrand,
                 (x, Omega[0][0], Omega[0][1]),
                 (y, Omega[1][0], Omega[1][1]))

# Fall back on numerical integration if symbolic integration
# was unsuccessful
if isinstance(I, sp.Integral):
    integrand = sp.lambdify([x,y], integrand)
    I = sp.mpmath.quad(integrand,
                       [Omega[0][0], Omega[0][1]],
                       [Omega[1][0], Omega[1][1]])
!ec

!split
===== Implementation; 2D basis functions =====

Tensor product of 1D "Taylor-style" polynomials $x^i$:

!bc pycod
def taylor(x, y, Nx, Ny):
    return [x**i*y**j for i in range(Nx+1) for j in range(Ny+1)]
!ec

Tensor product of 1D sine functions $\sin((i+1)\pi x)$:

!bc pycod
def sines(x, y, Nx, Ny):
    return [sp.sin(sp.pi*(i+1)*x)*sp.sin(sp.pi*(j+1)*y)
            for i in range(Nx+1) for j in range(Ny+1)]
!ec
Complete code in
"`approx2D.py`": "${src_fem}/fe_approx2D.py"

!split
===== Implementation; application =====

$f(x,y) = (1+x^2)(1+2y^2)$

!bc ipy
>>> from approx2D import *
>>> f = (1+x**2)*(1+2*y**2)
>>> psi = taylor(x, y, 1, 1)
>>> Omega = [[0, 2], [0, 2]]
>>> u, c = least_squares(f, psi, Omega)
>>> print u
8*x*y - 2*x/3 + 4*y/3 - 1/9
>>> print sp.expand(f)
2*x**2*y**2 + x**2 + 2*y**2 + 1
!ec

!split
===== Implementation; trying a perfect expansion =====

Add higher powers to the basis such that $f\in V$:

!bc ipy
>>> psi = taylor(x, y, 2, 2)
>>> u, c = least_squares(f, psi, Omega)
>>> print u
2*x**2*y**2 + x**2 + 2*y**2 + 1
>>> print u-f
0
!ec

Expected: $u=f$ when $f\in V$

!split
===== Generalization to 3D =====
label{fem:approx:3D:global}

Key idea:

!bt
\[ V = V_x\otimes V_y\otimes V_z\]
!et

!bnotice Repeated outer tensor product of multiple vectors

!bt
\begin{align*}
a^{(q)} &= (a^{(q)}_0,\ldots,a^{(q)}_{N_q}),\quad q=0,\ldots,m\\
p &= a^{(0)}\otimes\cdots\otimes a^{(m)}\\
p_{i_0,i_1,\ldots,i_m} &= a^{(0)}_{i_1}a^{(1)}_{i_1}\cdots a^{(m)}_{i_m}
\end{align*}
!et

!enotice

!bt
\begin{align*}
\baspsi_{p,q,r}(x,y,z) &= \hat\baspsi_p(x)\hat\baspsi_q(y)\hat\baspsi_r(z)\\
u(x,y,z) &= \sum_{p\in\Ix}\sum_{q\in\Iy}\sum_{r\in\Iz} c_{p,q,r}
\baspsi_{p,q,r}(x,y,z)
\end{align*}
!et

!split
======= Finite elements in 2D and 3D =======

The two great advantages of the finite element method:

  * Can handle complex-shaped domains in 2D and 3D
  * Can easily provide higher-order polynomials in the approximation

Finite elements in 1D: mostly for learning, insight, debugging


!split
===== Examples on cell types =====

2D:

 * triangles
 * quadrilaterals

3D:

 * tetrahedra
 * hexahedra

!split
===== Rectangular domain with 2D P1 elements =====

FIGURE: [fig-fem/mesh2D_rect_P1, width=800]


!split
===== Deformed geometry with 2D P1 elements =====

FIGURE: [fig-fem/mesh2D_quarter_circle, width=600 frac=0.8]

!split
===== Rectangular domain with 2D Q1 elements =====

FIGURE: [fig-fem/mesh2D_rect_Q1, width=500]

!split
===== Basis functions over triangles in the physical domain =====

The P1 triangular 2D element: $u$ is linear $ax + by + c$ over each
triangular cell

FIGURE: [fig-fem/demo2D_4x3r, width=400]

!split
===== Basic features of 2D P1 elements =====

 * $\basphi_r(X,Y)$ is a linear function over each element
 * Cells = triangles
 * Vertices = corners of the cells
 * Nodes = vertices
 * Degrees of freedom = function values at the nodes

!split
===== Linear mapping of reference element onto general triangular cell =====

FIGURE: [fig-fem/ElmT3n2D_map, width=400]

!split
===== $\basphi_i$: pyramid shape, composed of planes =====

 * $\basphi_i(X,Y)$ varies linearly over an element
 * $\basphi_i=1$ at vertex (node) $i$, 0 at all other vertices (nodes)

FIGURE: [fig-fem/demo2D_basisfunc, width=400]

!split
===== Element matrices and vectors =====

 * As in 1D, the contribution from one cell to the matrix involves
   just a few numbers, collected in the element matrix and vector
 * $\basphi_i\basphi_j\neq 0$ only if $i$ and $j$ are
   degrees of freedom (vertices/nodes) in the same element
 * The 2D P1 has a $3\times 3$ element matrix

!split
===== Basis functions over triangles in the reference cell =====

FIGURE: [fig-fem/fenics-book/elements/P1_2d, width=100 frac=0.3]

!bt
\begin{align}
\refphi_0(X,Y) &= 1 - X - Y\\
\refphi_1(X,Y) &= X\\
\refphi_2(X,Y) &= Y
\end{align}
!et

Higher-degree $\refphi_r$ introduce more nodes (dof = node values)

!split
===== 2D P1, P2, P3, P4, P5, and P6 elements =====

FIGURE: [fig-fem/fenics-book/elements/P1-6_2d, width=320 frac=0.5]

!split
===== P1 elements in 1D, 2D, and 3D =====

FIGURE: [fig-fem/fenics-book/elements/P1-1d2d3d, width=320 frac=0.5]

!split
===== P2 elements in 1D, 2D, and 3D =====

idx{simplex elements} idx{simplices} idx{faces} idx{edges}

FIGURE: [fig-fem/fenics-book/elements/P2-1d2d3d, width=320 frac=0.5]

 * Interval, triangle, tetrahedron: *simplex* element
   (plural quick-form: *simplices*)
 * Side of the cell is called *face*
 * Thetrahedron has also *edges*

!split
===== Affine mapping of the reference cell; formula =====

Mapping of local $\X = (X,Y)$ coordinates in the reference cell to
global, physical $\x = (x,y)$ coordinates:

!bt
\begin{equation}
\x = \sum_{r} \refphi_r^{(1)}(\X)\xdno{q(e,r)}
label{fem:approx:fe:affine:map}
\end{equation}
!et

where

  * $r$ runs over the local vertex numbers in the cell
  * $\xdno{i}$ are the $(x,y)$ coordinates of vertex $i$
  * $\refphi_r^{(1)}$ are P1 basis functions

This mapping preserves the straight/planar faces and edges.

!split
===== Affine mapping of the reference cell; figure =====

FIGURE: [fig-fem/ElmT3n2D_map, width=400]

!split
===== Isoparametric mapping of the reference cell =====

idx{isoparametric mapping} idx{mapping of reference cells!isoparametric mapping}

Idea: Use the basis functions
of the element (not only the P1 functions)
to map the element

!bt
\begin{equation}
\x = \sum_{r} \refphi_r(\X)\xdno{q(e,r)}
label{fem:approx:fe:isop:map}
\end{equation}
!et

Advantage: higher-order polynomial basis functions now map the
reference cell to a *curved* triangle or tetrahedron.

FIGURE: [fig-fem/ElmT6n2D_map, width=400]

!split
===== Computing integrals =====

Integrals must be transformed from $\Omega^{(e)}$ (physical cell)
to $\tilde\Omega^r$ (reference cell):

!bt
\begin{align}
\int_{\Omega^{(e)}}\basphi_i (\x) \basphi_j (\x) \dx &=
\int_{\tilde\Omega^r} \refphi_i (\X) \refphi_j (\X)
\det J\, \dX\\
\int_{\Omega^{(e)}}\basphi_i (\x) f(\x) \dx &=
\int_{\tilde\Omega^r} \refphi_i (\X) f(\x(\X)) \det J\, \dX
\end{align}
!et
where $\dx = dx dy$ or $\dx = dxdydz$ and $\det J$ is the determinant of the
Jacobian of the mapping $\x(\X)$.

!bt
\begin{equation}
J = \left[\begin{array}{cc}
\frac{\partial x}{\partial X} & \frac{\partial x}{\partial Y}\\
\frac{\partial y}{\partial X} & \frac{\partial y}{\partial Y}
\end{array}\right], \quad
\det J = \frac{\partial x}{\partial X}\frac{\partial y}{\partial Y}
- \frac{\partial x}{\partial Y}\frac{\partial y}{\partial X}
label{fem:approx:fe:2D:mapping:J:detJ}
\end{equation}
!et

Affine mapping
(ref{fem:approx:fe:affine:map}): $\det J=2\Delta$, $\Delta = \hbox{cell volume}$

!slide
===== Remark on going from 1D to 2D/3D =====

!bblock
Finite elements in 2D and 3D builds on the same
*ideas* and *concepts* as in 1D, but there is simply much
more to compute because the
specific mathematical formulas in 2D and 3D are more complicated
and the book keeping with dof maps also gets more complicated.
The manual work is tedious, lengthy, and error-prone
so automation by the computer is a must.
!eblock

